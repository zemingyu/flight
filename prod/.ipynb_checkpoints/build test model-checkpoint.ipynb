{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.17.0.2:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f882f1ff208>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: colorama in /opt/conda/lib/python3.6/site-packages\n",
      "Connecting to H2O server at http://172.17.0.2:54321... successful.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O cluster uptime:</td>\n",
       "<td>6 mins 13 secs</td></tr>\n",
       "<tr><td>H2O cluster version:</td>\n",
       "<td>3.14.0.7</td></tr>\n",
       "<tr><td>H2O cluster version age:</td>\n",
       "<td>1 month </td></tr>\n",
       "<tr><td>H2O cluster name:</td>\n",
       "<td>sparkling-water-jovyan_local-1511267289384</td></tr>\n",
       "<tr><td>H2O cluster total nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O cluster free memory:</td>\n",
       "<td>15.28 Gb</td></tr>\n",
       "<tr><td>H2O cluster total cores:</td>\n",
       "<td>36</td></tr>\n",
       "<tr><td>H2O cluster allowed cores:</td>\n",
       "<td>36</td></tr>\n",
       "<tr><td>H2O cluster status:</td>\n",
       "<td>locked, healthy</td></tr>\n",
       "<tr><td>H2O connection url:</td>\n",
       "<td>http://172.17.0.2:54321</td></tr>\n",
       "<tr><td>H2O connection proxy:</td>\n",
       "<td>None</td></tr>\n",
       "<tr><td>H2O internal security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>H2O API Extensions:</td>\n",
       "<td>XGBoost, Algos, AutoML, Core V3, Core V4</td></tr>\n",
       "<tr><td>Python version:</td>\n",
       "<td>3.6.3 final</td></tr></table></div>"
      ],
      "text/plain": [
       "--------------------------  ------------------------------------------\n",
       "H2O cluster uptime:         6 mins 13 secs\n",
       "H2O cluster version:        3.14.0.7\n",
       "H2O cluster version age:    1 month\n",
       "H2O cluster name:           sparkling-water-jovyan_local-1511267289384\n",
       "H2O cluster total nodes:    1\n",
       "H2O cluster free memory:    15.28 Gb\n",
       "H2O cluster total cores:    36\n",
       "H2O cluster allowed cores:  36\n",
       "H2O cluster status:         locked, healthy\n",
       "H2O connection url:         http://172.17.0.2:54321\n",
       "H2O connection proxy:\n",
       "H2O internal security:      False\n",
       "H2O API Extensions:         XGBoost, Algos, AutoML, Core V3, Core V4\n",
       "Python version:             3.6.3 final\n",
       "--------------------------  ------------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sparkling Water Context:\n",
      " * H2O name: sparkling-water-jovyan_local-1511267289384\n",
      " * cluster size: 1\n",
      " * list of used nodes:\n",
      "  (executorId, host, port)\n",
      "  ------------------------\n",
      "  (driver,172.17.0.2,54321)\n",
      "  ------------------------\n",
      "\n",
      "  Open H2O Flow in browser: http://172.17.0.2:54321 (CMD + click in Mac OSX)\n",
      "\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade colorama\n",
    "from pysparkling import *\n",
    "import h2o\n",
    "hc = H2OContext.getOrCreate(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas.io.sql as psql\n",
    "# import pyodbc\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "### Seaborn style\n",
    "sns.set_style(\"whitegrid\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "pd.set_option('display.float_format', lambda x: '{:,.1f}'.format(x))\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sqlContext.read.parquet('/home/jovyan/work/sparkling-water-2.2.2/data/s3/flight.pq.11.comb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "106453155"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df.sample(False, 0.000001, 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+--------------------------------+--------------------------------+---------------------+----------------------------+-------------------------+---------------------+-----------------------+---------------------+---------------------------+---------------------------------+--------------------------------+----------------------------------------+-------------------------------+-----------------------------------------+-----------------------------------+-----------------------------+----------------------------------+-----------------------------------+------------------------------------+----------------------------------+----------------------------------+-----------------------+------------------------------+---------------------------+-----------------------+-------------------------+-----------------------+-----------------------------+--------------------+----------+--------------------------------+--------------------------------+---------------------+----------------------------+-------------------------+---------------------+-----------------------+---------------------+---------------------------+---------------------------------+--------------------------------+----------------------------------------+-------------------------------+-----------------------------------------+-----------------------------------+-----------------------------+----------------------------------+-----------------------------------+------------------------------------+----------------------------------+----------------------------------+-----------------------+------------------------------+---------------------------+-----------------------+-------------------------+-----------------------+-----------------------------+--------------------+----------+-----+----------+--------+-----------------+-------+-----------------------------------------+-----------------------------+-----------------------------+----------------------------------+---------------------------------+-----------------------------+--------------------------+---------------------------------+------------------------------+--------------------------+----------------------------+--------------------------+--------------------------------+-------------------------+-----------------------------+------------------------------------------------------+-----------------------------+-----------------------------+----------------------------+------------------------------+-----------------------+---------------------------+-------------------------------------------+-------------------------------+-------------------------------+------------------------------------+-----------------------------------+-------------------------------+----------------------------+-----------------------------------+--------------------------------+----------------------------+------------------------------+----------------------------+----------------------------------+---------------------------------+------------------------+-----------------------+------------------------+--------------------------+-----------------+-----------------+------------------+-----------------------------------------+-----------------------------+-----------------------------+----------------------------------+---------------------------------+-----------------------------+--------------------------+---------------------------------+------------------------------+--------------------------+----------------------------+--------------------------+--------------------------------+-------------------------+-----------------------------+------------------------------------------------------+-----------------------------+-----------------------------+----------------------------+------------------------------+-----------------------+---------------------------+-------------------------------------------+-------------------------------+-------------------------------+------------------------------------+-----------------------------------+-------------------------------+----------------------------+-----------------------------------+--------------------------------+----------------------------+------------------------------+----------------------------+----------------------------------+---------------------------------+------------------------+-----------------------+------------------------+--------------------------+-----------------+-----------------+------------------+----+-------+--------+------+-----------+\n",
      "|currencyCode|   depDate|leg1_arrivalLocation_airportCity|leg1_arrivalLocation_airportCode|leg1_arrivalTime_date|leg1_arrivalTime_dateLongStr|leg1_arrivalTime_dateTime|leg1_arrivalTime_hour|leg1_arrivalTime_isoStr|leg1_arrivalTime_time|leg1_arrivalTime_travelDate|leg1_carrierSummary_airProviderId|leg1_carrierSummary_airlineCodes|leg1_carrierSummary_airlineImageFileName|leg1_carrierSummary_airlineName|leg1_carrierSummary_displayUrgencyMessage|leg1_carrierSummary_mixedCabinClass|leg1_carrierSummary_multiStop|leg1_carrierSummary_nextDayArrival|leg1_carrierSummary_noOfTicketsLeft|leg1_carrierSummary_seatMapAvailable|leg1_departureLocation_airportCity|leg1_departureLocation_airportCode|leg1_departureTime_date|leg1_departureTime_dateLongStr|leg1_departureTime_dateTime|leg1_departureTime_hour|leg1_departureTime_isoStr|leg1_departureTime_time|leg1_departureTime_travelDate|      leg1_stop_list|leg1_stops|leg2_arrivalLocation_airportCity|leg2_arrivalLocation_airportCode|leg2_arrivalTime_date|leg2_arrivalTime_dateLongStr|leg2_arrivalTime_dateTime|leg2_arrivalTime_hour|leg2_arrivalTime_isoStr|leg2_arrivalTime_time|leg2_arrivalTime_travelDate|leg2_carrierSummary_airProviderId|leg2_carrierSummary_airlineCodes|leg2_carrierSummary_airlineImageFileName|leg2_carrierSummary_airlineName|leg2_carrierSummary_displayUrgencyMessage|leg2_carrierSummary_mixedCabinClass|leg2_carrierSummary_multiStop|leg2_carrierSummary_nextDayArrival|leg2_carrierSummary_noOfTicketsLeft|leg2_carrierSummary_seatMapAvailable|leg2_departureLocation_airportCity|leg2_departureLocation_airportCode|leg2_departureTime_date|leg2_departureTime_dateLongStr|leg2_departureTime_dateTime|leg2_departureTime_hour|leg2_departureTime_isoStr|leg2_departureTime_time|leg2_departureTime_travelDate|      leg2_stop_list|leg2_stops|price|searchDate|stayDays|        tableName|task_id|timeline1_arrivalAirport_airportCityState|timeline1_arrivalAirport_city|timeline1_arrivalAirport_code|timeline1_arrivalAirport_localName|timeline1_arrivalAirport_longName|timeline1_arrivalAirport_name|timeline1_arrivalTime_date|timeline1_arrivalTime_dateLongStr|timeline1_arrivalTime_dateTime|timeline1_arrivalTime_hour|timeline1_arrivalTime_isoStr|timeline1_arrivalTime_time|timeline1_arrivalTime_travelDate|timeline1_brandedFareName|timeline1_carrier_airlineCode|timeline1_carrier_airlineImageFileNameWithoutExtension|timeline1_carrier_airlineName|timeline1_carrier_bookingCode|timeline1_carrier_cabinClass|timeline1_carrier_flightNumber|timeline1_carrier_plane|timeline1_carrier_planeCode|timeline1_departureAirport_airportCityState|timeline1_departureAirport_city|timeline1_departureAirport_code|timeline1_departureAirport_localName|timeline1_departureAirport_longName|timeline1_departureAirport_name|timeline1_departureTime_date|timeline1_departureTime_dateLongStr|timeline1_departureTime_dateTime|timeline1_departureTime_hour|timeline1_departureTime_isoStr|timeline1_departureTime_time|timeline1_departureTime_travelDate|timeline1_distance_formattedTotal|timeline1_distance_total|timeline1_distance_unit|timeline1_duration_hours|timeline1_duration_minutes|timeline1_layover|timeline1_segment|    timeline1_type|timeline2_arrivalAirport_airportCityState|timeline2_arrivalAirport_city|timeline2_arrivalAirport_code|timeline2_arrivalAirport_localName|timeline2_arrivalAirport_longName|timeline2_arrivalAirport_name|timeline2_arrivalTime_date|timeline2_arrivalTime_dateLongStr|timeline2_arrivalTime_dateTime|timeline2_arrivalTime_hour|timeline2_arrivalTime_isoStr|timeline2_arrivalTime_time|timeline2_arrivalTime_travelDate|timeline2_brandedFareName|timeline2_carrier_airlineCode|timeline2_carrier_airlineImageFileNameWithoutExtension|timeline2_carrier_airlineName|timeline2_carrier_bookingCode|timeline2_carrier_cabinClass|timeline2_carrier_flightNumber|timeline2_carrier_plane|timeline2_carrier_planeCode|timeline2_departureAirport_airportCityState|timeline2_departureAirport_city|timeline2_departureAirport_code|timeline2_departureAirport_localName|timeline2_departureAirport_longName|timeline2_departureAirport_name|timeline2_departureTime_date|timeline2_departureTime_dateLongStr|timeline2_departureTime_dateTime|timeline2_departureTime_hour|timeline2_departureTime_isoStr|timeline2_departureTime_time|timeline2_departureTime_travelDate|timeline2_distance_formattedTotal|timeline2_distance_total|timeline2_distance_unit|timeline2_duration_hours|timeline2_duration_minutes|timeline2_layover|timeline2_segment|    timeline2_type|trip|version|fromCity|toCity|searchMonth|\n",
      "+------------+----------+--------------------------------+--------------------------------+---------------------+----------------------------+-------------------------+---------------------+-----------------------+---------------------+---------------------------+---------------------------------+--------------------------------+----------------------------------------+-------------------------------+-----------------------------------------+-----------------------------------+-----------------------------+----------------------------------+-----------------------------------+------------------------------------+----------------------------------+----------------------------------+-----------------------+------------------------------+---------------------------+-----------------------+-------------------------+-----------------------+-----------------------------+--------------------+----------+--------------------------------+--------------------------------+---------------------+----------------------------+-------------------------+---------------------+-----------------------+---------------------+---------------------------+---------------------------------+--------------------------------+----------------------------------------+-------------------------------+-----------------------------------------+-----------------------------------+-----------------------------+----------------------------------+-----------------------------------+------------------------------------+----------------------------------+----------------------------------+-----------------------+------------------------------+---------------------------+-----------------------+-------------------------+-----------------------+-----------------------------+--------------------+----------+-----+----------+--------+-----------------+-------+-----------------------------------------+-----------------------------+-----------------------------+----------------------------------+---------------------------------+-----------------------------+--------------------------+---------------------------------+------------------------------+--------------------------+----------------------------+--------------------------+--------------------------------+-------------------------+-----------------------------+------------------------------------------------------+-----------------------------+-----------------------------+----------------------------+------------------------------+-----------------------+---------------------------+-------------------------------------------+-------------------------------+-------------------------------+------------------------------------+-----------------------------------+-------------------------------+----------------------------+-----------------------------------+--------------------------------+----------------------------+------------------------------+----------------------------+----------------------------------+---------------------------------+------------------------+-----------------------+------------------------+--------------------------+-----------------+-----------------+------------------+-----------------------------------------+-----------------------------+-----------------------------+----------------------------------+---------------------------------+-----------------------------+--------------------------+---------------------------------+------------------------------+--------------------------+----------------------------+--------------------------+--------------------------------+-------------------------+-----------------------------+------------------------------------------------------+-----------------------------+-----------------------------+----------------------------+------------------------------+-----------------------+---------------------------+-------------------------------------------+-------------------------------+-------------------------------+------------------------------------+-----------------------------------+-------------------------------+----------------------------+-----------------------------------+--------------------------------+----------------------------+------------------------------+----------------------------+----------------------------------+---------------------------------+------------------------+-----------------------+------------------------+--------------------------+-----------------+-----------------+------------------+----+-------+--------+------+-----------+\n",
      "|         AUD|2017-12-25|                            Rome|                             FCO|           25/12/2017|               Mon., 25 Dec.|            1514223900000|                 null|   2017-12-25T18:45:...|               6:45pm|                       null|                                7|                        [EK, EK]|                                  EK.gif|                       Emirates|                                    false|                              false|                         true|                             false|                                  9|                                true|                          Shanghai|                               PVG|             25/12/2017|                 Mon., 25 Dec.|              1514153700000|                      6|     2017-12-25T06:15:...|                 6:15am|                         null|[[Dubai, United A...|         1|                        Shanghai|                             PVG|             2/1/2018|                Tue., 2 Jan.|            1514898300000|                 null|   2018-01-02T21:05:...|               9:05pm|                       null|                                7|                        [EK, EK]|                                  EK.gif|                       Emirates|                                    false|                              false|                         true|                              true|                                  9|                                true|                              Rome|                               FCO|               1/1/2018|                  Mon., 1 Jan.|              1514814900000|                     14|     2018-01-01T14:55:...|                 2:55pm|                         null|[[Dubai, United A...|         1|977.0|2017-06-28|       7|flight_6_36_price|  25197|                     [Dubai, United Ar...|                [Dubai, Rome]|                   [DXB, FCO]|              [Dubai Intl., Fiu...|             [Dubai, United Ar...|         [Dubai (Dubai Int...|      [25/12/2017, 25/1...|             [Mon., 25 Dec., M...|          [1514190000000, 1...|              [null, null]|        [2017-12-25T12:20...|         [12:20pm, 6:45pm]|            [12/25/17, 12/25/17]|                     [, ]|                     [EK, EK]|                                              [EK, EK]|         [Emirates, Emirates]|                       [T, T]|                      [3, 3]|                     [305, 95]|   [Airbus Industrie...|                 [388, 77W]|                       [Shanghai, China,...|              [Shanghai, Dubai]|                     [PVG, DXB]|                [Pudong Intl., Du...|               [Shanghai, China ...|           [Shanghai (PVG), ...|        [25/12/2017, 25/1...|               [Mon., 25 Dec., M...|            [1514153700000, 1...|                [null, null]|          [2017-12-25T06:15...|            [6:15am, 3:05pm]|              [12/25/17, 12/25/17]|                           [0, 0]|                  [0, 0]|               [km, km]|                 [10, 6]|                   [5, 40]|   [false, false]|     [true, true]|[Segment, Segment]|                     [Dubai, United Ar...|            [Dubai, Shanghai]|                   [DXB, PVG]|              [Dubai Intl., Pud...|             [Dubai, United Ar...|         [Dubai (Dubai Int...|      [1/1/2018, 2/1/2018]|             [Mon., 1 Jan., Tu...|          [1514836200000, 1...|              [null, null]|        [2018-01-01T23:50...|         [11:50pm, 9:05pm]|            [01/01/18, 01/02/18]|                     [, ]|                     [EK, EK]|                                              [EK, EK]|         [Emirates, Emirates]|                       [T, T]|                      [3, 3]|                     [98, 304]|   [Airbus Industrie...|                 [388, 388]|                       [Rome, Italy, Dub...|                  [Rome, Dubai]|                     [FCO, DXB]|                [Fiumicino - Leon...|               [Rome, Italy (FCO...|           [Rome (FCO), Duba...|        [1/1/2018, 2/1/2018]|               [Mon., 1 Jan., Tu...|            [1514814900000, 1...|                [null, null]|          [2018-01-01T14:55...|            [2:55pm, 9:15am]|              [01/01/18, 01/02/18]|                           [0, 0]|                  [0, 0]|               [km, km]|                  [5, 7]|                  [55, 50]|   [false, false]|     [true, true]|[Segment, Segment]|   2|    1.1|shanghai|  Rome|     201706|\n",
      "+------------+----------+--------------------------------+--------------------------------+---------------------+----------------------------+-------------------------+---------------------+-----------------------+---------------------+---------------------------+---------------------------------+--------------------------------+----------------------------------------+-------------------------------+-----------------------------------------+-----------------------------------+-----------------------------+----------------------------------+-----------------------------------+------------------------------------+----------------------------------+----------------------------------+-----------------------+------------------------------+---------------------------+-----------------------+-------------------------+-----------------------+-----------------------------+--------------------+----------+--------------------------------+--------------------------------+---------------------+----------------------------+-------------------------+---------------------+-----------------------+---------------------+---------------------------+---------------------------------+--------------------------------+----------------------------------------+-------------------------------+-----------------------------------------+-----------------------------------+-----------------------------+----------------------------------+-----------------------------------+------------------------------------+----------------------------------+----------------------------------+-----------------------+------------------------------+---------------------------+-----------------------+-------------------------+-----------------------+-----------------------------+--------------------+----------+-----+----------+--------+-----------------+-------+-----------------------------------------+-----------------------------+-----------------------------+----------------------------------+---------------------------------+-----------------------------+--------------------------+---------------------------------+------------------------------+--------------------------+----------------------------+--------------------------+--------------------------------+-------------------------+-----------------------------+------------------------------------------------------+-----------------------------+-----------------------------+----------------------------+------------------------------+-----------------------+---------------------------+-------------------------------------------+-------------------------------+-------------------------------+------------------------------------+-----------------------------------+-------------------------------+----------------------------+-----------------------------------+--------------------------------+----------------------------+------------------------------+----------------------------+----------------------------------+---------------------------------+------------------------+-----------------------+------------------------+--------------------------+-----------------+-----------------+------------------+-----------------------------------------+-----------------------------+-----------------------------+----------------------------------+---------------------------------+-----------------------------+--------------------------+---------------------------------+------------------------------+--------------------------+----------------------------+--------------------------+--------------------------------+-------------------------+-----------------------------+------------------------------------------------------+-----------------------------+-----------------------------+----------------------------+------------------------------+-----------------------+---------------------------+-------------------------------------------+-------------------------------+-------------------------------+------------------------------------+-----------------------------------+-------------------------------+----------------------------+-----------------------------------+--------------------------------+----------------------------+------------------------------+----------------------------+----------------------------------+---------------------------------+------------------------+-----------------------+------------------------+--------------------------+-----------------+-----------------+------------------+----+-------+--------+------+-----------+\n",
      "only showing top 1 row\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/lda.py:6: DeprecationWarning: lda.LDA has been moved to discriminant_analysis.LinearDiscriminantAnalysis in 0.17 and will be removed in 0.19\n",
      "  \"in 0.17 and will be removed in 0.19\", DeprecationWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/learning_curve.py:23: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the functions are moved. This module will be removed in 0.20\n",
      "  DeprecationWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/qda.py:6: DeprecationWarning: qda.QDA has been moved to discriminant_analysis.QuadraticDiscriminantAnalysis in 0.17 and will be removed in 0.19.\n",
      "  \"in 0.17 and will be removed in 0.19.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark import SQLContext\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import TimestampType\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import *\n",
    "from sklearn import preprocessing\n",
    "# import lightgbm as lgb\n",
    "from multiprocessing import *\n",
    "from sklearn.model_selection import StratifiedKFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[price: double, priceWillDrop: int, futureMinPrice: double, saving: double, fromCity: string, toCity: string, searchDate: string, routeCombKey: string, leadTime: int, leg1_stops: bigint, leg2_stops: bigint, leg1_noOfTicketsLeft: bigint, leg2_noOfTicketsLeft: bigint, leg1_carrierSummary_airlineName: string, leg2_carrierSummary_airlineName: string, leg1_departureTime_hour: bigint, leg2_departureTime_hour: bigint, depWeekOfYear: int, depDayOfWeek: string, retWeekOfYear: int, retDayOfWeek: string, searchDayOfWeek: string, leg1_cabinClass_0: string, leg1_cabinClass_1: string, leg1_cabinClass_2: string, leg2_cabinClass_0: string, leg2_cabinClass_1: string, leg2_cabinClass_2: string, trip: bigint]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lookforwardDays = 19\n",
    "ndays = lambda i: i * 86400 # number of seconds in a day\n",
    "byVar = ['toCity', 'depDate', 'stayDays', 'timeline1_departureTime_time', 'timeline2_departureTime_time',\n",
    "         'timeline1_carrier_airlineCode', 'timeline2_carrier_airlineCode']\n",
    "\n",
    "w = (Window.partitionBy('routeCombKey')\n",
    "     .orderBy(F.col('searchDate').cast('timestamp').cast('long'))\n",
    "     .rowsBetween(0, ndays(lookforwardDays)))\n",
    "\n",
    "threshold = 20\n",
    "\n",
    "udfGetWeekNumer = lambda dt: F.udf(dt.isocalendar()[1], IntegerType())\n",
    "# udfGetAvg = lambda x: F.udf(sum(x)/float(len(x)))\\n\\n\n",
    "\n",
    "df2 = (df.filter(df.price > 0)\n",
    "       .withColumn('routeCombKey', F.concat_ws('-', F.col('fromCity'),\n",
    "                                   F.col('toCity'),\n",
    "                                   F.col('stayDays'), F.col('depDate'),\n",
    "                                   F.col('timeline1_departureTime_time'),\n",
    "                                   F.col('timeline2_departureTime_time'),\n",
    "                                   F.col('timeline1_carrier_airlineCode'),\n",
    "                                   F.col('timeline2_carrier_airlineCode')))\n",
    "                                   # join_udf(F.col('timeline1_departureTime_time')),\n",
    "                                   # join_udf(F.col('timeline2_departureTime_time')),\n",
    "                                   # join_udf(F.col('timeline1_carrier_airlineCode')),\n",
    "                                   # join_udf(F.col('timeline2_carrier_airlineCode'))))\n",
    "       .withColumn('futureMinPrice', F.min(F.col('price')).over(w))\n",
    "       .withColumn('priceWillDrop', (F.col('price') - F.col('futureMinPrice') > threshold).cast('int'))\n",
    "       .withColumn('saving0', F.col('price') - F.col('futureMinPrice'))\n",
    "       .withColumn('temp0', F.lit(0))\n",
    "       .withColumn('saving', F.greatest('saving0', 'temp0'))\n",
    "       .drop('saving0', 'temp0')\n",
    "       .withColumn('leadTime', F.datediff(F.col('depDate'), F.col('searchDate')))\n",
    "       .withColumn('depWeekOfYear', F.weekofyear(F.col('depDate')))\n",
    "       .withColumn('retWeekOfYear', F.weekofyear(F.col('leg2_departureTime_date')))\n",
    "       .withColumn('depMonth', F.month(F.col('depDate')))\n",
    "       .withColumn('depDayOfMonth', F.dayofmonth(F.col('depDate')))\n",
    "       .withColumn('depDayOfYear', F.dayofyear(F.col('depDate')))\n",
    "       .withColumn('depDayOfWeek', F.date_format(F.col('depDate'), 'EEEE'))\n",
    "       .withColumn('retDayOfWeek', F.date_format(F.col('leg2_departureTime_date'), 'EEEE'))\n",
    "       .withColumn('searchDayOfWeek', F.date_format(F.col('searchDate'), 'EEEE'))\n",
    "       .withColumn('leg1_noOfTicketsLeft', \n",
    "                   F.when(F.col('leg1_carrierSummary_noOfTicketsLeft') > 0, \n",
    "                          F.col('leg1_carrierSummary_noOfTicketsLeft')).otherwise(99))\n",
    "        .withColumn('leg2_noOfTicketsLeft', \n",
    "                    F.when(F.col('leg2_carrierSummary_noOfTicketsLeft') > 0, \n",
    "                           F.col('leg2_carrierSummary_noOfTicketsLeft')).otherwise(99))\n",
    "       .withColumn('leg1_cabinClass_0', F.col('timeline1_carrier_cabinClass').getItem(0))\n",
    "       .withColumn('leg1_cabinClass_1', F.col('timeline1_carrier_cabinClass').getItem(1))\n",
    "       .withColumn('leg1_cabinClass_2', F.col('timeline1_carrier_cabinClass').getItem(2))\n",
    "       .withColumn('leg2_cabinClass_0', F.col('timeline2_carrier_cabinClass').getItem(0))\n",
    "       .withColumn('leg2_cabinClass_1', F.col('timeline2_carrier_cabinClass').getItem(1))\n",
    "       .withColumn('leg2_cabinClass_2', F.col('timeline2_carrier_cabinClass').getItem(2))\n",
    "       .select('price', 'priceWillDrop', 'futureMinPrice', 'saving',\n",
    "               'fromCity', 'toCity',\n",
    "               'searchDate',\n",
    "               'routeCombKey',\n",
    "               'leadTime', \n",
    "               'leg1_stops', 'leg2_stops',\n",
    "               'leg1_noOfTicketsLeft', 'leg2_noOfTicketsLeft',\n",
    "               'leg1_carrierSummary_airlineName', 'leg2_carrierSummary_airlineName',\n",
    "               'leg1_departureTime_hour', 'leg2_departureTime_hour',\n",
    "               'depWeekOfYear', 'depDayOfWeek','retWeekOfYear', 'retDayOfWeek',\n",
    "               'searchDayOfWeek',\n",
    "               'leg1_cabinClass_0', 'leg1_cabinClass_1', 'leg1_cabinClass_2',\n",
    "               'leg2_cabinClass_0', 'leg2_cabinClass_1', 'leg2_cabinClass_2',\n",
    "               'trip')\n",
    "      )\n",
    "\n",
    "df2.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 ms, sys: 0 ns, total: 4 ms\n",
      "Wall time: 71.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Slow!\n",
    "df3 = df2.filter(F.col('searchDate') <= F.lit('2017-06-23').cast(TimestampType()))\n",
    "df3 = df3.withColumn('randVar', F.round(F.rand()*200, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir /home/jovyan/.local/h2o_jar\n",
    "# !cp /home/jovyan/work/h2o-3.14.0.7/h2o.jar /home/jovyan/.local/h2o_jar\n",
    "# # H2O\n",
    "\n",
    "# # turn off proxy so that h2o can run properly\n",
    "# import os\n",
    "# # os.environ.pop(\"HTTP_PROXY\")\n",
    "\n",
    "# import h2o\n",
    "# h2o.init(nthreads = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o241.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 17 in stage 14.0 failed 1 times, most recent failure: Lost task 17.0 in stage 14.0 (TID 452, localhost, executor driver): java.io.IOException: No space left on device\n\tat java.io.FileOutputStream.writeBytes(Native Method)\n\tat java.io.FileOutputStream.write(FileOutputStream.java:326)\n\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:58)\n\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:205)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:158)\n\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:95)\n\tat java.io.DataOutputStream.writeInt(DataOutputStream.java:197)\n\tat org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$1.writeValue(UnsafeRowSerializer.scala:68)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:239)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:151)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2087)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:935)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:278)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2430)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2429)\n\tat org.apache.spark.sql.Dataset$$anonfun$55.apply(Dataset.scala:2837)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:2836)\n\tat org.apache.spark.sql.Dataset.count(Dataset.scala:2429)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.io.IOException: No space left on device\n\tat java.io.FileOutputStream.writeBytes(Native Method)\n\tat java.io.FileOutputStream.write(FileOutputStream.java:326)\n\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:58)\n\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:205)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:158)\n\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:95)\n\tat java.io.DataOutputStream.writeInt(DataOutputStream.java:197)\n\tat org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$1.writeValue(UnsafeRowSerializer.scala:68)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:239)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:151)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/tmp/spark/work/spark-2dc743e4-ecce-4b95-8395-7b5c52d412b6/userFiles-b2e13367-8116-406a-96b2-62ca61b161f8/h2o_pysparkling_2.2-2.2.2.zip/pysparkling/context.py\u001b[0m in \u001b[0;36mas_h2o_frame\u001b[0;34m(self, dataframe, framename)\u001b[0m\n\u001b[1;32m    211\u001b[0m         \"\"\"\n\u001b[1;32m    212\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_as_h2o_frame_from_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0;31m# First check if the type T in RDD[T] is one of the python \"primitive\" types\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/spark/work/spark-2dc743e4-ecce-4b95-8395-7b5c52d412b6/userFiles-b2e13367-8116-406a-96b2-62ca61b161f8/h2o_pysparkling_2.2-2.2.2.zip/pysparkling/conversions.py\u001b[0m in \u001b[0;36m_as_h2o_frame_from_dataframe\u001b[0;34m(h2oContext, dataframe, frame_name)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_as_h2o_frame_from_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh2oContext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mdataframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cannot transform empty H2OFrame'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mj_h2o_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh2oContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jhc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masH2OFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    425\u001b[0m         \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m         \"\"\"\n\u001b[0;32m--> 427\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o241.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 17 in stage 14.0 failed 1 times, most recent failure: Lost task 17.0 in stage 14.0 (TID 452, localhost, executor driver): java.io.IOException: No space left on device\n\tat java.io.FileOutputStream.writeBytes(Native Method)\n\tat java.io.FileOutputStream.write(FileOutputStream.java:326)\n\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:58)\n\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:205)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:158)\n\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:95)\n\tat java.io.DataOutputStream.writeInt(DataOutputStream.java:197)\n\tat org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$1.writeValue(UnsafeRowSerializer.scala:68)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:239)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:151)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2087)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:935)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:278)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2430)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2429)\n\tat org.apache.spark.sql.Dataset$$anonfun$55.apply(Dataset.scala:2837)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:2836)\n\tat org.apache.spark.sql.Dataset.count(Dataset.scala:2429)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.io.IOException: No space left on device\n\tat java.io.FileOutputStream.writeBytes(Native Method)\n\tat java.io.FileOutputStream.write(FileOutputStream.java:326)\n\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:58)\n\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:205)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:158)\n\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:95)\n\tat java.io.DataOutputStream.writeInt(DataOutputStream.java:197)\n\tat org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$1.writeValue(UnsafeRowSerializer.scala:68)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:239)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:151)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "hf = hc.as_h2o_frame(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf.head(5)\n",
    "\n",
    "hf.describe()\n",
    "\n",
    "hf.types\n",
    "\n",
    "hf['leg1_carrierSummary_airlineName'].levels()\n",
    "\n",
    "hf['depDayOfWeek'].levels()\n",
    "\n",
    "# hf['JV_X_Excess'] = hf.interaction(['JV Description', 'Destination'], pairwise=False, max_factors=10, min_occurrence=500)\n",
    "\n",
    "hf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['price',\n",
    "         'fromCity',\n",
    "         'toCity', \n",
    "         'leadTime',\n",
    "         'leg1_stops',\n",
    "         'leg2_stops',\n",
    "         'leg1_noOfTicketsLeft',\n",
    "         'leg2_noOfTicketsLeft',\n",
    "         'leg1_carrierSummary_airlineName',\n",
    "         'leg2_carrierSummary_airlineName',\n",
    "         'leg1_departureTime_hour',\n",
    "         'leg2_departureTime_hour',\n",
    "         'depWeekOfYear',\n",
    "         'depDayOfWeek',\n",
    "         'retWeekOfYear',\n",
    "         'retDayOfWeek',\n",
    "         'searchDayOfWeek',\n",
    "         'leg1_cabinClass_0',\n",
    "         'leg1_cabinClass_1',\n",
    "         \n",
    "         'leg2_cabinClass_0',\n",
    "         'leg2_cabinClass_1',\n",
    "         \n",
    "         'trip'\n",
    "         ]\n",
    "\n",
    "target = 'priceWillDrop'\n",
    "\n",
    "other_cols = [ 'futureMinPrice', 'saving','searchDate', 'routeCombKey','randVar',\n",
    "             'leg1_cabinClass_2','leg2_cabinClass_2',]\n",
    "\n",
    "features, target\n",
    "\n",
    "hf.describe()\n",
    "\n",
    "hf[hf['leg1_cabinClass_1'].isna(), 'leg1_cabinClass_1'] = 0 #missing\n",
    "hf.describe()\n",
    "\n",
    "hf[target] = hf[target].asfactor()\n",
    "\n",
    "# target = targets[0]\n",
    "# target\n",
    "\n",
    "# hf2 = hf[targets + features2]\n",
    "\n",
    "# # hf2[hf2[target] <= 0, target]=0\n",
    "# for target in targets:\n",
    "#     hf2[hf2[target] > 0, target]=1\n",
    "#     hf2[hf2[target] <= 0, target]=0\n",
    "#     hf2[target] = hf2[target].asfactor()\n",
    "# # for target in targets:\n",
    "# #     hf2[hf2[target] <= 0, target]=0\n",
    "\n",
    "# hf2[hf2['Lead Time'] < 0, 'Lead Time']=0\n",
    "# hf2[hf2['Lead Time'] > 366, 'Lead Time']=366\n",
    "# hf2[hf2['Trip Length'] < 0, 'Trip Length']=0\n",
    "# hf2[hf2['Trip Length'] > 366, 'Trip Length']=366\n",
    "# hf2['excess'] = hf2['excess'].asfactor()\n",
    "# hf2[hf2['oldest age'] < 15, 'oldest age']=15\n",
    "# hf2[hf2['oldest age'] >= 90, 'oldest age']=90\n",
    "# hf2[hf2['Traveller Count'] >= 3, 'Traveller Count']=3\n",
    "\n",
    "df['searchDate'].value_counts()\n",
    "\n",
    "\n",
    "import datetime\n",
    "\n",
    "train=hf[hf['searchDate']<=datetime.datetime(2017, 6, 19, 0, 0, 0), :]\n",
    "valid=hf[(hf['searchDate']>datetime.datetime(2017, 6, 19, 0, 0, 0)) & \n",
    "         (hf['searchDate']<=datetime.datetime(2017, 6, 21, 0, 0, 0)), :]\n",
    "test=hf[hf['searchDate']>datetime.datetime(2017, 6, 21, 0, 0, 0), :]\n",
    "\n",
    "train.shape, test.shape\n",
    "\n",
    "train.nrow, valid.nrow, test.nrow\n",
    "\n",
    "# splits = hf.split_frame(ratios=[0.7, 0.15], seed=1)  \n",
    "\n",
    "# train = splits[0]\n",
    "# valid = splits[1]\n",
    "# test = splits[2]\n",
    "\n",
    "# gbm_regressor.train(x=features, y=targets[0], training_frame=train)\n",
    "\n",
    "# gbm_regressor.model_performance(test)\n",
    "\n",
    "%%time\n",
    "\n",
    "from h2o.estimators.gbm import H2OGradientBoostingEstimator\n",
    "from h2o.automl import H2OAutoML\n",
    "\n",
    "# print(\"############### Modelling: \" + target + \" ################\")\n",
    "gbm = H2OGradientBoostingEstimator(distribution=\"bernoulli\", \n",
    "                                             ntrees=1, max_depth=1, min_rows=1000, learn_rate=1,\n",
    "                                                stopping_rounds=10,\n",
    "                                            stopping_tolerance=0.01, seed=0)\n",
    "gbm.train(x=features, y=target,\n",
    "          training_frame = train,\n",
    "          validation_frame = valid)\n",
    "gbm\n",
    "\n",
    "\n",
    "# Run AutoML for 30 seconds\n",
    "# aml = H2OAutoML(max_runtime_secs = 3600)\n",
    "# aml.train(x=features, y=target,\n",
    "#           training_frame = train,\n",
    "#           validation_frame = valid,\n",
    "#           leaderboard_frame = test)\n",
    "\n",
    "# # View the AutoML Leaderboard\n",
    "# lb = aml.leaderboard\n",
    "# lb\n",
    "# # # aml.leader    \n",
    "\n",
    "aml.leader\n",
    "\n",
    "important_features\n",
    "\n",
    "hf[important_features]\n",
    "\n",
    "%%time\n",
    "\n",
    "imp_threshold = 0.01\n",
    "\n",
    "print(\"############### Modelling: \" + target + \" ################\")\n",
    "model = aml.leader\n",
    "print(model.confusion_matrix(valid=True))    \n",
    "model.varimp_plot()   \n",
    "important_features = [i[0] for i in model.varimp() if i[3]  > imp_threshold]\n",
    "for f in important_features:        \n",
    "    if f in ['price',\n",
    "#              'leg1_carrierSummary_airlineName',\n",
    "             'depWeekOfYear',\n",
    "             'leadTime',\n",
    "#              'leg2_carrierSummary_airlineName',\n",
    "             'depDayOfWeek',\n",
    "             'leg1_departureTime_hour',\n",
    "             'toCity',\n",
    "             'leg2_departureTime_hour',\n",
    "             'searchDayOfWeek',\n",
    "             'leg1_noOfTicketsLeft',\n",
    "             'leg2_noOfTicketsLeft',\n",
    "             'fromCity',\n",
    "             'leg1_stops',\n",
    "             'trip']:\n",
    "#             model.partial_plot(data=valid,cols=[f],server=True, plot=True, nbins=272) \n",
    "#         else:\n",
    "        model.partial_plot(data=test,cols=[f],server=True, plot=True, nbins=21)     #21    \n",
    "model.plot()\n",
    "\n",
    "model.download_mojo(path='D:\\\\flight.pq.11\\\\', get_genmodel_jar=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

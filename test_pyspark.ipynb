{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resources list**\n",
    "1. pyspark tutorial https://github.com/pydatasg/Pydata_meetup_Nov_16/blob/master/RF_modeling_2.3_business_weimin.ipynb\n",
    "2. follow this tutorial: https://www.analyticsvidhya.com/blog/2016/10/spark-dataframe-and-operations/\n",
    "3. and this one: https://www.dezyre.com/apache-spark-tutorial/pyspark-tutorial\n",
    "4. data engineer trick: http://nadbordrozd.github.io/blog/2016/05/22/one-weird-trick-that-will-fix-your-pyspark-schemas/\n",
    "5. git http://rogerdudler.github.io/git-guide/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get data\n",
    "## Downloand data from s3\n",
    "version 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://flight.price/flight_1_5/flight_1_5_price_2017-04-14.zip to ./flight_1_5_price_2017-04-14.zip\n",
      "download: s3://flight.price/flight_1_5/flight_1_5_price_2017-04-04.zip to ./flight_1_5_price_2017-04-04.zip\n",
      "download: s3://flight.price/flight_1_5/flight_1_5_price_2017-04-15.zip to ./flight_1_5_price_2017-04-15.zip\n",
      "download: s3://flight.price/flight_1_5/flight_1_5_price_2017-04-18.zip to ./flight_1_5_price_2017-04-18.zip\n",
      "download: s3://flight.price/flight_1_5/flight_1_5_price_2017-04-16.zip to ./flight_1_5_price_2017-04-16.zip\n",
      "download: s3://flight.price/flight_1_5/flight_1_5_price_2017-04-19.zip to ./flight_1_5_price_2017-04-19.zip\n",
      "download: s3://flight.price/flight_1_5/flight_1_5_price_2017-04-20.zip to ./flight_1_5_price_2017-04-20.zip\n",
      "download: s3://flight.price/flight_1_5/flight_1_5_price_2017-04-21.zip to ./flight_1_5_price_2017-04-21.zip\n",
      "download: s3://flight.price/flight_1_5/flight_1_5_price_2017-04-22.zip to ./flight_1_5_price_2017-04-22.zip\n",
      "download: s3://flight.price/flight_1_5/flight_1_5_price_2017-04-09.zip to ./flight_1_5_price_2017-04-09.zip\n",
      "download: s3://flight.price/flight_1_5/flight_1_5_price_2017-04-13.zip to ./flight_1_5_price_2017-04-13.zip\n",
      "download: s3://flight.price/flight_1_5/flight_1_5_price_2017-04-25.zip to ./flight_1_5_price_2017-04-25.zip\n",
      "download: s3://flight.price/flight_1_5/flight_1_5_price_2017-04-26.zip to ./flight_1_5_price_2017-04-26.zip\n",
      "download: s3://flight.price/flight_1_5/flight_1_5_price_2017-04-27.zip to ./flight_1_5_price_2017-04-27.zip\n",
      "download: s3://flight.price/flight_1_5/flight_1_5_price_2017-04-12.zip to ./flight_1_5_price_2017-04-12.zip\n",
      "download: s3://flight.price/flight_1_5/flight_1_5_price_2017-04-29.zip to ./flight_1_5_price_2017-04-29.zip\n",
      "download: s3://flight.price/flight_1_5/flight_1_5_price_2017-04-30.zip to ./flight_1_5_price_2017-04-30.zip\n",
      "download: s3://flight.price/flight_1_5/flight_1_5_price_2017-05-01.zip to ./flight_1_5_price_2017-05-01.zip\n",
      "download: s3://flight.price/flight_1_5/flight_1_5_price_2017-05-02.zip to ./flight_1_5_price_2017-05-02.zip\n",
      "download: s3://flight.price/flight_1_5/flight_1_5_price_2017-05-03.zip to ./flight_1_5_price_2017-05-03.zip\n",
      "download: s3://flight.price/flight_1_5/flight_1_5_price_2017-04-06.zip to ./flight_1_5_price_2017-04-06.zip\n",
      "download: s3://flight.price/flight_1_5/flight_1_5_price_2017-05-07.zip to ./flight_1_5_price_2017-05-07.zip\n",
      "download: s3://flight.price/flight_1_5/flight_1_5_price_2017-05-08.zip to ./flight_1_5_price_2017-05-08.zip\n",
      "download: s3://flight.price/flight_1_5/flight_1_5_price_2017-05-09.zip to ./flight_1_5_price_2017-05-09.zip\n",
      "download: s3://flight.price/flight_1_5/flight_1_5_price_2017-05-10.zip to ./flight_1_5_price_2017-05-10.zip\n",
      "download: s3://flight.price/flight_1_5/flight_1_5_price_2017-04-28.zip to ./flight_1_5_price_2017-04-28.zip\n",
      "download: s3://flight.price/flight_1_5/flight_1_5_price_2017-04-24.zip to ./flight_1_5_price_2017-04-24.zip\n",
      "download: s3://flight.price/flight_1_5/flight_1_5_price_2017-04-23.zip to ./flight_1_5_price_2017-04-23.zip\n",
      "download: s3://flight.price/flight_1_5/flight_1_5_price_2017-05-06.zip to ./flight_1_5_price_2017-05-06.zip\n",
      "download: s3://flight.price/flight_1_5/flight_1_5_price_2017-05-11.zip to ./flight_1_5_price_2017-05-11.zip\n"
     ]
    }
   ],
   "source": [
    "! cd /home/ubuntu/s3/flight_1_5\n",
    "! aws s3 sync s3://flight.price/flight_1_5 . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "version 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-05-11 12:21:21   51976702 flight_1_5/flight_1_5_price_2017-05-11.zip\n",
      "2017-05-12 11:41:26  488154487 flight_1_5/flight_1_5_price_2017-05-12.zip\n",
      "2017-05-13 11:12:53  488920458 flight_1_5/flight_1_5_price_2017-05-13.zip\n",
      "2017-05-14 23:50:24  489340830 flight_1_5/flight_1_5_price_2017-05-14.zip\n",
      "download: s3://flight.price.11/flight_1_5/flight_1_5_price_2017-05-11.zip to ./flight_1_5_price_2017-05-11.zip\n",
      "download: s3://flight.price.11/flight_1_5/flight_1_5_price_2017-05-14.zip to ./flight_1_5_price_2017-05-14.zip\n",
      "download: s3://flight.price.11/flight_1_5/flight_1_5_price_2017-05-13.zip to ./flight_1_5_price_2017-05-13.zip\n",
      "download: s3://flight.price.11/flight_1_5/flight_1_5_price_2017-05-12.zip to ./flight_1_5_price_2017-05-12.zip\n"
     ]
    }
   ],
   "source": [
    "! aws s3 ls s3://flight.price.11/flight_1_5 --recursive\n",
    "! aws s3 sync s3://flight.price.11/flight_1_5 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0K\t./spark-warehouse\r\n",
      "1.5G\t./extracted/final_results\r\n",
      "1.6G\t./extracted\r\n",
      "4.5G\t.\r\n"
     ]
    }
   ],
   "source": [
    "! du -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract\n",
    "Note: version 1.1 from 2017-05-11 onwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "# dir_in = 'C:\\\\s3\\\\20170503\\\\flight_1_6' # sydney to shanghai\n",
    "# dir_in = 'C:\\\\s3\\\\20170503\\\\flight_5_1' # beijing to sydney\n",
    "# dir_in = 'C:\\\\s3\\\\20170503'\n",
    "dir_in = '/home/ubuntu/s3/flight_1_5'\n",
    "\n",
    "# dir_out = 'C:\\\\s3\\\\20170503_extracted\\\\flight_1_6'\n",
    "dir_out = '/home/ubuntu/s3/flight_1_5/extracted'\n",
    "extension = \".zip\"\n",
    "\n",
    "os.chdir(dir_in) # change directory from working dir to dir with files\n",
    "\n",
    "for subdir, dirs, files in os.walk(dir_in):\n",
    "    for item in files:\n",
    "        if item.endswith(extension): # check for \".zip\" extension\n",
    "            file_name = os.path.join(subdir, item)\n",
    "#             file_name = os.path.abspath(item) # get full path of files\n",
    "            zip_ref = zipfile.ZipFile(file_name) # create zipfile object\n",
    "            zip_ref.extractall(dir_out) # extract file to dir\n",
    "            zip_ref.close() # close file  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move files in final_results to the main folder and delete the folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ! cd /home/ubuntu/s3/flight_1_5/extracted\n",
    "# ! mv /home/ubuntu/s3/flight_1_5/extracted/final_results/*.txt .\n",
    "! find /home/ubuntu/s3/flight_1_5/extracted/final_results -name '*.txt' -exec mv {} /home/ubuntu/s3/flight_1_5/extracted \\;\n",
    "# -exec runs any command,  {} inserts the filename found, \\; marks the end of the exec command.\n",
    "! rmdir /home/ubuntu/s3/flight_1_5/extracted/final_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://stackoverflow.com/questions/9157138/recursively-counting-files-in-a-linux-directory\n",
    "* -type f to include only files.  \n",
    "* | (and not Â¦) redirects find command's standard output to wc command's standard input.  \n",
    "* wc (short for word count) counts newlines, words and bytes on its input (docs).  \n",
    "* -l to count just newlines.\n",
    "* You can also remove the -type f to include directories (and symlinks) in the count.\n",
    "* It's possible this command will overcount if filenames can contain newline characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26261\n",
      "5.9G\t.\n"
     ]
    }
   ],
   "source": [
    "! find /home/ubuntu/s3/flight_1_5/extracted -type f | wc -l\n",
    "! du -sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Launch spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import h2o\n",
    "# import pysparkling\n",
    "import os\n",
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "from IPython.display import display\n",
    "from pyspark.sql.functions import regexp_extract, col, split, udf, trim, when, from_unixtime, unix_timestamp, minute, hour, datediff\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import IntegerType, StringType, BooleanType\n",
    "# from pyspark.sql.types import *\n",
    "import datetime\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext()\n",
    "\n",
    "# # # spark_home = os.environ.get('SPARK_HOME', None)\n",
    "# spark_home = \"C:\\spark-2.1.0-bin-hadoop2.7\\spark-2.1.0-bin-hadoop2.7\"\n",
    "\n",
    "# if not spark_home:\n",
    "\n",
    "#     raise ValueError('SPARK_HOME environment variable is not set')\n",
    "\n",
    "# sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "\n",
    "# sys.path.insert(0, os.path.join(spark_home, 'C:\\spark-2.1.0-bin-hadoop2.7\\spark-2.1.0-bin-hadoop2.7\\python\\lib\\py4j-0.10.4-src.zip')) ## may need to adjust on your system depending on which Spark version you're using and where you installed it.\n",
    "\n",
    "# exec(open(os.path.join(spark_home, 'python/pyspark/shell.py')).read())\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"Python Spark SQL basic example\") \\\n",
    "        .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "        .getOrCreate()\n",
    "        \n",
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# hc= H2OContext(sc).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# h2o not working yet\n",
    "# # Start H2O Context\n",
    "# from pysparkling import *\n",
    "# sc\n",
    "# hc= H2OContext(sc).start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in json files and aggregate into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# Copyright (C) 2015-2025 Wang,Jing   <jingwangian@gmail.com>\n",
    "#\n",
    "# This file is part of Flight Inforation Query System (fiqs)\n",
    "#\n",
    "# fiqs is free software: you can redistribute it and/or modify\n",
    "# it under the terms of the GNU General Public License as published by\n",
    "# the Free Software Foundation, either version 3 of the License, or\n",
    "# (at your option) any later version.\n",
    "#\n",
    "# fiqs is distributed in the hope that it will be useful,\n",
    "# but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "# GNU General Public License for more details.\n",
    "#\n",
    "# You should have received a copy of the GNU General Public License\n",
    "# along with fiqs.  If not, see <http://www.gnu.org/licenses/>.\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# def spark_write(spark,jsonl_file_name,parquet_file_name):\n",
    "#     df = spark.read.json(jsonl_file_name)\n",
    "#     df.write.parquet(parquet_file_name)\n",
    "    \n",
    "# def spark_print_json(spark,json_file_name):\n",
    "#     d = read_from_json_file(json_file_name)\n",
    "    \n",
    "#     jsonl_file_name = json_file_name.replace('.json','.jsonl')\n",
    "    \n",
    "#     with open(jsonl_file_name,'w') as f:\n",
    "#         json.dump(d, f)\n",
    "    \n",
    "#     df = spark.read.json(jsonl_file_name)\n",
    "    \n",
    "#     df.createGlobalTempView(\"flight\")\n",
    "    \n",
    "#     spark.sql(\"SELECT fromCity,toCity,flight_leg1.departureTime, flight_leg1.departureLocation.airportLongName FROM global_temp.flight\").show()\n",
    "    \n",
    "def read_from_json_file(filename):\n",
    "    with open(filename,'r') as f:\n",
    "        d = json.load(f)\n",
    "        \n",
    "    return d\n",
    "\n",
    "# def pq_to_json_file(spark, pq_file_name, json_file_name):\n",
    "#     df = spark.read.load(pq_file_name)\n",
    "    \n",
    "#     df.write.json(json_file_name)\n",
    "    \n",
    "# def read_data_from_pq_file(spark,filename):\n",
    "#     df = spark.read.load(filename)\n",
    "    \n",
    "#     df.select('from_city_name','to_city_name').show()\n",
    "#     #spark.sql(\"SELECT from_city_name,to_city_name FROM global_temp.flight\").show()\n",
    "    \n",
    "# def main():\n",
    "#     full_path_pq_file_name = \"test/t.parquet\"\n",
    "\n",
    "#     spark = SparkSession \\\n",
    "#         .builder \\\n",
    "#         .appName(\"Python Spark SQL basic example\") \\\n",
    "#         .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "#         .getOrCreate()\n",
    "            \n",
    "#     parser = argparse.ArgumentParser()\n",
    "    \n",
    "#     parser.add_argument('cmd',help='[json,pq,json2pq,jsonprint]')\n",
    "#     parser.add_argument('--json',help='json file namejson,pq]')\n",
    "#     parser.add_argument('--pq',help='parquet file name')\n",
    "    \n",
    "#     args = parser.parse_args()\n",
    "    \n",
    "#     cmd_ind = args.cmd\n",
    "#     json_file_name = args.json\n",
    "#     pq_file_name = args.pq\n",
    "    \n",
    "#     if cmd_ind == 'json2pq':\n",
    "#         if json_file_name is None:\n",
    "#             print(\"Please set the json file name by --json\")\n",
    "#             quit(0)\n",
    "            \n",
    "#         if pq_file_name is None:\n",
    "#             print(\"Please set the pqrquet file name by --pq\")\n",
    "#             quit(0)\n",
    "#         d = read_from_json_file(json_file_name)\n",
    "        \n",
    "#         jsonl_file_name = json_file_name.replace('.json','.jsonl')\n",
    "        \n",
    "#         with open(jsonl_file_name,'w') as f:\n",
    "#             json.dump(d, f)\n",
    "            \n",
    "#         spark_write(spark, jsonl_file_name, pq_file_name)\n",
    "#     elif cmd_ind == 'pq':\n",
    "#         if pq_file_name is None:\n",
    "#             print(\"Please set the pqrquet file name by --pq\")\n",
    "#             quit(0)\n",
    "#         read_data_from_pq_file(spark,full_path_pq_file_name)\n",
    "#     elif cmd_ind == 'pq2json':\n",
    "#         if json_file_name is None:\n",
    "#             print(\"Please set the json file name by --json\")\n",
    "#             quit(0)\n",
    "            \n",
    "#         if pq_file_name is None:\n",
    "#             print(\"Please set the pqrquet file name by --pq\")\n",
    "#             quit(0)\n",
    "            \n",
    "#         pq_to_json_file(spark,pq_file_name,json_file_name)\n",
    "        \n",
    "#     elif cmd_ind== 'jsonprint':\n",
    "#         if json_file_name is None:\n",
    "#             print(\"Please set the json file name by --json\")\n",
    "#             quit(0)\n",
    "            \n",
    "#         spark_print_json(spark,json_file_name)\n",
    "#     else:\n",
    "#         print('invalid cmd command')\n",
    "        \n",
    "#     spark.stop()\n",
    "    \n",
    "    \n",
    "# if __name__== '__main__':\n",
    "#     main()\n",
    "    \n",
    "    \n",
    "# spark-submit sp.py jsonprint --json final_flight_result_format_v1.1.json\n",
    "\n",
    "\n",
    "# json_file_name = \"D:\\\\Data Science\\\\pySpark\\\\test_schema\\\\final_flight_result_format_v1.1.json\"\n",
    "# jsonl_file_name = \"C:\\\\s3\\\\20170503_jsonl\\\\flight_1_6.jsonl\"\n",
    "jsonl_file_name = \"C:\\\\s3\\\\a3.json\"\n",
    "\n",
    "# d = read_from_json_file(json_file_name)\n",
    "\n",
    "# jsonl_file_name = json_file_name.replace('.json','.jsonl')\n",
    "\n",
    "# with open(jsonl_file_name,'w') as f:\n",
    "#     json.dump(d, f)\n",
    "\n",
    "# flight = spark.read.json(jsonl_file_name)\n",
    "\n",
    "# df.createGlobalTempView(\"flight\")\n",
    "\n",
    "# spark.sql(\"SELECT fromCity,toCity,flight_leg1.departureTime, flight_leg1.departureLocation.airportLongName FROM global_temp.flight\").show()\n",
    "  \n",
    "# flight.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# flight.printSchema()\n",
    "# flight.drop('airline_codes').repartition(1).write.format('com.databricks.spark.csv').save('C:\\\\s3\\\\20170503_jsonl\\\\flight_1_6.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# flight.write.parquet(\"C:\\\\s3\\\\20170503_jsonl\\\\flight.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# flight = spark.read.parquet(\"C:\\\\s3\\\\20170503_jsonl\\\\flight.parquet\")\n",
    "flight = spark.read.parquet(\"/home/ubuntu/parquet/flight.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1862553"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flight.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- airline_code: string (nullable = true)\n",
      " |-- airline_codes: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- arr_time: string (nullable = true)\n",
      " |-- check_bag_inc: boolean (nullable = true)\n",
      " |-- company: string (nullable = true)\n",
      " |-- dep_time: string (nullable = true)\n",
      " |-- duration: string (nullable = true)\n",
      " |-- flight_code: string (nullable = true)\n",
      " |-- flight_number: string (nullable = true)\n",
      " |-- from_city_name: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- index: long (nullable = true)\n",
      " |-- plane: string (nullable = true)\n",
      " |-- power: boolean (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- price_code: string (nullable = true)\n",
      " |-- search_date: string (nullable = true)\n",
      " |-- span_days: long (nullable = true)\n",
      " |-- start_date: string (nullable = true)\n",
      " |-- stay_days: string (nullable = true)\n",
      " |-- stop: long (nullable = true)\n",
      " |-- stop_info: string (nullable = true)\n",
      " |-- table_name: string (nullable = true)\n",
      " |-- task_id: string (nullable = true)\n",
      " |-- ticket_left: long (nullable = true)\n",
      " |-- to_city_name: string (nullable = true)\n",
      " |-- trip: string (nullable = true)\n",
      " |-- version: string (nullable = true)\n",
      " |-- video: boolean (nullable = true)\n",
      " |-- wifi: boolean (nullable = true)\n",
      "\n",
      "+------------+-------------+-----------------------------+-------------+----------------+-----------------------------+--------+-----------+-------------+--------------+----+-----+-------------------------+-----+-------+----------+-----------+---------+----------+---------+----+-----------------+----------------+-------+-----------+------------+----+-------+-----+-----+\n",
      "|airline_code|airline_codes|arr_time                     |check_bag_inc|company         |dep_time                     |duration|flight_code|flight_number|from_city_name|id  |index|plane                    |power|price  |price_code|search_date|span_days|start_date|stay_days|stop|stop_info        |table_name      |task_id|ticket_left|to_city_name|trip|version|video|wifi |\n",
      "+------------+-------------+-----------------------------+-------------+----------------+-----------------------------+--------+-----------+-------------+--------------+----+-----+-------------------------+-----+-------+----------+-----------+---------+----------+---------+----+-----------------+----------------+-------+-----------+------------+----+-------+-----+-----+\n",
      "|QF          |[QF]         |2017-04-05T19:20:00.000+08:00|false        |Qantas Airways  |2017-04-05T11:00:00.000+10:00|10h20m  |QF301      |301          |sydney        |null|12   |AIRBUS INDUSTRIE A330-200|false|1178.42|AUD       |2017-04-04 |0        |2017-04-05|0        |0   |                 |flight_1_6_price|901    |9          |shanghai    |1   |1.0    |true |false|\n",
      "|VN          |[VN, VN]     |2017-04-06T14:25:00.000+08:00|false        |Vietnam Airlines|2017-04-05T14:15:00.000+10:00|26h10m  |VN786      |786          |sydney        |null|69   |Boeing 787               |false|1095.22|AUD       |2017-04-04 |0        |2017-04-05|0        |1   |Hanoi(HAN):13h35m|flight_1_6_price|901    |4          |shanghai    |1   |1.0    |false|false|\n",
      "+------------+-------------+-----------------------------+-------------+----------------+-----------------------------+--------+-----------+-------------+--------------+----+-----+-------------------------+-----+-------+----------+-----------+---------+----------+---------+----+-----------------+----------------+-------+-----------+------------+----+-------+-----+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1862553"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "flight.printSchema()\n",
    "flight.show(2,truncate=False)\n",
    "display(flight.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# path_to_input = \"D:\\Data Science\\pySpark\\original_json\"\n",
    "# original_data = spark.read.json(sc.wholeTextFiles(path_to_input).values())\n",
    "\n",
    "# original_data = sqlContext.read.json(\"D:\\\\Data Science\\\\pySpark\\\\original_json\\\\original_flight_info.txt\")\n",
    "# original_data.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import json\n",
    "# from pprint import pprint\n",
    "\n",
    "# # with open(\"D:\\\\Data Science\\\\pySpark\\\\original_json\\\\original_flight_info.txt\") as data_file:    \n",
    "# #     data = json.load(data_file)\n",
    "\n",
    "# # pprint(data)\n",
    "# # # data.head()\n",
    "\n",
    "# df = spark.read.json(\"D:\\\\Data Science\\\\Flight v2\\\\python\\\\final_flight_result_format_v1.1.json\")\n",
    "# # Displays the content of the DataFrame to stdout\n",
    "# df.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# flight = sc.textFile(\"C:\\\\s3_temp\\\\summary\\\\flat_table.csv\").map(lambda line: (line.split(',')[5], line.split(',')[10])).collect()\n",
    "# flight = sc.textFile(\"C:\\\\s3_temp\\\\summary\\\\flat_table.csv\")\n",
    "# flight = spark.read.csv(\"C:\\\\s3_temp\\\\summary\\\\flat_table.csv\", header=True, mode=\"DROPMALFORMED\")\n",
    "\n",
    "# flight.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# flight.repartition(1).write.format('com.databricks.spark.csv').save('D://Data Science//pySpark//test2.csv', header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(airline_code='QF', airline_codes=['QF'], arr_time='2017-04-05T19:20:00.000+08:00', check_bag_inc=False, company='Qantas Airways', dep_time='2017-04-05T11:00:00.000+10:00', duration='10h20m', flight_code='QF301', flight_number='301', from_city_name='sydney', id=None, index=12, plane='AIRBUS INDUSTRIE A330-200', power=False, price=1178.42, price_code='AUD', search_date='2017-04-04', span_days=0, start_date='2017-04-05', stay_days='0', stop=0, stop_info='', table_name='flight_1_6_price', task_id='901', ticket_left=9, to_city_name='shanghai', trip='1', version='1.0', video=True, wifi=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flight.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# flight.rdd.takeSample(False, 3, 1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('airline_code', 'string'),\n",
       " ('airline_codes', 'array<string>'),\n",
       " ('arr_time', 'string'),\n",
       " ('check_bag_inc', 'boolean'),\n",
       " ('company', 'string'),\n",
       " ('dep_time', 'string'),\n",
       " ('duration', 'string'),\n",
       " ('flight_code', 'string'),\n",
       " ('flight_number', 'string'),\n",
       " ('from_city_name', 'string'),\n",
       " ('id', 'string'),\n",
       " ('index', 'bigint'),\n",
       " ('plane', 'string'),\n",
       " ('power', 'boolean'),\n",
       " ('price', 'double'),\n",
       " ('price_code', 'string'),\n",
       " ('search_date', 'string'),\n",
       " ('span_days', 'bigint'),\n",
       " ('start_date', 'string'),\n",
       " ('stay_days', 'string'),\n",
       " ('stop', 'bigint'),\n",
       " ('stop_info', 'string'),\n",
       " ('table_name', 'string'),\n",
       " ('task_id', 'string'),\n",
       " ('ticket_left', 'bigint'),\n",
       " ('to_city_name', 'string'),\n",
       " ('trip', 'string'),\n",
       " ('version', 'string'),\n",
       " ('video', 'boolean'),\n",
       " ('wifi', 'boolean')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flight.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import year, month, dayofmonth, hour, minute, weekofyear, crosstab\n",
    "# flight2.select(year(\"arr_time\").alias('year'), \n",
    "# from pyspark.sql.functions import regexp_extract, col, split\n",
    "# # import re\n",
    "\n",
    "# from pyspark.sql.functions import udf\n",
    "# from pyspark.sql.types import IntegerType, StringType, BooleanType\n",
    "\n",
    "def toBool(aString):\n",
    "    if aString.lower() == 'true':\n",
    "        return True\n",
    "    else: \n",
    "        return False\n",
    " \n",
    "udfToBool = udf(toBool, BooleanType())\n",
    "\n",
    "\n",
    "def groupTicketLeft(aString):\n",
    "    if aString == '0':\n",
    "        return 'plenty'\n",
    "    else: \n",
    "        return aString\n",
    " \n",
    "udfGroupTicketLeft = udf(groupTicketLeft, StringType())\n",
    "\n",
    "# def getMinutes(aString):\n",
    "#     return minute(unix_timestamp(aString, \"HH'h'mm'm'\").cast(\"timestamp\"))\n",
    "\n",
    "# udfGetMinutes = udf(getMinutes, IntegerType())\n",
    "\n",
    "# def getHours(x):\n",
    "#   return re.match('([0-9]+(?=h))', x)\n",
    "# getHours(\"14h\")\n",
    "# len(flight.columns)\n",
    "# flight.columns\n",
    "# flight.dtypes\n",
    "flight2 = (flight.withColumn('search_date', flight.search_date.cast('timestamp'))\n",
    "#           .withColumn('search_date_y', flight.search_date_y.cast('timestamp'))\n",
    "#           .withColumnRenamed('search_date_y', 'search_date')\n",
    "          .withColumn('stay_days', flight.stay_days.cast('int'))                                \n",
    "           #create local time first\n",
    "           .withColumn('dep_time_local', flight.dep_time.substr(1, 23).cast('timestamp'))                      \n",
    "          .withColumn('dep_time', flight.dep_time.cast('timestamp'))\n",
    "           #create local time first\n",
    "          .withColumn('arr_time_local', flight.arr_time.substr(1, 23).cast('timestamp'))                      \n",
    "          .withColumn('arr_time', flight.arr_time.cast('timestamp'))\n",
    "           #duration           \n",
    "           .withColumn('duration_h',split(flight.duration,'h').getItem(0))\n",
    "           .withColumn('duration_m',split(flight.duration,'h').getItem(1))\n",
    "           #stop info\n",
    "           .withColumn('stop_info1',split(flight.stop_info,';').getItem(0))\n",
    "           .withColumn('stop_info2',split(flight.stop_info,';').getItem(1))\n",
    "#            .withColumn('druation_hours', \n",
    "#                        flight.selectExpr(\"duration\", \"regexp_extract(duration,'([0-9]+(?=h))', 1) as duration_hours\")\n",
    "#                        .duration_hours\n",
    "#                        .cast('int'))\n",
    "          .withColumn('start_date', flight.start_date.cast('date'))           \n",
    "          .withColumn('price', flight.price.cast('double'))          \n",
    "           # to correct the name\n",
    "          .withColumnRenamed('check_bag_inc', 'check_bag_not_inc')\n",
    "#           .withColumn('check_bag_not_inc', udfToBool(flight.check_bag_inc))\n",
    "          .withColumn('power', flight.power.cast('boolean'))           \n",
    "          .withColumn('video', flight.video.cast('boolean'))\n",
    "          .withColumn('wifi', flight.wifi.cast('boolean'))           \n",
    "          .withColumn('stop', flight.stop.cast('int')) \n",
    "          .withColumn('span_days', flight.span_days.cast('int'))           \n",
    "           .withColumn('ticket_left', udfGroupTicketLeft(flight.ticket_left))\n",
    "           .drop('search_date_x', 'check_bag_inc')\n",
    "          )\n",
    "\n",
    "flight2 = (flight2.withColumn('stop_loc1',split(flight2.stop_info1,':').getItem(0))\n",
    "           .withColumn('stop_duration1',split(flight2.stop_info1, ':').getItem(1))\n",
    "           .withColumn('stop_loc2',split(flight2.stop_info2,':').getItem(0))\n",
    "           .withColumn('stop_duration2',split(flight2.stop_info2, ':').getItem(1)))\n",
    "\n",
    "flight2 = (flight2.withColumn('stop_duration_h1',split(flight2.stop_duration1,'h').getItem(0))\n",
    "           .withColumn('stop_duration_m1',split(flight2.stop_duration1,'h').getItem(1))\n",
    "           .withColumn('stop_duration_h2',split(flight2.stop_duration2,'h').getItem(0))\n",
    "           .withColumn('stop_duration_m2',split(flight2.stop_duration2,'h').getItem(1)))\n",
    "# .withColumn('arr_time_zone', flight.arr_time.substr(24, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+--------------------+-----------------+----------------+--------------------+--------+-----------+-------------+--------------+----+-----+--------------------+-----+-------+----------+--------------------+---------+----------+---------+----+-----------------+----------------+-------+-----------+------------+----+-------+-----+-----+--------------------+--------------------+----------+----------+-----------------+----------+----------+--------------+---------+--------------+----------------+----------------+----------------+----------------+----------------+-------------+-------------+\n",
      "|airline_code|airline_codes|            arr_time|check_bag_not_inc|         company|            dep_time|duration|flight_code|flight_number|from_city_name|  id|index|               plane|power|  price|price_code|         search_date|span_days|start_date|stay_days|stop|        stop_info|      table_name|task_id|ticket_left|to_city_name|trip|version|video| wifi|      dep_time_local|      arr_time_local|duration_h|duration_m|       stop_info1|stop_info2| stop_loc1|stop_duration1|stop_loc2|stop_duration2|stop_duration_h1|stop_duration_m1|stop_duration_h2|stop_duration_m2|duration_minutes|stop1_minutes|stop2_minutes|\n",
      "+------------+-------------+--------------------+-----------------+----------------+--------------------+--------+-----------+-------------+--------------+----+-----+--------------------+-----+-------+----------+--------------------+---------+----------+---------+----+-----------------+----------------+-------+-----------+------------+----+-------+-----+-----+--------------------+--------------------+----------+----------+-----------------+----------+----------+--------------+---------+--------------+----------------+----------------+----------------+----------------+----------------+-------------+-------------+\n",
      "|          QF|         [QF]|2017-04-05 21:20:...|            false|  Qantas Airways|2017-04-05 11:00:...|  10h20m|      QF301|          301|        sydney|null|   12|AIRBUS INDUSTRIE ...|false|1178.42|       AUD|2017-04-04 00:00:...|        0|2017-04-05|        0|   0|                 |flight_1_6_price|    901|          9|    shanghai|   1|    1.0| true|false|2017-04-05 11:00:...|2017-04-05 19:20:...|        10|       20m|                 |      null|          |          null|     null|          null|            null|            null|            null|            null|             620|         null|         null|\n",
      "|          VN|     [VN, VN]|2017-04-06 16:25:...|            false|Vietnam Airlines|2017-04-05 14:15:...|  26h10m|      VN786|          786|        sydney|null|   69|          Boeing 787|false|1095.22|       AUD|2017-04-04 00:00:...|        0|2017-04-05|        0|   1|Hanoi(HAN):13h35m|flight_1_6_price|    901|          4|    shanghai|   1|    1.0|false|false|2017-04-05 14:15:...|2017-04-06 14:25:...|        26|       10m|Hanoi(HAN):13h35m|      null|Hanoi(HAN)|        13h35m|     null|          null|              13|             35m|            null|            null|            1570|          815|         null|\n",
      "+------------+-------------+--------------------+-----------------+----------------+--------------------+--------+-----------+-------------+--------------+----+-----+--------------------+-----+-------+----------+--------------------+---------+----------+---------+----+-----------------+----------------+-------+-----------+------------+----+-------+-----+-----+--------------------+--------------------+----------+----------+-----------------+----------+----------+--------------+---------+--------------+----------------+----------------+----------------+----------------+----------------+-------------+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define udf\n",
    "\n",
    "def getMinutes(hString, minString):\n",
    "    if (hString is not None) & (minString is not None): return int(hString) * 60 + int(minString[:-1])\n",
    "    else: return None\n",
    "#     if score >= 80: return 'A'\n",
    "#     elif score >= 60: return 'B'\n",
    "#     elif score >= 35: return 'C'\n",
    "#     else: return 'D'\n",
    " \n",
    "udfGetMinutes = udf(getMinutes, IntegerType())\n",
    "flight2 = (flight2.withColumn(\"duration_minutes\", udfGetMinutes(\"duration_h\", \"duration_m\"))\n",
    "                    .withColumn(\"stop1_minutes\", udfGetMinutes(\"stop_duration_h1\", \"stop_duration_m1\"))\n",
    "                   .withColumn(\"stop2_minutes\", udfGetMinutes(\"stop_duration_h2\", \"stop_duration_m2\")))\n",
    "                 \n",
    "flight2.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o286.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 8.0 failed 1 times, most recent failure: Lost task 0.0 in stage 8.0 (TID 6, localhost, executor driver): java.lang.RuntimeException: file:/home/ubuntu/flight.parquet/part-00008-f2da479a-50b6-4ead-9188-97fc7359de20.snappy.parquet is not a Parquet file. expected magic number at tail [80, 65, 82, 49] but found [0, 33, 32, 33]\n\tat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:423)\n\tat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:386)\n\tat org.apache.spark.sql.execution.datasources.parquet.SpecificParquetRecordReaderBase.initialize(SpecificParquetRecordReaderBase.java:107)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.initialize(VectorizedParquetRecordReader.java:109)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$buildReader$1.apply(ParquetFileFormat.scala:381)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$buildReader$1.apply(ParquetFileFormat.scala:355)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:168)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:109)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.scan_nextBatch$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.agg_doAggregateWithKeys$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1925)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1951)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:333)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$execute$1$1.apply(Dataset.scala:2386)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)\n\tat org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2788)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$execute$1(Dataset.scala:2385)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collect(Dataset.scala:2392)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2128)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2127)\n\tat org.apache.spark.sql.Dataset.withTypedCallback(Dataset.scala:2818)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2127)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2342)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:248)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.RuntimeException: file:/home/ubuntu/flight.parquet/part-00008-f2da479a-50b6-4ead-9188-97fc7359de20.snappy.parquet is not a Parquet file. expected magic number at tail [80, 65, 82, 49] but found [0, 33, 32, 33]\n\tat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:423)\n\tat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:386)\n\tat org.apache.spark.sql.execution.datasources.parquet.SpecificParquetRecordReaderBase.initialize(SpecificParquetRecordReaderBase.java:107)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.initialize(VectorizedParquetRecordReader.java:109)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$buildReader$1.apply(ParquetFileFormat.scala:381)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$buildReader$1.apply(ParquetFileFormat.scala:355)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:168)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:109)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.scan_nextBatch$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.agg_doAggregateWithKeys$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-85fcb343e0a1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m                 \u001b[1;34m'stop_duration1'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'stop_duration_h1'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'stop_duration_m1'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m                 \u001b[1;34m'stop_info2'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'stop_loc2'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m                 'stop_duration2',  'stop_duration_h2', 'stop_duration_m2').count().show(500, truncate=False)\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;31m# flight2.groupby('stop_duration1').count().show()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[1;34m(self, n, truncate)\u001b[0m\n\u001b[0;32m    318\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 320\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    321\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu/anaconda3/lib/python3.5/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1133\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1135\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu/anaconda3/lib/python3.5/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    318\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 319\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    320\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o286.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 8.0 failed 1 times, most recent failure: Lost task 0.0 in stage 8.0 (TID 6, localhost, executor driver): java.lang.RuntimeException: file:/home/ubuntu/flight.parquet/part-00008-f2da479a-50b6-4ead-9188-97fc7359de20.snappy.parquet is not a Parquet file. expected magic number at tail [80, 65, 82, 49] but found [0, 33, 32, 33]\n\tat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:423)\n\tat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:386)\n\tat org.apache.spark.sql.execution.datasources.parquet.SpecificParquetRecordReaderBase.initialize(SpecificParquetRecordReaderBase.java:107)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.initialize(VectorizedParquetRecordReader.java:109)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$buildReader$1.apply(ParquetFileFormat.scala:381)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$buildReader$1.apply(ParquetFileFormat.scala:355)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:168)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:109)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.scan_nextBatch$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.agg_doAggregateWithKeys$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1925)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1951)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:333)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$execute$1$1.apply(Dataset.scala:2386)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)\n\tat org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2788)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$execute$1(Dataset.scala:2385)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collect(Dataset.scala:2392)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2128)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2127)\n\tat org.apache.spark.sql.Dataset.withTypedCallback(Dataset.scala:2818)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2127)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2342)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:248)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.RuntimeException: file:/home/ubuntu/flight.parquet/part-00008-f2da479a-50b6-4ead-9188-97fc7359de20.snappy.parquet is not a Parquet file. expected magic number at tail [80, 65, 82, 49] but found [0, 33, 32, 33]\n\tat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:423)\n\tat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:386)\n\tat org.apache.spark.sql.execution.datasources.parquet.SpecificParquetRecordReaderBase.initialize(SpecificParquetRecordReaderBase.java:107)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.initialize(VectorizedParquetRecordReader.java:109)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$buildReader$1.apply(ParquetFileFormat.scala:381)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$buildReader$1.apply(ParquetFileFormat.scala:355)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:168)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:109)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.scan_nextBatch$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.agg_doAggregateWithKeys$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "# flight2.groupby('search_date', 'from_city_name', 'to_city_name', 'trip').count().show()\n",
    "# flight2.groupby('table_name').count().show()\n",
    "flight2.groupby('stop_info', \n",
    "                'stop_info1', 'stop_loc1', \n",
    "                'stop_duration1', 'stop_duration_h1', 'stop_duration_m1',\n",
    "                'stop_info2', 'stop_loc2', \n",
    "                'stop_duration2',  'stop_duration_h2', 'stop_duration_m2').count().show(500, truncate=False)\n",
    "# flight2.groupby('stop_duration1').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('airline_code', 'string'),\n",
       " ('airline_codes', 'array<string>'),\n",
       " ('arr_time', 'timestamp'),\n",
       " ('check_bag_not_inc', 'boolean'),\n",
       " ('company', 'string'),\n",
       " ('dep_time', 'timestamp'),\n",
       " ('duration', 'string'),\n",
       " ('flight_code', 'string'),\n",
       " ('flight_number', 'string'),\n",
       " ('from_city_name', 'string'),\n",
       " ('id', 'string'),\n",
       " ('index', 'bigint'),\n",
       " ('plane', 'string'),\n",
       " ('power', 'boolean'),\n",
       " ('price', 'double'),\n",
       " ('price_code', 'string'),\n",
       " ('search_date', 'timestamp'),\n",
       " ('span_days', 'int'),\n",
       " ('start_date', 'date'),\n",
       " ('stay_days', 'int'),\n",
       " ('stop', 'int'),\n",
       " ('stop_info', 'string'),\n",
       " ('table_name', 'string'),\n",
       " ('task_id', 'string'),\n",
       " ('ticket_left', 'string'),\n",
       " ('to_city_name', 'string'),\n",
       " ('trip', 'string'),\n",
       " ('version', 'string'),\n",
       " ('video', 'boolean'),\n",
       " ('wifi', 'boolean'),\n",
       " ('dep_time_local', 'timestamp'),\n",
       " ('arr_time_local', 'timestamp'),\n",
       " ('duration_h', 'string'),\n",
       " ('duration_m', 'string'),\n",
       " ('stop_info1', 'string'),\n",
       " ('stop_info2', 'string'),\n",
       " ('stop_loc1', 'string'),\n",
       " ('stop_duration1', 'string'),\n",
       " ('stop_loc2', 'string'),\n",
       " ('stop_duration2', 'string'),\n",
       " ('stop_duration_h1', 'string'),\n",
       " ('stop_duration_m1', 'string'),\n",
       " ('stop_duration_h2', 'string'),\n",
       " ('stop_duration_m2', 'string')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flight2.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# flight2 = flight2.drop('stop_duration_h1', 'stop_duration_m1', 'stop_duration_h2', 'stop_duration_m2',\n",
    "#                       'stop_duration1', 'stop_duration2',\n",
    "#                       'duration_h', 'duration_m', \n",
    "#                        'stop_info', 'stop_info1', 'stop_info2')\n",
    "# display(flight2.show(2))\n",
    "# display(flight2.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+--------------------+-----------------+----------------+--------------------+--------+-----------+-------------+--------------+----+-----+--------------------+-----+-------+----------+--------------------+---------+----------+---------+----+-----------------+----------------+-------+-----------+------------+----+-------+-----+-----+--------------------+--------------------+----------+----------+-----------------+----------+----------+--------------+---------+--------------+----------------+----------------+----------------+----------------+\n",
      "|airline_code|airline_codes|            arr_time|check_bag_not_inc|         company|            dep_time|duration|flight_code|flight_number|from_city_name|  id|index|               plane|power|  price|price_code|         search_date|span_days|start_date|stay_days|stop|        stop_info|      table_name|task_id|ticket_left|to_city_name|trip|version|video| wifi|      dep_time_local|      arr_time_local|duration_h|duration_m|       stop_info1|stop_info2| stop_loc1|stop_duration1|stop_loc2|stop_duration2|stop_duration_h1|stop_duration_m1|stop_duration_h2|stop_duration_m2|\n",
      "+------------+-------------+--------------------+-----------------+----------------+--------------------+--------+-----------+-------------+--------------+----+-----+--------------------+-----+-------+----------+--------------------+---------+----------+---------+----+-----------------+----------------+-------+-----------+------------+----+-------+-----+-----+--------------------+--------------------+----------+----------+-----------------+----------+----------+--------------+---------+--------------+----------------+----------------+----------------+----------------+\n",
      "|          QF|         [QF]|2017-04-05 11:20:...|            false|  Qantas Airways|2017-04-05 01:00:...|  10h20m|      QF301|          301|        sydney|null|   12|AIRBUS INDUSTRIE ...|false|1178.42|       AUD|2017-04-04 00:00:...|        0|2017-04-05|        0|   0|                 |flight_1_6_price|    901|          9|    shanghai|   1|    1.0| true|false|2017-04-05 11:00:...|2017-04-05 19:20:...|        10|       20m|                 |      null|          |          null|     null|          null|            null|            null|            null|            null|\n",
      "|          VN|     [VN, VN]|2017-04-06 06:25:...|            false|Vietnam Airlines|2017-04-05 04:15:...|  26h10m|      VN786|          786|        sydney|null|   69|          Boeing 787|false|1095.22|       AUD|2017-04-04 00:00:...|        0|2017-04-05|        0|   1|Hanoi(HAN):13h35m|flight_1_6_price|    901|          4|    shanghai|   1|    1.0|false|false|2017-04-05 14:15:...|2017-04-06 14:25:...|        26|       10m|Hanoi(HAN):13h35m|      null|Hanoi(HAN)|        13h35m|     null|          null|              13|             35m|            null|            null|\n",
      "+------------+-------------+--------------------+-----------------+----------------+--------------------+--------+-----------+-------------+--------------+----+-----+--------------------+-----+-------+----------+--------------------+---------+----------+---------+----+-----------------+----------------+-------+-----------+------------+----+-------+-----+-----+--------------------+--------------------+----------+----------+-----------------+----------+----------+--------------+---------+--------------+----------------+----------------+----------------+----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flight2.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[airline_code: string, airline_codes: array<string>, arr_time: timestamp, check_bag_not_inc: boolean, company: string, dep_time: timestamp, duration: string, flight_code: string, flight_number: string, from_city_name: string, id: string, index: bigint, plane: string, power: boolean, price: double, price_code: string, search_date: timestamp, span_days: int, start_date: date, stay_days: int, stop: int, stop_info: string, table_name: string, task_id: string, ticket_left: string, to_city_name: string, trip: string, version: string, video: boolean, wifi: boolean, dep_time_local: timestamp, arr_time_local: timestamp, duration_h: string, duration_m: string, stop_info1: string, stop_info2: string, stop_loc1: string, stop_duration1: string, stop_loc2: string, stop_duration2: string, stop_duration_h1: string, stop_duration_m1: string, stop_duration_h2: string, stop_duration_m2: string, arr_time_group: string, dep_time_group: string]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def groupTime(dt):\n",
    "    h = dt.hour\n",
    "    if h < 5 : return 'early morning'    \n",
    "    elif h < 12 : return 'morning'\n",
    "    elif h < 18 : return 'afternoon'    \n",
    "    else: return 'evening'\n",
    "#     if h >= 18 or h < 7 : return 'night'    \n",
    "#     elif 7 <= h < 12 : return 'morning'\n",
    "#     elif 12 <= h < 18 : return 'afternoon'    \n",
    "#     else: return 'error'\n",
    "    \n",
    "    \n",
    "udfGroupTime = udf(groupTime, StringType())\n",
    "flight2 = flight2.withColumn(\"arr_time_group\", udfGroupTime(\"arr_time_local\")) \\\n",
    "                .withColumn(\"dep_time_group\", udfGroupTime(\"dep_time_local\"))              \n",
    "flight2.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "# flight2.select(\"dep_time_local\", \"dep_time_group\", \"arr_time_local\", \"arr_time_group\").show(10)\n",
    "#flight2.describe(['price']).show()# groupBy(\"dep_time_group\", \"arr_time_group\")\n",
    "# flight2.freqItems(flight2.trip)\n",
    "# flight2.groupby(flight2.trip, flight2.stay_days, flight.dep_time_group).agg(func.mean('price')).show()\n",
    "# flight2.groupby(flight2.trip, flight2.dep_time_group).agg(func.mean('price')).orderBy(['trip', 'avg(price)'], ascending=[1, 0]).show()\n",
    "# flight2.groupby(flight2.trip, flight2.company).agg(F.mean('price'), F.stddev('price'), F.max('price'), F.min('price')).orderBy(['trip', 'stddev_samp(price)'], ascending=[1, 0]).show(10000)\n",
    "flight2.filter(flight2.price <= 0).count() / flight2.count()\n",
    "\n",
    "# drop rows with zero price\n",
    "flight2 = flight2.filter(flight2.price > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# duration_test.select(trim(col(\"duration\"))).show()\n",
    "# df\n",
    "# .withColumn('Created-formatted',when((df.Created.isNull() | (df.Created == '')) ,'0')\n",
    "# .otherwise(unix_timestamp(df.Created,'yyyy-MM-dd')))\n",
    "\n",
    "duration_test = flight2.select(\"stop_duration1\")\n",
    "duration_test.show()\n",
    "\n",
    "duration_test.withColumn('duration_h', when(duration_test.stop_duration1.isNull(), None)\n",
    "                          .otherwise(hour(unix_timestamp(duration_test.stop_duration1,\"HH'h'mm'm'\").cast(\"timestamp\")))).show(20)          \n",
    "    \n",
    "#     .withColumn('duration_m', minute(unix_timestamp(duration_test.duration,\"HH'h'mm'm'\").cast(\"timestamp\")))).show(2)\n",
    "\n",
    "\n",
    "# duration_test.withColumn('duration2',when((duration_test.duration.isNull() | (duration_test.duration == '')) , 0)\\\n",
    "#   .otherwise(unix_timestamp(duration_test.duration,\"HH'h'mm'm'\").cast(\"timestamp\"))).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from datetime import date\n",
    "\n",
    "# import holidays\n",
    "\n",
    "# us_holidays = holidays.UnitedStates()  # or holidays.US()\n",
    "\n",
    "# date(2015, 1, 1) in us_holidays  # True\n",
    "# date(2020, 12, 25) in us_holidays  # False\n",
    "\n",
    "# # print(us_holidays)\n",
    "\n",
    "\n",
    "# for date, name in sorted(holidays.AU(state='NSW', years=2017).items()):\n",
    "#      print(date, name)\n",
    "        \n",
    "# for date, name in sorted(holidays.NZ(state='AUK', years=2017).items()):\n",
    "#      print(date, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # The Holiday class will also recognize strings of any format\n",
    "# # and int/float representing a Unix timestamp\n",
    "# '2014-01-01' in us_holidays  # True\n",
    "# '1/1/2014' in us_holidays    # True\n",
    "# 1388597445 in us_holidays    # True\n",
    "\n",
    "# us_holidays.get('2014-01-01')  # \"New Year's Day\"\n",
    "\n",
    "# # Easily create custom Holiday objects with your own dates instead\n",
    "# # of using the pre-defined countries/states/provinces available\n",
    "# custom_holidays = holidays.HolidayBase()\n",
    "# # Append custom holiday dates by passing:\n",
    "# # 1) a dict with date/name key/value pairs,\n",
    "# custom_holidays.append({\"2015-01-01\": \"New Year's Day\"})\n",
    "# # 2) a list of dates (in any format: date, datetime, string, integer),\n",
    "# custom_holidays.append(['2015-07-01', '07/04/2015'])\n",
    "# # 3) a single date item\n",
    "# custom_holidays.append(date(2015, 12, 25))\n",
    "\n",
    "# date(2015, 1, 1) in custom_holidays  # True\n",
    "# date(2015, 1, 2) in custom_holidays  # False\n",
    "# '12/25/2015' in custom_holidays      # True\n",
    "\n",
    "# # For more complex logic like 4th Monday of January, you can inherit the\n",
    "# # HolidayBase class and define your own _populate(year) method. See below\n",
    "# # documentation for examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from datetime import date\n",
    "# from workalendar.europe import France\n",
    "# cal = France()\n",
    "# cal.holidays(2017)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+--------------------+-----------------+----------------+--------------------+--------+-----------+-------------+--------------+----+-----+--------------------+-----+-------+----------+--------------------+---------+----------+---------+----+-----------------+----------------+-------+-----------+------------+----+-------+-----+-----+--------------------+--------------------+----------+----------+-----------------+----------+----------+--------------+---------+--------------+----------------+----------------+----------------+----------------+----------------+-------------+-------------+--------------+--------------+-----------+-----------+---------+\n",
      "|airline_code|airline_codes|            arr_time|check_bag_not_inc|         company|            dep_time|duration|flight_code|flight_number|from_city_name|  id|index|               plane|power|  price|price_code|         search_date|span_days|start_date|stay_days|stop|        stop_info|      table_name|task_id|ticket_left|to_city_name|trip|version|video| wifi|      dep_time_local|      arr_time_local|duration_h|duration_m|       stop_info1|stop_info2| stop_loc1|stop_duration1|stop_loc2|stop_duration2|stop_duration_h1|stop_duration_m1|stop_duration_h2|stop_duration_m2|duration_minutes|stop1_minutes|stop2_minutes|arr_time_group|dep_time_group|dep_weekday|dep_weeknum|lead_time|\n",
      "+------------+-------------+--------------------+-----------------+----------------+--------------------+--------+-----------+-------------+--------------+----+-----+--------------------+-----+-------+----------+--------------------+---------+----------+---------+----+-----------------+----------------+-------+-----------+------------+----+-------+-----+-----+--------------------+--------------------+----------+----------+-----------------+----------+----------+--------------+---------+--------------+----------------+----------------+----------------+----------------+----------------+-------------+-------------+--------------+--------------+-----------+-----------+---------+\n",
      "|          QF|         [QF]|2017-04-05 21:20:...|            false|  Qantas Airways|2017-04-05 11:00:...|  10h20m|      QF301|          301|        sydney|null|   12|AIRBUS INDUSTRIE ...|false|1178.42|       AUD|2017-04-04 00:00:...|        0|2017-04-05|        0|   0|                 |flight_1_6_price|    901|          9|    shanghai|   1|    1.0| true|false|2017-04-05 11:00:...|2017-04-05 19:20:...|        10|       20m|                 |      null|          |          null|     null|          null|            null|            null|            null|            null|             620|         null|         null|       evening|       morning|          2|         14|        1|\n",
      "|          VN|     [VN, VN]|2017-04-06 16:25:...|            false|Vietnam Airlines|2017-04-05 14:15:...|  26h10m|      VN786|          786|        sydney|null|   69|          Boeing 787|false|1095.22|       AUD|2017-04-04 00:00:...|        0|2017-04-05|        0|   1|Hanoi(HAN):13h35m|flight_1_6_price|    901|          4|    shanghai|   1|    1.0|false|false|2017-04-05 14:15:...|2017-04-06 14:25:...|        26|       10m|Hanoi(HAN):13h35m|      null|Hanoi(HAN)|        13h35m|     null|          null|              13|             35m|            null|            null|            1570|          815|         null|     afternoon|     afternoon|          2|         14|        1|\n",
      "+------------+-------------+--------------------+-----------------+----------------+--------------------+--------+-----------+-------------+--------------+----+-----+--------------------+-----+-------+----------+--------------------+---------+----------+---------+----+-----------------+----------------+-------+-----------+------------+----+-------+-----+-----+--------------------+--------------------+----------+----------+-----------------+----------+----------+--------------+---------+--------------+----------------+----------------+----------------+----------------+----------------+-------------+-------------+--------------+--------------+-----------+-----------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1835752"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1835752"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|         lead_time|\n",
      "+-------+------------------+\n",
      "|  count|           1835752|\n",
      "|   mean| 90.30425950782023|\n",
      "| stddev|51.508851575859815|\n",
      "|    min|                 1|\n",
      "|    max|               180|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# flight2.show(2)\n",
    "flight3 = flight2\n",
    "\n",
    "\n",
    "def getWeekday(dt):\n",
    "    return dt.weekday()\n",
    "    \n",
    "def getWeeknumber(dt):\n",
    "    return dt.isocalendar()[1]\n",
    "\n",
    "    \n",
    "udfGetWeekday = udf(getWeekday, StringType())\n",
    "udfGetWeeknumber = udf(getWeeknumber, IntegerType())\n",
    "\n",
    "flight3 = flight3.withColumn(\"dep_weekday\", udfGetWeekday(\"start_date\")) \\\n",
    "                .withColumn(\"dep_weeknum\", udfGetWeeknumber(\"start_date\")) \\\n",
    "                .withColumn(\"lead_time\", datediff(flight3.start_date, flight3.search_date)) \n",
    "#                 .drop('start_date', 'search_date')\n",
    "\n",
    "flight3.show(2)\n",
    "display(flight2.count(), flight3.count())\n",
    "flight3.describe('lead_time').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+--------------------+-----------------+--------------------+--------------------+--------+-----------+-------------+--------------+----+-----+--------------------+-----+-------+----------+--------------------+---------+----------+---------+----+--------------------+----------------+-------+-----------+------------+----+-------+-----+-----+--------------------+--------------------+----------+----------+--------------------+----------+--------------+--------------+---------+--------------+----------------+----------------+----------------+----------------+----------------+-------------+-------------+--------------+--------------+-----------+-----------+---------+\n",
      "|airline_code|airline_codes|            arr_time|check_bag_not_inc|             company|            dep_time|duration|flight_code|flight_number|from_city_name|  id|index|               plane|power|  price|price_code|         search_date|span_days|start_date|stay_days|stop|           stop_info|      table_name|task_id|ticket_left|to_city_name|trip|version|video| wifi|      dep_time_local|      arr_time_local|duration_h|duration_m|          stop_info1|stop_info2|     stop_loc1|stop_duration1|stop_loc2|stop_duration2|stop_duration_h1|stop_duration_m1|stop_duration_h2|stop_duration_m2|duration_minutes|stop1_minutes|stop2_minutes|arr_time_group|dep_time_group|dep_weekday|dep_weeknum|lead_time|\n",
      "+------------+-------------+--------------------+-----------------+--------------------+--------------------+--------+-----------+-------------+--------------+----+-----+--------------------+-----+-------+----------+--------------------+---------+----------+---------+----+--------------------+----------------+-------+-----------+------------+----+-------+-----+-----+--------------------+--------------------+----------+----------+--------------------+----------+--------------+--------------+---------+--------------+----------------+----------------+----------------+----------------+----------------+-------------+-------------+--------------+--------------+-----------+-----------+---------+\n",
      "|          QF|         [QF]|2017-04-05 21:20:...|            false|      Qantas Airways|2017-04-05 11:00:...|  10h20m|      QF301|          301|        sydney|null|   12|AIRBUS INDUSTRIE ...|false|1178.42|       AUD|2017-04-04 00:00:...|        0|2017-04-05|        0|   0|                    |flight_1_6_price|    901|          9|    shanghai|   1|    1.0| true|false|2017-04-05 11:00:...|2017-04-05 19:20:...|        10|       20m|                    |      null|              |          null|     null|          null|            null|            null|            null|            null|             620|         null|         null|       evening|       morning|          2|         14|        1|\n",
      "|          VN|     [VN, VN]|2017-04-06 16:25:...|            false|    Vietnam Airlines|2017-04-05 14:15:...|  26h10m|      VN786|          786|        sydney|null|   69|          Boeing 787|false|1095.22|       AUD|2017-04-04 00:00:...|        0|2017-04-05|        0|   1|   Hanoi(HAN):13h35m|flight_1_6_price|    901|          4|    shanghai|   1|    1.0|false|false|2017-04-05 14:15:...|2017-04-06 14:25:...|        26|       10m|   Hanoi(HAN):13h35m|      null|    Hanoi(HAN)|        13h35m|     null|          null|              13|             35m|            null|            null|            1570|          815|         null|     afternoon|     afternoon|          2|         14|        1|\n",
      "|          CZ|     [CZ, CZ]|2017-04-06 14:25:...|            false|China Southern Ai...|2017-04-05 21:45:...|  16h40m|      CZ302|          302|        sydney|null|   20|         Airbus A330|false|1279.32|       AUD|2017-04-04 00:00:...|        0|2017-04-05|        0|   1|Guangzhou(CAN):4h35m|flight_1_6_price|    901|          1|    shanghai|   1|    1.0|false|false|2017-04-05 21:45:...|2017-04-06 12:25:...|        16|       40m|Guangzhou(CAN):4h35m|      null|Guangzhou(CAN)|         4h35m|     null|          null|               4|             35m|            null|            null|            1000|          275|         null|     afternoon|       evening|          2|         14|        1|\n",
      "|          QF|     [QF, CA]|2017-04-06 09:15:...|            false|      Qantas Airways|2017-04-05 13:15:...|   20h0m|      QF147|          147|        sydney|null|   49|BOEING 737-800 (W...|false|2233.52|       AUD|2017-04-04 00:00:...|        0|2017-04-05|        0|   1| Auckland(AKL):4h35m|flight_1_6_price|    901|          9|    shanghai|   1|    1.0|false|false|2017-04-05 13:15:...|2017-04-06 07:15:...|        20|        0m| Auckland(AKL):4h35m|      null| Auckland(AKL)|         4h35m|     null|          null|               4|             35m|            null|            null|            1200|          275|         null|       morning|     afternoon|          2|         14|        1|\n",
      "|          CZ|     [CZ, FM]|2017-04-06 12:50:...|            false|China Southern Ai...|2017-04-05 21:45:...|   15h5m|      CZ302|          302|        sydney|null|   80|         Airbus A330|false|1722.32|       AUD|2017-04-04 00:00:...|        0|2017-04-05|        0|   1| Guangzhou(CAN):3h5m|flight_1_6_price|    901|          1|    shanghai|   1|    1.0|false|false|2017-04-05 21:45:...|2017-04-06 10:50:...|        15|        5m| Guangzhou(CAN):3h5m|      null|Guangzhou(CAN)|          3h5m|     null|          null|               3|              5m|            null|            null|             905|          185|         null|       morning|       evening|          2|         14|        1|\n",
      "+------------+-------------+--------------------+-----------------+--------------------+--------------------+--------+-----------+-------------+--------------+----+-----+--------------------+-----+-------+----------+--------------------+---------+----------+---------+----+--------------------+----------------+-------+-----------+------------+----+-------+-----+-----+--------------------+--------------------+----------+----------+--------------------+----------+--------------+--------------+---------+--------------+----------------+----------------+----------------+----------------+----------------+-------------+-------------+--------------+--------------+-----------+-----------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('airline_code', 'string'),\n",
       " ('airline_codes', 'array<string>'),\n",
       " ('arr_time', 'timestamp'),\n",
       " ('check_bag_not_inc', 'boolean'),\n",
       " ('company', 'string'),\n",
       " ('dep_time', 'timestamp'),\n",
       " ('duration', 'string'),\n",
       " ('flight_code', 'string'),\n",
       " ('flight_number', 'string'),\n",
       " ('from_city_name', 'string'),\n",
       " ('id', 'string'),\n",
       " ('index', 'bigint'),\n",
       " ('plane', 'string'),\n",
       " ('power', 'boolean'),\n",
       " ('price', 'double'),\n",
       " ('price_code', 'string'),\n",
       " ('search_date', 'timestamp'),\n",
       " ('span_days', 'int'),\n",
       " ('start_date', 'date'),\n",
       " ('stay_days', 'int'),\n",
       " ('stop', 'int'),\n",
       " ('stop_info', 'string'),\n",
       " ('table_name', 'string'),\n",
       " ('task_id', 'string'),\n",
       " ('ticket_left', 'string'),\n",
       " ('to_city_name', 'string'),\n",
       " ('trip', 'string'),\n",
       " ('version', 'string'),\n",
       " ('video', 'boolean'),\n",
       " ('wifi', 'boolean'),\n",
       " ('dep_time_local', 'timestamp'),\n",
       " ('arr_time_local', 'timestamp'),\n",
       " ('duration_h', 'string'),\n",
       " ('duration_m', 'string'),\n",
       " ('stop_info1', 'string'),\n",
       " ('stop_info2', 'string'),\n",
       " ('stop_loc1', 'string'),\n",
       " ('stop_duration1', 'string'),\n",
       " ('stop_loc2', 'string'),\n",
       " ('stop_duration2', 'string'),\n",
       " ('stop_duration_h1', 'string'),\n",
       " ('stop_duration_m1', 'string'),\n",
       " ('stop_duration_h2', 'string'),\n",
       " ('stop_duration_m2', 'string'),\n",
       " ('duration_minutes', 'int'),\n",
       " ('stop1_minutes', 'int'),\n",
       " ('stop2_minutes', 'int'),\n",
       " ('arr_time_group', 'string'),\n",
       " ('dep_time_group', 'string'),\n",
       " ('dep_weekday', 'string'),\n",
       " ('dep_weeknum', 'int'),\n",
       " ('lead_time', 'int')]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flight3.show(5)\n",
    "flight3.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get distance to nearest holiday: http://stackoverflow.com/questions/40752378/spark-sql-distance-to-nearest-holiday\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# datetime.datetime('2017-09-01')\n",
    "# date1 = datetime.datetime.strptime(\"2015-01-30\", \"%Y-%m-%d\").strftime(\"%d-%m-%Y\")\n",
    "# print(date1)\n",
    "# start_date_test = flight3.select(\"start_date\").distinct()\n",
    "flight3 = flight3.withColumn(\"start_date_str\", flight3.start_date.cast('string'))\n",
    "\n",
    "holidays = ['2017-01-01', '2017-01-02', '2017-01-27', '2017-01-28',\n",
    "            '2017-01-29', '2017-01-30', '2017-01-31', '2017-02-01', '2017-02-02',\n",
    "            '2017-04-02', '2017-04-03', '2017-04-04',\n",
    "            '2017-05-01', '2017-05-28', '2017-05-29', '2017-05-30',\n",
    "            '2017-10-01', '2017-10-02', '2017-10-03', '2017-10-04', '217-10-05', '2017-10-06']\n",
    " \n",
    "\n",
    "index = spark.sparkContext.broadcast(sorted(holidays))\n",
    "\n",
    "def last_holiday(date):\n",
    "    last_holiday = index.value[0]\n",
    "    for next_holiday in index.value:\n",
    "        if next_holiday >= date:\n",
    "            break\n",
    "        last_holiday = next_holiday\n",
    "    if last_holiday > date:\n",
    "        last_holiday = None\n",
    "    if next_holiday < date:\n",
    "        next_holiday = None        \n",
    "    return last_holiday\n",
    "\n",
    "def next_holiday(date):\n",
    "    last_holiday = index.value[0]\n",
    "    for next_holiday in index.value:\n",
    "        if next_holiday >= date:\n",
    "            break\n",
    "        last_holiday = next_holiday\n",
    "    if last_holiday > date:\n",
    "        last_holiday = None\n",
    "    if next_holiday < date:\n",
    "        next_holiday = None        \n",
    "    return next_holiday\n",
    "\n",
    "\n",
    "# return_type = StructType([StructField('last_holiday', StringType()), StructField('next_holiday', StringType())])\n",
    "\n",
    "last_holiday_udf = udf(last_holiday, StringType())\n",
    "next_holiday_udf = udf(next_holiday, StringType())\n",
    "\n",
    "flight4 = flight3.withColumn('last_holiday', last_holiday_udf('start_date_str'))\n",
    "flight4 = flight4.withColumn('next_holiday', next_holiday_udf('start_date_str'))\n",
    "flight4 = flight4.withColumn('days_to_last_holiday', datediff('last_holiday', 'start_date_str'))\n",
    "flight4 = flight4.withColumn('days_to_next_holiday', datediff('next_holiday', 'start_date_str'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------------+------------+----------+\n",
      "|days_to_last_holiday|days_to_next_holiday|last_holiday|next_holiday|start_date|\n",
      "+--------------------+--------------------+------------+------------+----------+\n",
      "|                 -40|                  84|  2017-05-30|  2017-10-01|2017-07-09|\n",
      "|                 -17|                  10|  2017-04-04|  2017-05-01|2017-04-21|\n",
      "|                  -8|                  19|  2017-04-04|  2017-05-01|2017-04-12|\n",
      "|                 -34|                  90|  2017-05-30|  2017-10-01|2017-07-03|\n",
      "|                 -22|                 102|  2017-05-30|  2017-10-01|2017-06-21|\n",
      "|                 -70|                  54|  2017-05-30|  2017-10-01|2017-08-08|\n",
      "|                 -75|                  49|  2017-05-30|  2017-10-01|2017-08-13|\n",
      "|                 -28|                  96|  2017-05-30|  2017-10-01|2017-06-27|\n",
      "|                 -16|                  11|  2017-04-04|  2017-05-01|2017-04-20|\n",
      "|                 -12|                  15|  2017-05-01|  2017-05-28|2017-05-13|\n",
      "|                 -19|                   8|  2017-05-01|  2017-05-28|2017-05-20|\n",
      "|                -100|                  24|  2017-05-30|  2017-10-01|2017-09-07|\n",
      "|                 -27|                  97|  2017-05-30|  2017-10-01|2017-06-26|\n",
      "|                 -65|                  59|  2017-05-30|  2017-10-01|2017-08-03|\n",
      "|                 -23|                null|  2017-10-06|   217-10-05|2017-10-29|\n",
      "|                 -46|                  78|  2017-05-30|  2017-10-01|2017-07-15|\n",
      "|                 -10|                  17|  2017-04-04|  2017-05-01|2017-04-14|\n",
      "|                 -11|                  16|  2017-04-04|  2017-05-01|2017-04-15|\n",
      "|                 -84|                  40|  2017-05-30|  2017-10-01|2017-08-22|\n",
      "|                 -98|                  26|  2017-05-30|  2017-10-01|2017-09-05|\n",
      "+--------------------+--------------------+------------+------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(flight4\n",
    "    .select('days_to_last_holiday', 'days_to_next_holiday', 'last_holiday', 'next_holiday', 'start_date')    \n",
    "    .distinct().show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[airline_code: string, airline_codes: array<string>, arr_time: timestamp, check_bag_not_inc: boolean, company: string, dep_time: timestamp, duration: string, flight_code: string, flight_number: string, from_city_name: string, id: string, index: bigint, plane: string, power: boolean, price: double, price_code: string, search_date: timestamp, span_days: int, start_date: date, stay_days: int, stop: int, stop_info: string, table_name: string, task_id: string, ticket_left: string, to_city_name: string, trip: string, version: string, video: boolean, wifi: boolean, dep_time_local: timestamp, arr_time_local: timestamp, duration_h: string, duration_m: string, stop_info1: string, stop_info2: string, stop_loc1: string, stop_duration1: string, stop_loc2: string, stop_duration2: string, stop_duration_h1: string, stop_duration_m1: string, stop_duration_h2: string, stop_duration_m2: string, duration_minutes: int, stop1_minutes: int, stop2_minutes: int, arr_time_group: string, dep_time_group: string, dep_weekday: string, dep_weeknum: int, lead_time: int, start_date_str: string, last_holiday: string, next_holiday: string, days_to_last_holiday: int, days_to_next_holiday: int, future_min_price: double, price_will_drop: boolean, price_will_drop_num: int]"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# flight4.show()\n",
    "#'table_name', 'trip', 'stay_days'\n",
    "# flight5 = (flight4.filter((flight4.trip==1) & (flight4.start_date=='2017-10-01') & (flight4.company=='AirAsiaX'))\n",
    "#          .select('start_date', 'company', 'dep_time_local', 'stop_info', 'duration', 'search_date', 'price')\n",
    "#          .sort(F.desc('start_date'), 'company', 'dep_time_local', 'stop_info', 'duration', 'search_date'))       \n",
    "         \n",
    "flight5 = flight4.sort('table_name', 'trip', 'stay_days', \n",
    "                       'start_date', 'company', 'dep_time_local',\n",
    "                       'stop_info', 'duration', 'search_date')       \n",
    "    \n",
    "byVar = ['table_name', 'trip', 'stay_days', 'start_date', 'company', 'dep_time_local', 'stop_info', 'duration']\n",
    "\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "Threshold = 5\n",
    "\n",
    "w =  Window.partitionBy(byVar).orderBy('search_date').rowsBetween(0, sys.maxsize)\n",
    "flight6 = (flight5.withColumn('future_min_price', F.min(col('price')).over(w))\n",
    "          .withColumn('price_will_drop',                       \n",
    "                      (col('price') - col('future_min_price')) > Threshold ))\n",
    "flight6 = flight6.withColumn('price_will_drop_num', flight6.price_will_drop.cast('int'))\n",
    "flight6.cache()\n",
    "# min_price = (flight5\n",
    "#                   .groupBy(byVar)\n",
    "#                   .agg(F.min(col(\"price\"))).alias(\"min_price\"))\n",
    "\n",
    "# flight6 = flight5.join(min_price, on=byVar, how='left')\n",
    "# # flight6.select('price', 'min(price)').show(10)\n",
    "# # flight6.select('price', 'min(price)').show()\n",
    "# flight6 = flight6.withColumn('price_diff', col('price')-col('min(price)'))\n",
    "# flight6.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----+----------------+---------------+\n",
      "|duration|         search_date|price|future_min_price|price_will_drop|\n",
      "+--------+--------------------+-----+----------------+---------------+\n",
      "|  14h20m|2017-04-04 00:00:...|529.5|           439.0|           true|\n",
      "|  14h20m|2017-04-06 00:00:...|529.5|           439.0|           true|\n",
      "|  14h20m|2017-04-15 00:00:...|529.5|           439.0|           true|\n",
      "|  14h20m|2017-04-16 00:00:...|529.5|           439.0|           true|\n",
      "|  14h20m|2017-04-19 00:00:...|529.5|           439.0|           true|\n",
      "|  14h20m|2017-04-20 00:00:...|529.5|           439.0|           true|\n",
      "|  14h20m|2017-04-21 00:00:...|579.5|           439.0|           true|\n",
      "|  14h20m|2017-04-22 00:00:...|579.5|           439.0|           true|\n",
      "|  14h20m|2017-04-24 00:00:...|579.5|           439.0|           true|\n",
      "|  14h20m|2017-04-25 00:00:...|579.5|           439.0|           true|\n",
      "|  14h20m|2017-04-26 00:00:...|535.5|           439.0|           true|\n",
      "|  14h20m|2017-04-27 00:00:...|439.0|           439.0|          false|\n",
      "|  14h20m|2017-04-28 00:00:...|439.0|           439.0|          false|\n",
      "|  14h20m|2017-04-29 00:00:...|439.0|           439.0|          false|\n",
      "|  14h20m|2017-04-30 00:00:...|439.0|           439.0|          false|\n",
      "|  14h20m|2017-05-01 00:00:...|535.5|           535.5|          false|\n",
      "|  14h20m|2017-05-02 00:00:...|535.5|           535.5|          false|\n",
      "|  28h35m|2017-04-04 00:00:...|579.5|           439.0|           true|\n",
      "|  28h35m|2017-04-06 00:00:...|579.5|           439.0|           true|\n",
      "|  28h35m|2017-04-13 00:00:...|579.5|           439.0|           true|\n",
      "|  28h35m|2017-04-15 00:00:...|579.5|           439.0|           true|\n",
      "|  28h35m|2017-04-16 00:00:...|579.5|           439.0|           true|\n",
      "|  28h35m|2017-04-19 00:00:...|579.5|           439.0|           true|\n",
      "|  28h35m|2017-04-20 00:00:...|579.5|           439.0|           true|\n",
      "|  28h35m|2017-04-21 00:00:...|579.5|           439.0|           true|\n",
      "|  28h35m|2017-04-22 00:00:...|579.5|           439.0|           true|\n",
      "|  28h35m|2017-04-24 00:00:...|579.5|           439.0|           true|\n",
      "|  28h35m|2017-04-25 00:00:...|450.0|           439.0|           true|\n",
      "|  28h35m|2017-04-26 00:00:...|439.0|           439.0|          false|\n",
      "|  28h35m|2017-04-27 00:00:...|439.0|           439.0|          false|\n",
      "|  28h35m|2017-04-28 00:00:...|439.0|           439.0|          false|\n",
      "|  28h35m|2017-04-29 00:00:...|439.0|           439.0|          false|\n",
      "|  28h35m|2017-04-30 00:00:...|439.0|           439.0|          false|\n",
      "|  28h35m|2017-05-01 00:00:...|439.0|           439.0|          false|\n",
      "|  28h35m|2017-05-02 00:00:...|469.0|           469.0|          false|\n",
      "+--------+--------------------+-----+----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flight6.filter((flight6.trip==1) & \\\n",
    "               (flight6.start_date=='2017-10-01') & \\\n",
    "               (flight6.company=='AirAsiaX')) \\\n",
    "    .select('duration', 'search_date', 'price', 'future_min_price', 'price_will_drop') \\\n",
    "    .show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[airline_code: string, airline_codes: array<string>, arr_time: timestamp, check_bag_not_inc: boolean, company: string, dep_time: timestamp, duration: string, flight_code: string, flight_number: string, from_city_name: string, id: string, index: bigint, plane: string, power: boolean, price: double, price_code: string, search_date: timestamp, span_days: int, start_date: date, stay_days: int, stop: int, stop_info: string, table_name: string, task_id: string, ticket_left: string, to_city_name: string, trip: string, version: string, video: boolean, wifi: boolean, dep_time_local: timestamp, arr_time_local: timestamp, duration_h: string, duration_m: string, stop_info1: string, stop_info2: string, stop_loc1: string, stop_duration1: string, stop_loc2: string, stop_duration2: string, stop_duration_h1: string, stop_duration_m1: string, stop_duration_h2: string, stop_duration_m2: string, duration_minutes: int, stop1_minutes: int, stop2_minutes: int, arr_time_group: string, dep_time_group: string]"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flight2.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[airline_code: string, airline_codes: array<string>, arr_time: timestamp, check_bag_not_inc: boolean, company: string, dep_time: timestamp, duration: string, flight_code: string, flight_number: string, from_city_name: string, id: string, index: bigint, plane: string, power: boolean, price: double, price_code: string, search_date: timestamp, span_days: int, start_date: date, stay_days: int, stop: int, stop_info: string, table_name: string, task_id: string, ticket_left: string, to_city_name: string, trip: string, version: string, video: boolean, wifi: boolean, dep_time_local: timestamp, arr_time_local: timestamp, duration_h: string, duration_m: string, stop_info1: string, stop_info2: string, stop_loc1: string, stop_duration1: string, stop_loc2: string, stop_duration2: string, stop_duration_h1: string, stop_duration_m1: string, stop_duration_h2: string, stop_duration_m2: string, duration_minutes: int, stop1_minutes: int, stop2_minutes: int, arr_time_group: string, dep_time_group: string, dep_weekday: string, dep_weeknum: int, lead_time: int, start_date_str: string, last_holiday: string, next_holiday: string, days_to_last_holiday: int, days_to_next_holiday: int, future_min_price: double, price_will_drop: boolean, price_will_drop_num: int]"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flight6.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+---------+------------------------+\n",
      "|trip|stay_days|lead_time|avg(price_will_drop_num)|\n",
      "+----+---------+---------+------------------------+\n",
      "|   1|        0|      180|     0.36288265306122447|\n",
      "|   1|        0|      179|     0.35714285714285715|\n",
      "|   1|        0|      178|      0.3497304582210243|\n",
      "|   1|        0|      177|     0.37575757575757573|\n",
      "|   1|        0|      176|     0.33737244897959184|\n",
      "|   1|        0|      175|     0.32705248990578734|\n",
      "|   1|        0|      174|      0.3657289002557545|\n",
      "|   1|        0|      173|       0.361376673040153|\n",
      "|   1|        0|      172|     0.36607142857142855|\n",
      "|   1|        0|      171|     0.36630504148053605|\n",
      "|   1|        0|      170|      0.3647959183673469|\n",
      "|   1|        0|      169|     0.37333333333333335|\n",
      "|   1|        0|      168|      0.3832908163265306|\n",
      "|   1|        0|      167|      0.3860880663688577|\n",
      "|   1|        0|      166|     0.38049713193116635|\n",
      "|   1|        0|      165|                 0.40625|\n",
      "|   1|        0|      164|       0.412252712188896|\n",
      "|   1|        0|      163|      0.4218181818181818|\n",
      "|   1|        0|      162|      0.4175757575757576|\n",
      "|   1|        0|      161|     0.40700808625336926|\n",
      "|   1|        0|      160|     0.42510612492419647|\n",
      "|   1|        0|      159|     0.39794064203513024|\n",
      "|   1|        0|      158|     0.41600970285021227|\n",
      "|   1|        0|      157|      0.3925970873786408|\n",
      "|   1|        0|      156|      0.3812121212121212|\n",
      "|   1|        0|      155|     0.37393939393939396|\n",
      "|   1|        0|      154|     0.38326258338386904|\n",
      "|   1|        0|      153|       0.438058748403576|\n",
      "|   1|        0|      152|      0.3949121744397335|\n",
      "|   1|        0|      151|      0.4077855775366943|\n",
      "|   1|        0|      150|     0.41322815533980584|\n",
      "|   1|        0|      149|      0.4018181818181818|\n",
      "|   1|        0|      148|      0.4115151515151515|\n",
      "|   1|        0|      147|     0.40388114008489995|\n",
      "|   1|        0|      146|     0.38872043662825956|\n",
      "|   1|        0|      145|     0.37970644543714105|\n",
      "|   1|        0|      144|     0.39357186173438446|\n",
      "|   1|        0|      143|      0.4240102171136654|\n",
      "|   1|        0|      142|      0.3933333333333333|\n",
      "|   1|        0|      141|       0.396969696969697|\n",
      "|   1|        0|      140|      0.3997569866342649|\n",
      "|   1|        0|      139|     0.38811400848999394|\n",
      "|   1|        0|      138|     0.40520896426408237|\n",
      "|   1|        0|      137|     0.39963614311704065|\n",
      "|   1|        0|      136|       0.397444089456869|\n",
      "|   1|        0|      135|     0.41333333333333333|\n",
      "|   1|        0|      134|      0.4115151515151515|\n",
      "|   1|        0|      133|       0.422680412371134|\n",
      "|   1|        0|      132|      0.4063068526379624|\n",
      "|   1|        0|      131|     0.38711734693877553|\n",
      "|   1|        0|      130|      0.4166161309884779|\n",
      "|   1|        0|      129|     0.41686893203883496|\n",
      "|   1|        0|      128|      0.4072727272727273|\n",
      "|   1|        0|      127|     0.39454545454545453|\n",
      "|   1|        0|      126|     0.42449969678593086|\n",
      "|   1|        0|      125|     0.42328684050939963|\n",
      "|   1|        0|      124|      0.4179285281647486|\n",
      "|   1|        0|      123|      0.4335961188599151|\n",
      "|   1|        0|      122|      0.4362864077669903|\n",
      "|   1|        0|      121|      0.4192724952137843|\n",
      "|   1|        0|      120|     0.43333333333333335|\n",
      "|   1|        0|      119|      0.4408732565191025|\n",
      "|   1|        0|      118|     0.42813826561552454|\n",
      "|   1|        0|      117|     0.41005451241671714|\n",
      "|   1|        0|      116|      0.4275318374772589|\n",
      "|   1|        0|      115|      0.4362864077669903|\n",
      "|   1|        0|      114|     0.45151515151515154|\n",
      "|   1|        0|      113|     0.42303030303030303|\n",
      "|   1|        0|      112|     0.41954022988505746|\n",
      "|   1|        0|      111|      0.4342025469981807|\n",
      "|   1|        0|      110|      0.4439733494851605|\n",
      "|   1|        0|      109|     0.44630484988452657|\n",
      "|   1|        0|      108|      0.4429611650485437|\n",
      "|   1|        0|      107|     0.44431621465666477|\n",
      "|   1|        0|      106|      0.4339167169583585|\n",
      "|   1|        0|      105|      0.4482957827845176|\n",
      "|   1|        0|      104|     0.44771808203350666|\n",
      "|   1|        0|      103|     0.43499101258238465|\n",
      "|   1|        0|      102|     0.43117040630685266|\n",
      "|   1|        0|      101|      0.4144508670520231|\n",
      "|   1|        0|      100|      0.4106602059357965|\n",
      "|   1|        0|       99|     0.43636363636363634|\n",
      "|   1|        0|       98|      0.4393203883495146|\n",
      "|   1|        0|       97|       0.446360153256705|\n",
      "|   1|        0|       96|      0.4409448818897638|\n",
      "|   1|        0|       95|      0.4208611279563372|\n",
      "|   1|        0|       94|     0.40191693290734826|\n",
      "|   1|        0|       93|      0.4161114476075106|\n",
      "|   1|        0|       92|     0.42727272727272725|\n",
      "|   1|        0|       91|      0.4532766990291262|\n",
      "|   1|        0|       90|     0.45057610673135234|\n",
      "|   1|        0|       89|     0.42761962447001817|\n",
      "|   1|        0|       88|     0.41904184354154034|\n",
      "|   1|        0|       87|      0.4405339805825243|\n",
      "|   1|        0|       86|      0.4403391883706844|\n",
      "|   1|        0|       85|      0.4381818181818182|\n",
      "|   1|        0|       84|      0.4578544061302682|\n",
      "|   1|        0|       83|     0.42510612492419647|\n",
      "|   1|        0|       82|     0.44700181708055725|\n",
      "|   1|        0|       81|      0.4478787878787879|\n",
      "|   1|        0|       80|       0.422680412371134|\n",
      "|   1|        0|       79|      0.4496969696969697|\n",
      "|   1|        0|       78|      0.4380069524913094|\n",
      "|   1|        0|       77|     0.45465049104563837|\n",
      "|   1|        0|       76|      0.4436741767764298|\n",
      "|   1|        0|       75|     0.45239469128678594|\n",
      "|   1|        0|       74|      0.4618937644341801|\n",
      "|   1|        0|       73|      0.4405339805825243|\n",
      "|   1|        0|       72|      0.4382217090069284|\n",
      "|   1|        0|       71|      0.4270051933064051|\n",
      "|   1|        0|       70|      0.4343551185656449|\n",
      "|   1|        0|       69|      0.4246100519930676|\n",
      "|   1|        0|       68|     0.42008078476630123|\n",
      "|   1|        0|       67|     0.42345465049104564|\n",
      "|   1|        0|       66|     0.43893591293833134|\n",
      "|   1|        0|       65|     0.40877598152424943|\n",
      "|   1|        0|       64|     0.40704387990762125|\n",
      "|   1|        0|       63|     0.42987249544626593|\n",
      "|   1|        0|       62|                     0.4|\n",
      "|   1|        0|       61|      0.4193939393939394|\n",
      "|   1|        0|       60|      0.4246100519930676|\n",
      "|   1|        0|       59|     0.42926533090467517|\n",
      "|   1|        0|       58|      0.4122401847575058|\n",
      "|   1|        0|       57|     0.44451182534869615|\n",
      "|   1|        0|       56|      0.3888888888888889|\n",
      "|   1|        0|       55|     0.42744383727990287|\n",
      "|   1|        0|       54|      0.4309090909090909|\n",
      "|   1|        0|       53|      0.4191693290734824|\n",
      "|   1|        0|       52|     0.44289185905224787|\n",
      "|   1|        0|       51|     0.43117040630685266|\n",
      "|   1|        0|       50|     0.43412264723740135|\n",
      "|   1|        0|       49|      0.4340425531914894|\n",
      "|   1|        0|       48|     0.42581423401688784|\n",
      "|   1|        0|       47|     0.42354368932038833|\n",
      "|   1|        0|       46|      0.3989071038251366|\n",
      "|   1|        0|       45|      0.4094775212636695|\n",
      "|   1|        0|       44|     0.41530054644808745|\n",
      "|   1|        0|       43|      0.4269254093389933|\n",
      "|   1|        0|       42|      0.4297872340425532|\n",
      "|   1|        0|       41|     0.42354368932038833|\n",
      "|   1|        0|       40|      0.4074074074074074|\n",
      "|   1|        0|       39|      0.4077669902912621|\n",
      "|   1|        0|       38|     0.42597087378640774|\n",
      "|   1|        0|       37|      0.4211165048543689|\n",
      "|   1|        0|       36|     0.42319368548876746|\n",
      "|   1|        0|       35|      0.4513973268529769|\n",
      "|   1|        0|       34|     0.45051608986035213|\n",
      "|   1|        0|       33|     0.42597087378640774|\n",
      "|   1|        0|       32|     0.44410692588092343|\n",
      "|   1|        0|       31|      0.4541590771098968|\n",
      "|   1|        0|       30|      0.4272030651340996|\n",
      "|   1|        0|       29|     0.43412264723740135|\n",
      "|   1|        0|       28|      0.4251918158567775|\n",
      "|   1|        0|       27|      0.4286628278950736|\n",
      "|   1|        0|       26|      0.4095846645367412|\n",
      "|   1|        0|       25|     0.43039513677811553|\n",
      "|   1|        0|       24|      0.4397810218978102|\n",
      "|   1|        0|       23|     0.42137219186399516|\n",
      "|   1|        0|       22|      0.4111253196930946|\n",
      "|   1|        0|       21|     0.43647416413373863|\n",
      "|   1|        0|       20|       0.409367396593674|\n",
      "|   1|        0|       19|      0.4356014580801944|\n",
      "|   1|        0|       18|      0.4297017650639075|\n",
      "|   1|        0|       17|      0.4723067559342666|\n",
      "|   1|        0|       16|     0.42744383727990287|\n",
      "|   1|        0|       15|     0.41008505467800727|\n",
      "|   1|        0|       14|      0.4142335766423358|\n",
      "|   1|        0|       13|     0.41105039465695203|\n",
      "|   1|        0|       12|        0.40485312899106|\n",
      "|   1|        0|       11|      0.4116575591985428|\n",
      "|   1|        0|       10|     0.40765492102065615|\n",
      "|   1|        0|        9|     0.38447543966040026|\n",
      "|   1|        0|        8|      0.4008489993935719|\n",
      "|   1|        0|        7|      0.3600485731633273|\n",
      "|   1|        0|        6|     0.34081261370527594|\n",
      "|   1|        0|        5|     0.34902200488997553|\n",
      "|   1|        0|        4|      0.2760511882998172|\n",
      "|   1|        0|        3|     0.19854280510018216|\n",
      "|   1|        0|        2|     0.11333333333333333|\n",
      "|   1|        0|        1|    0.011078717201166181|\n",
      "|   2|        7|      180|      0.6121270452358036|\n",
      "|   2|        7|      179|      0.6142786316311354|\n",
      "|   2|        7|      178|      0.5876777251184834|\n",
      "|   2|        7|      177|      0.6447952639368525|\n",
      "|   2|        7|      176|      0.5970079476390837|\n",
      "|   2|        7|      175|      0.5989583333333334|\n",
      "|   2|        7|      174|      0.6037825059101655|\n",
      "|   2|        7|      173|      0.5816125860373648|\n",
      "|   2|        7|      172|      0.6315294117647059|\n",
      "|   2|        7|      171|      0.5863770977295163|\n",
      "|   2|        7|      170|      0.6248168050806058|\n",
      "|   2|        7|      169|      0.5760765550239234|\n",
      "|   2|        7|      168|      0.6121635094715853|\n",
      "|   2|        7|      167|      0.5910176779741997|\n",
      "|   2|        7|      166|      0.6016617790811339|\n",
      "|   2|        7|      165|       0.604073522106309|\n",
      "|   2|        7|      164|       0.597037037037037|\n",
      "|   2|        7|      163|       0.598856598380181|\n",
      "|   2|        7|      162|       0.581048581048581|\n",
      "|   2|        7|      161|      0.6233703524867213|\n",
      "|   2|        7|      160|      0.6291033806957373|\n",
      "|   2|        7|      159|      0.6057494866529775|\n",
      "|   2|        7|      158|      0.5232273838630807|\n",
      "|   2|        7|      157|      0.5371428571428571|\n",
      "|   2|        7|      156|       0.586073500967118|\n",
      "|   2|        7|      155|      0.5642787046123651|\n",
      "|   2|        7|      154|      0.5601761252446184|\n",
      "|   2|        7|      153|      0.5522174535050072|\n",
      "|   2|        7|      152|       0.551622418879056|\n",
      "|   2|        7|      151|      0.6101871101871101|\n",
      "|   2|        7|      150|      0.5988483685220729|\n",
      "|   2|        7|      149|      0.5450941526263627|\n",
      "|   2|        7|      148|      0.5634995296331138|\n",
      "|   2|        7|      147|       0.549074074074074|\n",
      "|   2|        7|      146|      0.5798664612223934|\n",
      "|   2|        7|      145|      0.6026399635867091|\n",
      "|   2|        7|      144|      0.6111869031377899|\n",
      "|   2|        7|      143|      0.6199298948422634|\n",
      "|   2|        7|      142|      0.6032461677186655|\n",
      "|   2|        7|      141|      0.5422953818015547|\n",
      "|   2|        7|      140|      0.5227775320654577|\n",
      "|   2|        7|      139|      0.5020842982862437|\n",
      "|   2|        7|      138|      0.5302768166089965|\n",
      "|   2|        7|      137|      0.5415411106328024|\n",
      "|   2|        7|      136|      0.5513464171611137|\n",
      "|   2|        7|      135|      0.5464362850971922|\n",
      "|   2|        7|      134|      0.5850920520880107|\n",
      "|   2|        7|      133|      0.5656656207978485|\n",
      "|   2|        7|      132|      0.5716096324461344|\n",
      "|   2|        7|      131|      0.5746739587715608|\n",
      "|   2|        7|      130|      0.5801847187237615|\n",
      "|   2|        7|      129|      0.5880806310254163|\n",
      "|   2|        7|      128|       0.603607567091949|\n",
      "|   2|        7|      127|      0.5894378194207837|\n",
      "|   2|        7|      126|      0.5931352030138133|\n",
      "|   2|        7|      125|      0.6158318425760286|\n",
      "|   2|        7|      124|      0.5940803382663847|\n",
      "|   2|        7|      123|      0.6084945332211943|\n",
      "|   2|        7|      122|      0.5841161400512382|\n",
      "|   2|        7|      121|      0.5777964676198486|\n",
      "|   2|        7|      120|      0.6033684210526316|\n",
      "|   2|        7|      119|      0.5554150863885378|\n",
      "|   2|        7|      118|      0.5703805593764328|\n",
      "|   2|        7|      117|      0.5533683289588801|\n",
      "|   2|        7|      116|      0.5801385681293303|\n",
      "|   2|        7|      115|      0.5981308411214953|\n",
      "|   2|        7|      114|      0.6306306306306306|\n",
      "|   2|        7|      113|      0.6193633952254642|\n",
      "|   2|        7|      112|      0.5705521472392638|\n",
      "|   2|        7|      111|      0.5848724714160071|\n",
      "|   2|        7|      110|      0.5544412607449857|\n",
      "|   2|        7|      109|      0.5579839429081177|\n",
      "|   2|        7|      108|      0.5972346119536128|\n",
      "|   2|        7|      107|      0.5665254237288135|\n",
      "|   2|        7|      106|      0.5940254652301665|\n",
      "|   2|        7|      105|      0.5611285266457681|\n",
      "|   2|        7|      104|       0.563212927756654|\n",
      "|   2|        7|      103|      0.6054391440035667|\n",
      "|   2|        7|      102|      0.5976223136716964|\n",
      "|   2|        7|      101|      0.5558659217877095|\n",
      "|   2|        7|      100|       0.559040590405904|\n",
      "|   2|        7|       99|      0.5775780510879849|\n",
      "|   2|        7|       98|      0.5873015873015873|\n",
      "|   2|        7|       97|      0.5633410672853828|\n",
      "|   2|        7|       96|      0.5602171767028628|\n",
      "|   2|        7|       95|      0.6049618320610687|\n",
      "|   2|        7|       94|      0.6269430051813472|\n",
      "|   2|        7|       93|      0.6021450459652706|\n",
      "|   2|        7|       92|      0.5722460658082976|\n",
      "|   2|        7|       91|       0.607125890736342|\n",
      "|   2|        7|       90|      0.6147501213003397|\n",
      "|   2|        7|       89|      0.6031518624641834|\n",
      "|   2|        7|       88|      0.5836096636665088|\n",
      "|   2|        7|       87|      0.6409614421632449|\n",
      "|   2|        7|       86|      0.6494897959183673|\n",
      "|   2|        7|       85|      0.6856732151413513|\n",
      "|   2|        7|       84|      0.6708354177088545|\n",
      "|   2|        7|       83|      0.6485911749069644|\n",
      "|   2|        7|       82|      0.6147919876733436|\n",
      "|   2|        7|       81|       0.603598971722365|\n",
      "|   2|        7|       80|      0.6143931256713212|\n",
      "|   2|        7|       79|      0.6308758040573973|\n",
      "|   2|        7|       78|      0.6593301435406699|\n",
      "|   2|        7|       77|      0.5968054211035818|\n",
      "|   2|        7|       76|      0.6647673314339981|\n",
      "|   2|        7|       75|      0.5880398671096345|\n",
      "|   2|        7|       74|      0.5888324873096447|\n",
      "|   2|        7|       73|      0.6284704694598687|\n",
      "|   2|        7|       72|      0.6129184347006129|\n",
      "|   2|        7|       71|      0.6389280677009873|\n",
      "|   2|        7|       70|      0.5432269995376792|\n",
      "|   2|        7|       69|      0.5670198065407647|\n",
      "|   2|        7|       68|      0.5571095571095571|\n",
      "|   2|        7|       67|      0.5490105844454671|\n",
      "|   2|        7|       66|       0.536484625975218|\n",
      "|   2|        7|       65|      0.5490654205607477|\n",
      "|   2|        7|       64|      0.5694138386954606|\n",
      "|   2|        7|       63|      0.6136261766024205|\n",
      "|   2|        7|       62|      0.5985267034990792|\n",
      "|   2|        7|       61|      0.6174988547869904|\n",
      "|   2|        7|       60|      0.5622943112364833|\n",
      "|   2|        7|       59|      0.5859338612016768|\n",
      "|   2|        7|       58|      0.5953038674033149|\n",
      "|   2|        7|       57|      0.5959409594095941|\n",
      "|   2|        7|       56|      0.5541343079031521|\n",
      "|   2|        7|       55|      0.5453290381960424|\n",
      "|   2|        7|       54|       0.591743119266055|\n",
      "|   2|        7|       53|      0.5989061075660893|\n",
      "|   2|        7|       52|      0.5859843245735362|\n",
      "|   2|        7|       51|      0.5171568627450981|\n",
      "|   2|        7|       50|       0.553519768563163|\n",
      "|   2|        7|       49|      0.5415129151291513|\n",
      "|   2|        7|       48|      0.5274223034734917|\n",
      "|   2|        7|       47|      0.5641399416909622|\n",
      "|   2|        7|       46|      0.5306615161757605|\n",
      "|   2|        7|       45|      0.5225102319236017|\n",
      "|   2|        7|       44|      0.5254237288135594|\n",
      "|   2|        7|       43|      0.5402912621359224|\n",
      "|   2|        7|       42|      0.5291723202170964|\n",
      "|   2|        7|       41|      0.5601295097132285|\n",
      "|   2|        7|       40|      0.5714285714285714|\n",
      "|   2|        7|       39|      0.5825621042377009|\n",
      "|   2|        7|       38|      0.5612575127138234|\n",
      "|   2|        7|       37|      0.5940499040307101|\n",
      "|   2|        7|       36|      0.6024930747922438|\n",
      "|   2|        7|       35|       0.611085870059064|\n",
      "|   2|        7|       34|      0.5956502441189525|\n",
      "|   2|        7|       33|      0.5946798917944094|\n",
      "|   2|        7|       32|       0.615931721194879|\n",
      "|   2|        7|       31|      0.6136261766024205|\n",
      "|   2|        7|       30|      0.6095860566448802|\n",
      "|   2|        7|       29|      0.5701519213583557|\n",
      "|   2|        7|       28|       0.608433734939759|\n",
      "|   2|        7|       27|      0.5954403218596335|\n",
      "|   2|        7|       26|       0.620253164556962|\n",
      "|   2|        7|       25|      0.6138702460850112|\n",
      "|   2|        7|       24|       0.562247191011236|\n",
      "|   2|        7|       23|      0.5816993464052288|\n",
      "|   2|        7|       22|      0.5892286627110909|\n",
      "|   2|        7|       21|      0.5736154885186853|\n",
      "|   2|        7|       20|      0.6089681774349084|\n",
      "|   2|        7|       19|      0.5501618122977346|\n",
      "|   2|        7|       18|      0.5448588709677419|\n",
      "|   2|        7|       17|      0.5811642214860389|\n",
      "|   2|        7|       16|      0.5613382899628253|\n",
      "|   2|        7|       15|      0.5304036791006643|\n",
      "|   2|        7|       14|      0.5402750491159135|\n",
      "|   2|        7|       13|      0.5538169748696065|\n",
      "|   2|        7|       12|     0.49836065573770494|\n",
      "|   2|        7|       11|      0.4195488721804511|\n",
      "|   2|        7|       10|      0.4899559039686428|\n",
      "|   2|        7|        9|     0.46116970278044106|\n",
      "|   2|        7|        8|      0.5294392523364486|\n",
      "|   2|        7|        7|     0.44595864661654133|\n",
      "|   2|        7|        6|      0.4298621017593913|\n",
      "|   2|        7|        5|      0.4014851485148515|\n",
      "|   2|        7|        4|     0.38365719980069757|\n",
      "|   2|        7|        3|        0.36980947728383|\n",
      "|   2|        7|        2|     0.22475401346452614|\n",
      "|   2|        7|        1|     0.02948525737131434|\n",
      "|   2|       14|      180|      0.5582948125321007|\n",
      "|   2|       14|      179|      0.5650504080652905|\n",
      "|   2|       14|      178|      0.5628310062590275|\n",
      "|   2|       14|      177|      0.5629807692307692|\n",
      "|   2|       14|      176|      0.5562814070351759|\n",
      "|   2|       14|      175|      0.5835762876579204|\n",
      "|   2|       14|      174|      0.5670362903225806|\n",
      "|   2|       14|      173|      0.5952725518572117|\n",
      "|   2|       14|      172|      0.6071789686552073|\n",
      "|   2|       14|      171|      0.6256632899179932|\n",
      "|   2|       14|      170|      0.6154216867469879|\n",
      "|   2|       14|      169|      0.6238670694864048|\n",
      "|   2|       14|      168|       0.603998096144693|\n",
      "|   2|       14|      167|      0.6084656084656085|\n",
      "|   2|       14|      166|      0.5597269624573379|\n",
      "|   2|       14|      165|      0.6154239019407559|\n",
      "|   2|       14|      164|      0.5582039911308204|\n",
      "|   2|       14|      163|      0.5989173228346457|\n",
      "|   2|       14|      162|      0.5912098298676749|\n",
      "|   2|       14|      161|      0.6195048004042446|\n",
      "|   2|       14|      160|      0.6261501210653753|\n",
      "|   2|       14|      159|       0.611138986452584|\n",
      "|   2|       14|      158|      0.6051859099804305|\n",
      "|   2|       14|      157|      0.6257011728709841|\n",
      "|   2|       14|      156|      0.6350435624394967|\n",
      "|   2|       14|      155|      0.5901298701298702|\n",
      "|   2|       14|      154|      0.6311597809855649|\n",
      "|   2|       14|      153|      0.6173870333988212|\n",
      "|   2|       14|      152|      0.6053140096618358|\n",
      "|   2|       14|      151|      0.5098877605558525|\n",
      "|   2|       14|      150|      0.5553921568627451|\n",
      "|   2|       14|      149|      0.5925742574257425|\n",
      "|   2|       14|      148|      0.5745721271393643|\n",
      "|   2|       14|      147|      0.5723552894211577|\n",
      "|   2|       14|      146|      0.5482160077145612|\n",
      "|   2|       14|      145|      0.5730804810360777|\n",
      "|   2|       14|      144|      0.5879868606288128|\n",
      "|   2|       14|      143|      0.5856067732831609|\n",
      "|   2|       14|      142|      0.5310344827586206|\n",
      "|   2|       14|      141|      0.5364020666979803|\n",
      "|   2|       14|      140|      0.5419175544233441|\n",
      "|   2|       14|      139|      0.5829683698296837|\n",
      "|   2|       14|      138|      0.5849146110056926|\n",
      "|   2|       14|      137|      0.5564435564435565|\n",
      "|   2|       14|      136|      0.5973967684021544|\n",
      "|   2|       14|      135|      0.5911528150134048|\n",
      "|   2|       14|      134|      0.5772870662460567|\n",
      "|   2|       14|      133|      0.5430728241563055|\n",
      "|   2|       14|      132|       0.513548951048951|\n",
      "|   2|       14|      131|      0.5356521739130434|\n",
      "|   2|       14|      130|      0.5601280292638318|\n",
      "|   2|       14|      129|      0.5511879049676026|\n",
      "|   2|       14|      128|       0.548885077186964|\n",
      "|   2|       14|      127|      0.5651423641069888|\n",
      "|   2|       14|      126|      0.5731019522776573|\n",
      "|   2|       14|      125|      0.5931477516059958|\n",
      "|   2|       14|      124|       0.586411889596603|\n",
      "|   2|       14|      123|      0.6065226598898772|\n",
      "|   2|       14|      122|      0.6259640102827764|\n",
      "|   2|       14|      121|      0.6264805414551607|\n",
      "|   2|       14|      120|      0.6055846422338569|\n",
      "|   2|       14|      119|      0.6009615384615384|\n",
      "|   2|       14|      118|      0.6154188948306596|\n",
      "|   2|       14|      117|      0.5797227036395147|\n",
      "|   2|       14|      116|      0.5887201735357918|\n",
      "|   2|       14|      115|      0.5754290876242095|\n",
      "|   2|       14|      114|      0.6072214883311317|\n",
      "|   2|       14|      113|      0.5777190664905328|\n",
      "|   2|       14|      112|      0.5604770017035775|\n",
      "|   2|       14|      111|      0.5775716694772344|\n",
      "|   2|       14|      110|      0.5499316005471956|\n",
      "|   2|       14|      109|      0.5876424189307625|\n",
      "|   2|       14|      108|      0.5874587458745875|\n",
      "|   2|       14|      107|      0.6063088897992626|\n",
      "|   2|       14|      106|      0.6147260273972602|\n",
      "|   2|       14|      105|      0.5875109938434476|\n",
      "|   2|       14|      104|      0.5954664341761116|\n",
      "|   2|       14|      103|      0.5714899098325461|\n",
      "|   2|       14|      102|      0.5863996273870516|\n",
      "|   2|       14|      101|      0.5931268561731013|\n",
      "|   2|       14|      100|      0.5758401453224341|\n",
      "|   2|       14|       99|      0.5951492537313433|\n",
      "|   2|       14|       98|      0.5765456329735035|\n",
      "|   2|       14|       97|      0.5604545454545454|\n",
      "|   2|       14|       96|      0.6151012891344383|\n",
      "|   2|       14|       95|      0.5812854442344045|\n",
      "|   2|       14|       94|      0.5799685369690614|\n",
      "|   2|       14|       93|      0.5564892623716153|\n",
      "|   2|       14|       92|      0.5998085208233604|\n",
      "|   2|       14|       91|      0.6044847328244275|\n",
      "|   2|       14|       90|      0.5953596287703016|\n",
      "|   2|       14|       89|      0.5889309366130558|\n",
      "|   2|       14|       88|      0.6056603773584905|\n",
      "|   2|       14|       87|      0.6449343339587242|\n",
      "|   2|       14|       86|      0.6441983630235917|\n",
      "|   2|       14|       85|      0.5930287329251059|\n",
      "|   2|       14|       84|      0.6195069667738478|\n",
      "|   2|       14|       83|      0.5852968897266729|\n",
      "|   2|       14|       82|      0.6023640661938534|\n",
      "|   2|       14|       81|      0.6131984585741811|\n",
      "|   2|       14|       80|      0.6579710144927536|\n",
      "|   2|       14|       79|        0.64733395696913|\n",
      "|   2|       14|       78|      0.6756885090218424|\n",
      "|   2|       14|       77|      0.7009302325581396|\n",
      "|   2|       14|       76|      0.6845114345114345|\n",
      "|   2|       14|       75|      0.6510617760617761|\n",
      "|   2|       14|       74|      0.6447628458498024|\n",
      "|   2|       14|       73|      0.6449494949494949|\n",
      "|   2|       14|       72|      0.6625062406390414|\n",
      "|   2|       14|       71|      0.7026768642447419|\n",
      "|   2|       14|       70|      0.6026355803345159|\n",
      "|   2|       14|       69|      0.6228971962616823|\n",
      "|   2|       14|       68|      0.5688847235238987|\n",
      "|   2|       14|       67|      0.5685224839400428|\n",
      "|   2|       14|       66|      0.6330409356725146|\n",
      "|   2|       14|       65|       0.595974472263132|\n",
      "|   2|       14|       64|      0.6072234762979684|\n",
      "|   2|       14|       63|      0.5701914311759344|\n",
      "|   2|       14|       62|      0.5802919708029197|\n",
      "|   2|       14|       61|      0.5575903614457831|\n",
      "|   2|       14|       60|      0.5521165225307237|\n",
      "|   2|       14|       59|      0.5526079136690647|\n",
      "|   2|       14|       58|      0.5660027790643817|\n",
      "|   2|       14|       57|      0.5768217734855136|\n",
      "|   2|       14|       56|       0.581756428918001|\n",
      "|   2|       14|       55|      0.5682131373449701|\n",
      "|   2|       14|       54|      0.6148681055155876|\n",
      "|   2|       14|       53|      0.5912408759124088|\n",
      "|   2|       14|       52|      0.5539971949509116|\n",
      "|   2|       14|       51|      0.6137299771167049|\n",
      "|   2|       14|       50|       0.645083932853717|\n",
      "|   2|       14|       49|      0.5919384057971014|\n",
      "|   2|       14|       48|      0.5687344913151364|\n",
      "|   2|       14|       47|      0.6030013642564802|\n",
      "|   2|       14|       46|      0.6022883295194508|\n",
      "|   2|       14|       45|      0.6074108818011257|\n",
      "|   2|       14|       44|      0.5557586837294333|\n",
      "|   2|       14|       43|      0.5841720036596524|\n",
      "|   2|       14|       42|      0.5329038196042338|\n",
      "|   2|       14|       41|      0.5246575342465754|\n",
      "|   2|       14|       40|      0.5132991133924405|\n",
      "|   2|       14|       39|      0.5625570776255707|\n",
      "|   2|       14|       38|      0.5183420676512626|\n",
      "|   2|       14|       37|       0.515370705244123|\n",
      "|   2|       14|       36|      0.5457875457875457|\n",
      "|   2|       14|       35|      0.5647644927536232|\n",
      "|   2|       14|       34|      0.5275800711743772|\n",
      "|   2|       14|       33|      0.5339981867633726|\n",
      "|   2|       14|       32|      0.5550983081847279|\n",
      "|   2|       14|       31|      0.5175059952038369|\n",
      "|   2|       14|       30|      0.5407336827060505|\n",
      "|   2|       14|       29|      0.5933589990375361|\n",
      "|   2|       14|       28|      0.5824693685202639|\n",
      "|   2|       14|       27|      0.6244705882352941|\n",
      "|   2|       14|       26|      0.6348847718029824|\n",
      "|   2|       14|       25|      0.6087743102668476|\n",
      "|   2|       14|       24|      0.6109725685785536|\n",
      "|   2|       14|       23|      0.5929515418502203|\n",
      "|   2|       14|       22|      0.6184269662921348|\n",
      "|   2|       14|       21|      0.6465201465201466|\n",
      "|   2|       14|       20|      0.5951589938300902|\n",
      "|   2|       14|       19|      0.6071428571428571|\n",
      "|   2|       14|       18|      0.5905072126570498|\n",
      "|   2|       14|       17|      0.5806302131603337|\n",
      "|   2|       14|       16|       0.577036310107949|\n",
      "|   2|       14|       15|      0.5652577798420808|\n",
      "|   2|       14|       14|      0.5495251017639078|\n",
      "|   2|       14|       13|      0.5701923076923077|\n",
      "|   2|       14|       12|      0.5373961218836565|\n",
      "|   2|       14|       11|      0.4992947813822285|\n",
      "|   2|       14|       10|      0.5035030359645025|\n",
      "|   2|       14|        9|      0.5158434296365331|\n",
      "|   2|       14|        8|     0.49252291365171247|\n",
      "|   2|       14|        7|      0.4186279419128693|\n",
      "|   2|       14|        6|      0.4829867674858223|\n",
      "|   2|       14|        5|      0.4145490782262083|\n",
      "|   2|       14|        4|      0.3601161665053243|\n",
      "|   2|       14|        3|      0.3426371511068335|\n",
      "|   2|       14|        2|     0.17262512768130744|\n",
      "|   2|       14|        1|    0.021068103870651642|\n",
      "|   2|       21|      180|      0.5463864684777038|\n",
      "|   2|       21|      179|      0.5548634403449928|\n",
      "|   2|       21|      178|      0.5142721217887726|\n",
      "|   2|       21|      177|      0.5291847563917028|\n",
      "|   2|       21|      176|      0.5613453339649456|\n",
      "|   2|       21|      175|      0.5425685425685426|\n",
      "|   2|       21|      174|      0.5216152019002376|\n",
      "|   2|       21|      173|      0.5610795454545454|\n",
      "|   2|       21|      172|      0.5856610800744879|\n",
      "|   2|       21|      171|       0.584385226741468|\n",
      "|   2|       21|      170|      0.5830972615675165|\n",
      "|   2|       21|      169|      0.6061814556331007|\n",
      "|   2|       21|      168|      0.6031443544545021|\n",
      "|   2|       21|      167|       0.586852207293666|\n",
      "|   2|       21|      166|      0.5835030549898167|\n",
      "|   2|       21|      165|      0.5914954610606784|\n",
      "|   2|       21|      164|       0.583945178658835|\n",
      "|   2|       21|      163|      0.6177325581395349|\n",
      "|   2|       21|      162|      0.6256736893679569|\n",
      "|   2|       21|      161|      0.6198830409356725|\n",
      "|   2|       21|      160|      0.6352052855120339|\n",
      "|   2|       21|      159|      0.5870483341154388|\n",
      "|   2|       21|      158|      0.6115537848605578|\n",
      "|   2|       21|      157|      0.6006051437216339|\n",
      "|   2|       21|      156|      0.6299449173760641|\n",
      "|   2|       21|      155|      0.6042094455852156|\n",
      "|   2|       21|      154|      0.6190019193857965|\n",
      "|   2|       21|      153|      0.6100961538461539|\n",
      "|   2|       21|      152|       0.622909275215408|\n",
      "|   2|       21|      151|      0.6259780907668232|\n",
      "|   2|       21|      150|      0.6547733847637416|\n",
      "|   2|       21|      149|      0.6237251744498121|\n",
      "|   2|       21|      148|      0.6214389183969097|\n",
      "|   2|       21|      147|      0.6310444874274661|\n",
      "|   2|       21|      146|       0.648023143683703|\n",
      "|   2|       21|      145|      0.6287482151356497|\n",
      "|   2|       21|      144|      0.5559864275327193|\n",
      "|   2|       21|      143|      0.5586450960566228|\n",
      "|   2|       21|      142|      0.5909317389138017|\n",
      "|   2|       21|      141|      0.5649320206282231|\n",
      "|   2|       21|      140|      0.5228235294117647|\n",
      "|   2|       21|      139|      0.5374368396876436|\n",
      "|   2|       21|      138|      0.5441243712848651|\n",
      "|   2|       21|      137|      0.6172718351324828|\n",
      "|   2|       21|      136|      0.5964749536178108|\n",
      "|   2|       21|      135|      0.5699123211813567|\n",
      "|   2|       21|      134|      0.5523069894929191|\n",
      "|   2|       21|      133|      0.5528492647058824|\n",
      "|   2|       21|      132|       0.597890875745071|\n",
      "|   2|       21|      131|      0.6137589928057554|\n",
      "|   2|       21|      130|      0.5821554770318021|\n",
      "|   2|       21|      129|      0.6030082041932543|\n",
      "|   2|       21|      128|      0.6024096385542169|\n",
      "|   2|       21|      127|      0.5844642021525503|\n",
      "|   2|       21|      126|      0.5520403484640073|\n",
      "|   2|       21|      125|       0.554423344140806|\n",
      "|   2|       21|      124|      0.5678260869565217|\n",
      "|   2|       21|      123|      0.5956989247311828|\n",
      "|   2|       21|      122|      0.5771079073831368|\n",
      "|   2|       21|      121|      0.5781928757602085|\n",
      "|   2|       21|      120|      0.6037084950409659|\n",
      "|   2|       21|      119|      0.5881590319792567|\n",
      "|   2|       21|      118|      0.5974137931034482|\n",
      "|   2|       21|      117|       0.579246935201401|\n",
      "|   2|       21|      116|      0.5759275237273511|\n",
      "|   2|       21|      115|      0.6038843721770552|\n",
      "|   2|       21|      114|      0.6194214876033057|\n",
      "|   2|       21|      113|      0.6381340579710145|\n",
      "|   2|       21|      112|      0.6036347901341411|\n",
      "|   2|       21|      111|      0.6373673036093418|\n",
      "|   2|       21|      110|      0.5935314685314685|\n",
      "|   2|       21|      109|      0.6115451388888888|\n",
      "|   2|       21|      108|      0.6098374322634431|\n",
      "|   2|       21|      107|      0.5808792146820316|\n",
      "|   2|       21|      106|      0.5870967741935483|\n",
      "|   2|       21|      105|      0.5700365408038977|\n",
      "|   2|       21|      104|      0.5813953488372093|\n",
      "|   2|       21|      103|      0.5356398499374739|\n",
      "|   2|       21|      102|      0.5913007796471071|\n",
      "|   2|       21|      101|      0.5926443202979516|\n",
      "|   2|       21|      100|      0.5945827872433377|\n",
      "|   2|       21|       99|       0.600909090909091|\n",
      "|   2|       21|       98|      0.5675675675675675|\n",
      "|   2|       21|       97|      0.5743913435527502|\n",
      "|   2|       21|       96|      0.5938095238095238|\n",
      "|   2|       21|       95|      0.5741718674987998|\n",
      "|   2|       21|       94|      0.5644850818094321|\n",
      "|   2|       21|       93|      0.5658379373848987|\n",
      "|   2|       21|       92|      0.5825882352941176|\n",
      "|   2|       21|       91|      0.5766590389016019|\n",
      "|   2|       21|       90|      0.5653792461610051|\n",
      "|   2|       21|       89|      0.5969387755102041|\n",
      "|   2|       21|       88|      0.5937059652418976|\n",
      "|   2|       21|       87|      0.5738610216290843|\n",
      "|   2|       21|       86|      0.5836829836829837|\n",
      "|   2|       21|       85|      0.6048689138576779|\n",
      "|   2|       21|       84|       0.599905303030303|\n",
      "|   2|       21|       83|      0.5949953660797034|\n",
      "|   2|       21|       82|      0.5812256809338522|\n",
      "|   2|       21|       81|      0.6354268891069677|\n",
      "|   2|       21|       80|      0.6384579219558063|\n",
      "|   2|       21|       79|      0.6115591397849462|\n",
      "|   2|       21|       78|      0.5868673050615595|\n",
      "|   2|       21|       77|      0.6357074109720885|\n",
      "|   2|       21|       76|      0.6004739336492891|\n",
      "|   2|       21|       75|       0.633887349953832|\n",
      "|   2|       21|       74|       0.599247412982126|\n",
      "|   2|       21|       73|      0.6606974552309143|\n",
      "|   2|       21|       72|      0.6774791473586654|\n",
      "|   2|       21|       71|      0.7106859542697154|\n",
      "|   2|       21|       70|       0.707807386629266|\n",
      "|   2|       21|       69|      0.6903864278982093|\n",
      "|   2|       21|       68|      0.6711445459430414|\n",
      "|   2|       21|       67|      0.6145833333333334|\n",
      "|   2|       21|       66|      0.6317380352644836|\n",
      "|   2|       21|       65|       0.648444863336475|\n",
      "|   2|       21|       64|      0.6743205248359887|\n",
      "|   2|       21|       63|      0.6009433962264151|\n",
      "|   2|       21|       62|      0.6454164727780363|\n",
      "|   2|       21|       61|      0.5732838589981447|\n",
      "|   2|       21|       60|      0.5708376421923474|\n",
      "|   2|       21|       59|      0.6440366972477064|\n",
      "|   2|       21|       58|      0.5874661857529305|\n",
      "|   2|       21|       57|      0.5936599423631124|\n",
      "|   2|       21|       56|        0.53189448441247|\n",
      "|   2|       21|       55|      0.5561345333964945|\n",
      "|   2|       21|       54|      0.5776722090261283|\n",
      "|   2|       21|       53|      0.5683008091385054|\n",
      "|   2|       21|       52|      0.5143651529193698|\n",
      "|   2|       21|       51|      0.5614358030372757|\n",
      "|   2|       21|       50|      0.5828779599271403|\n",
      "|   2|       21|       49|      0.6052884615384615|\n",
      "|   2|       21|       48|      0.6441999082989455|\n",
      "|   2|       21|       47|      0.6509390746678883|\n",
      "|   2|       21|       46|       0.594019030357952|\n",
      "|   2|       21|       45|      0.5997292418772563|\n",
      "|   2|       21|       44|      0.6334117647058823|\n",
      "|   2|       21|       43|      0.6501590186278964|\n",
      "|   2|       21|       42|      0.6084392014519057|\n",
      "|   2|       21|       41|      0.5554440541896638|\n",
      "|   2|       21|       40|      0.6075555555555555|\n",
      "|   2|       21|       39|      0.6133997283838841|\n",
      "|   2|       21|       38|      0.5711091234347049|\n",
      "|   2|       21|       37|      0.5463546354635463|\n",
      "|   2|       21|       36|      0.5772163965681602|\n",
      "|   2|       21|       35|      0.5469537333944113|\n",
      "|   2|       21|       34|      0.5525568181818182|\n",
      "|   2|       21|       33|      0.5318471337579618|\n",
      "|   2|       21|       32|      0.5336021505376344|\n",
      "|   2|       21|       31|      0.5094086021505376|\n",
      "|   2|       21|       30|      0.5110692416391899|\n",
      "|   2|       21|       29|      0.5398104265402843|\n",
      "|   2|       21|       28|      0.5336470588235294|\n",
      "|   2|       21|       27|      0.5633423180592992|\n",
      "|   2|       21|       26|      0.5843720038350911|\n",
      "|   2|       21|       25|      0.5794824399260629|\n",
      "|   2|       21|       24|      0.5735970561177552|\n",
      "|   2|       21|       23|      0.5685920577617328|\n",
      "|   2|       21|       22|      0.5870936438622029|\n",
      "|   2|       21|       21|      0.5939643347050755|\n",
      "|   2|       21|       20|      0.6073038773669973|\n",
      "|   2|       21|       19|      0.6382880809940175|\n",
      "|   2|       21|       18|       0.604390243902439|\n",
      "|   2|       21|       17|      0.6215585627624824|\n",
      "|   2|       21|       16|      0.5954198473282443|\n",
      "|   2|       21|       15|      0.5668498168498168|\n",
      "|   2|       21|       14|      0.5882088208820883|\n",
      "|   2|       21|       13|      0.5256645279560037|\n",
      "|   2|       21|       12|      0.6198050282195998|\n",
      "|   2|       21|       11|      0.5696436834798704|\n",
      "|   2|       21|       10|      0.5027803521779426|\n",
      "|   2|       21|        9|      0.5014044943820225|\n",
      "|   2|       21|        8|      0.5011655011655012|\n",
      "|   2|       21|        7|     0.44365192582025675|\n",
      "|   2|       21|        6|     0.49359886201991465|\n",
      "|   2|       21|        5|      0.4573229873908826|\n",
      "|   2|       21|        4|     0.38387096774193546|\n",
      "|   2|       21|        3|       0.299079754601227|\n",
      "|   2|       21|        2|      0.2178743961352657|\n",
      "|   2|       21|        1|    0.018518518518518517|\n",
      "|   2|       28|      180|      0.5510416666666667|\n",
      "|   2|       28|      179|      0.5257387988560533|\n",
      "|   2|       28|      178|      0.5210930009587728|\n",
      "|   2|       28|      177|      0.5292225201072386|\n",
      "|   2|       28|      176|      0.5440594059405941|\n",
      "|   2|       28|      175|      0.5385395537525355|\n",
      "|   2|       28|      174|      0.5151798225128444|\n",
      "|   2|       28|      173|      0.5589478628464067|\n",
      "|   2|       28|      172|       0.574108818011257|\n",
      "|   2|       28|      171|      0.5346153846153846|\n",
      "|   2|       28|      170|      0.5457129322595926|\n",
      "|   2|       28|      169|      0.6032273374466065|\n",
      "|   2|       28|      168|        0.59628217349857|\n",
      "|   2|       28|      167|      0.5526819923371648|\n",
      "|   2|       28|      166|      0.5753756665050896|\n",
      "|   2|       28|      165|      0.6098049024512256|\n",
      "|   2|       28|      164|      0.5930359085963003|\n",
      "|   2|       28|      163|      0.5938735177865613|\n",
      "|   2|       28|      162|      0.6017791732077447|\n",
      "|   2|       28|      161|      0.6302408563782337|\n",
      "|   2|       28|      160|      0.6019002375296912|\n",
      "|   2|       28|      159|      0.6080378250591016|\n",
      "|   2|       28|      158|      0.6050538137576041|\n",
      "|   2|       28|      157|      0.6014319809069213|\n",
      "|   2|       28|      156|                   0.625|\n",
      "|   2|       28|      155|      0.6202594810379242|\n",
      "|   2|       28|      154|       0.627931214174049|\n",
      "|   2|       28|      153|      0.6054081121682524|\n",
      "|   2|       28|      152|      0.5764760576476058|\n",
      "|   2|       28|      151|      0.6332391713747646|\n",
      "|   2|       28|      150|      0.5746847720659554|\n",
      "|   2|       28|      149|      0.5887214389888187|\n",
      "|   2|       28|      148|      0.5834923664122137|\n",
      "|   2|       28|      147|      0.6293807009121459|\n",
      "|   2|       28|      146|      0.6071263304025913|\n",
      "|   2|       28|      145|      0.5923041685753551|\n",
      "|   2|       28|      144|      0.6601992658626115|\n",
      "|   2|       28|      143|      0.6357779980178394|\n",
      "|   2|       28|      142|       0.624367816091954|\n",
      "|   2|       28|      141|      0.6000920386562356|\n",
      "|   2|       28|      140|      0.6096997690531177|\n",
      "|   2|       28|      139|      0.6207366984993179|\n",
      "|   2|       28|      138|       0.594792142530836|\n",
      "|   2|       28|      137|      0.5538250114521301|\n",
      "|   2|       28|      136|      0.5693933823529411|\n",
      "|   2|       28|      135|      0.5925079945180448|\n",
      "|   2|       28|      134|      0.5793611793611794|\n",
      "|   2|       28|      133|      0.5462920313219715|\n",
      "|   2|       28|      132|      0.5671414038657172|\n",
      "|   2|       28|      131|      0.5862068965517241|\n",
      "|   2|       28|      130|      0.5771428571428572|\n",
      "|   2|       28|      129|      0.6025763358778626|\n",
      "|   2|       28|      128|      0.5785388127853881|\n",
      "|   2|       28|      127|      0.5658653846153846|\n",
      "|   2|       28|      126|      0.5612590799031477|\n",
      "|   2|       28|      125|       0.596023497514686|\n",
      "|   2|       28|      124|      0.6412110257568912|\n",
      "|   2|       28|      123|      0.6060741402411791|\n",
      "|   2|       28|      122|      0.6300089047195013|\n",
      "|   2|       28|      121|       0.623622741295725|\n",
      "|   2|       28|      120|      0.6034406704896339|\n",
      "|   2|       28|      119|      0.5794599380256751|\n",
      "|   2|       28|      118|      0.5488185465893892|\n",
      "|   2|       28|      117|      0.5367860600193611|\n",
      "|   2|       28|      116|      0.5921343349536015|\n",
      "|   2|       28|      115|      0.5575916230366492|\n",
      "|   2|       28|      114|      0.5863471971066908|\n",
      "|   2|       28|      113|      0.5778670236501562|\n",
      "|   2|       28|      112|      0.5730941704035875|\n",
      "|   2|       28|      111|      0.6085470085470085|\n",
      "|   2|       28|      110|      0.5946784922394679|\n",
      "|   2|       28|      109|      0.5866845397676497|\n",
      "|   2|       28|      108|      0.6181150550795593|\n",
      "|   2|       28|      107|      0.6254071661237784|\n",
      "|   2|       28|      106|      0.6375310687655343|\n",
      "|   2|       28|      105|      0.6245398773006134|\n",
      "|   2|       28|      104|      0.6353684210526316|\n",
      "|   2|       28|      103|       0.591715976331361|\n",
      "|   2|       28|      102|      0.5970790378006873|\n",
      "|   2|       28|      101|      0.6018641810918774|\n",
      "|   2|       28|      100|      0.5413957520589511|\n",
      "|   2|       28|       99|      0.5902417962003454|\n",
      "|   2|       28|       98|        0.56239092495637|\n",
      "|   2|       28|       97|      0.5734450816056462|\n",
      "|   2|       28|       96|      0.5487640449438203|\n",
      "|   2|       28|       95|        0.57439293598234|\n",
      "|   2|       28|       94|      0.5850492390331244|\n",
      "|   2|       28|       93|      0.5709235528060097|\n",
      "|   2|       28|       92|      0.6001805054151624|\n",
      "|   2|       28|       91|      0.5596085409252669|\n",
      "|   2|       28|       90|      0.6160071942446043|\n",
      "|   2|       28|       89|      0.5708502024291497|\n",
      "|   2|       28|       88|       0.585447582575395|\n",
      "|   2|       28|       87|      0.5985533453887885|\n",
      "|   2|       28|       86|      0.5946321147616844|\n",
      "|   2|       28|       85|      0.5909510618651893|\n",
      "|   2|       28|       84|      0.5650161962054604|\n",
      "|   2|       28|       83|      0.5866417561886969|\n",
      "|   2|       28|       82|       0.593978102189781|\n",
      "|   2|       28|       81|      0.6354823073194377|\n",
      "|   2|       28|       80|      0.5849503076194983|\n",
      "|   2|       28|       79|      0.5679824561403509|\n",
      "|   2|       28|       78|      0.6096551724137931|\n",
      "|   2|       28|       77|      0.6071428571428571|\n",
      "|   2|       28|       76|      0.5743589743589743|\n",
      "|   2|       28|       75|       0.572463768115942|\n",
      "|   2|       28|       74|      0.6244218316373727|\n",
      "|   2|       28|       73|       0.650301065308013|\n",
      "|   2|       28|       72|      0.6388634280476627|\n",
      "|   2|       28|       71|      0.6123491179201486|\n",
      "|   2|       28|       70|      0.6300294406280668|\n",
      "|   2|       28|       69|      0.6251754796443613|\n",
      "|   2|       28|       68|      0.6007428040854225|\n",
      "|   2|       28|       67|      0.6010928961748634|\n",
      "|   2|       28|       66|       0.654320987654321|\n",
      "|   2|       28|       65|      0.6547619047619048|\n",
      "|   2|       28|       64|      0.7102587800369686|\n",
      "|   2|       28|       63|      0.7209729233593392|\n",
      "|   2|       28|       62|      0.6891495601173021|\n",
      "|   2|       28|       61|      0.6501883239171374|\n",
      "|   2|       28|       60|      0.5859632595383891|\n",
      "|   2|       28|       59|      0.6356011183597391|\n",
      "|   2|       28|       58|      0.6479331165815142|\n",
      "|   2|       28|       57|       0.629064039408867|\n",
      "|   2|       28|       56|      0.5672227674190383|\n",
      "|   2|       28|       55|      0.6389462809917356|\n",
      "|   2|       28|       54|      0.5648621041879469|\n",
      "|   2|       28|       53|      0.5728105906313645|\n",
      "|   2|       28|       52|      0.6020100502512563|\n",
      "|   2|       28|       51|      0.5731530661516175|\n",
      "|   2|       28|       50|      0.5921867555979038|\n",
      "|   2|       28|       49|      0.5258329309512313|\n",
      "|   2|       28|       48|      0.5462575546257554|\n",
      "|   2|       28|       47|      0.5684420109507218|\n",
      "|   2|       28|       46|       0.549047619047619|\n",
      "|   2|       28|       45|      0.5344497607655503|\n",
      "|   2|       28|       44|      0.5494350282485876|\n",
      "|   2|       28|       43|      0.5606343283582089|\n",
      "|   2|       28|       42|      0.5492610837438424|\n",
      "|   2|       28|       41|        0.68488253319714|\n",
      "|   2|       28|       40|      0.6304248515303792|\n",
      "|   2|       28|       39|      0.6038077969174978|\n",
      "|   2|       28|       38|       0.594170403587444|\n",
      "|   2|       28|       37|      0.6413387607417458|\n",
      "|   2|       28|       36|      0.6407156673114119|\n",
      "|   2|       28|       35|      0.6014330497089118|\n",
      "|   2|       28|       34|      0.5615348619560131|\n",
      "|   2|       28|       33|      0.5792764627065654|\n",
      "|   2|       28|       32|      0.5902439024390244|\n",
      "|   2|       28|       31|       0.587740978744439|\n",
      "|   2|       28|       30|      0.5357624831309041|\n",
      "|   2|       28|       29|      0.5644226482389657|\n",
      "|   2|       28|       28|      0.5297539149888143|\n",
      "|   2|       28|       27|      0.5389113810166442|\n",
      "|   2|       28|       26|      0.5510105871029837|\n",
      "|   2|       28|       25|      0.5451656831593282|\n",
      "|   2|       28|       24|       0.529359031824294|\n",
      "|   2|       28|       23|      0.5056331680937359|\n",
      "|   2|       28|       22|      0.5483576642335767|\n",
      "|   2|       28|       21|      0.5689733152419719|\n",
      "|   2|       28|       20|      0.5468146718146718|\n",
      "|   2|       28|       19|      0.5988909426987061|\n",
      "|   2|       28|       18|      0.5589390962671905|\n",
      "|   2|       28|       17|      0.6015144344533838|\n",
      "|   2|       28|       16|      0.5670816044260027|\n",
      "|   2|       28|       15|      0.5796831314072693|\n",
      "|   2|       28|       14|      0.5645454545454546|\n",
      "|   2|       28|       13|      0.5688868613138686|\n",
      "|   2|       28|       12|      0.5548543689320389|\n",
      "|   2|       28|       11|       0.563543599257885|\n",
      "|   2|       28|       10|       0.551276102088167|\n",
      "|   2|       28|        9|      0.5671785028790787|\n",
      "|   2|       28|        8|      0.5372625426205553|\n",
      "|   2|       28|        7|      0.5007481296758105|\n",
      "|   2|       28|        6|      0.5035095928872251|\n",
      "|   2|       28|        5|      0.5363044566850276|\n",
      "|   2|       28|        4|      0.4078635717669351|\n",
      "|   2|       28|        3|      0.2714217328865486|\n",
      "|   2|       28|        2|     0.21918465227817746|\n",
      "|   2|       28|        1|    0.023785425101214574|\n",
      "+----+---------+---------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ['table_name', 'trip', 'stay_days', 'start_date', 'company', 'dep_time_local', 'stop_info', 'duration']\n",
    "# groupby(flight2.trip, flight2.stay_days, flight.dep_time_group).agg(func.mean('price')).show()\n",
    "# flight6.groupBy(col('trip'), col('stay_days'), col('lead_time')).agg(F.mean('price_will_drop_num')).show()\n",
    "flight6.groupBy(col('trip'), col('stay_days'), col('lead_time')). \\\n",
    "    agg(F.mean('price_will_drop_num')).orderBy(['trip', 'stay_days', 'lead_time'], ascending=[1, 1, 0]).show(1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save to parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flight6.coalesce(2).write.parquet(\"C:\\\\s3\\\\20170503_jsonl\\\\flight6.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# flight6 = spark.read.parquet(\"C:\\\\s3\\\\20170503_jsonl\\\\flight6.parquet\")\n",
    "flight6 = spark.read.parquet(\"/home/ubuntu/parquet/flight6.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1835752"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flight6.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- airline_code: string (nullable = true)\n",
      " |-- airline_codes: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- arr_time: timestamp (nullable = true)\n",
      " |-- check_bag_not_inc: boolean (nullable = true)\n",
      " |-- company: string (nullable = true)\n",
      " |-- dep_time: timestamp (nullable = true)\n",
      " |-- duration: string (nullable = true)\n",
      " |-- flight_code: string (nullable = true)\n",
      " |-- flight_number: string (nullable = true)\n",
      " |-- from_city_name: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- index: long (nullable = true)\n",
      " |-- plane: string (nullable = true)\n",
      " |-- power: boolean (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- price_code: string (nullable = true)\n",
      " |-- search_date: timestamp (nullable = true)\n",
      " |-- span_days: integer (nullable = true)\n",
      " |-- start_date: date (nullable = true)\n",
      " |-- stay_days: integer (nullable = true)\n",
      " |-- stop: integer (nullable = true)\n",
      " |-- stop_info: string (nullable = true)\n",
      " |-- table_name: string (nullable = true)\n",
      " |-- task_id: string (nullable = true)\n",
      " |-- ticket_left: string (nullable = true)\n",
      " |-- to_city_name: string (nullable = true)\n",
      " |-- trip: string (nullable = true)\n",
      " |-- version: string (nullable = true)\n",
      " |-- video: boolean (nullable = true)\n",
      " |-- wifi: boolean (nullable = true)\n",
      " |-- dep_time_local: timestamp (nullable = true)\n",
      " |-- arr_time_local: timestamp (nullable = true)\n",
      " |-- duration_h: string (nullable = true)\n",
      " |-- duration_m: string (nullable = true)\n",
      " |-- stop_info1: string (nullable = true)\n",
      " |-- stop_info2: string (nullable = true)\n",
      " |-- stop_loc1: string (nullable = true)\n",
      " |-- stop_duration1: string (nullable = true)\n",
      " |-- stop_loc2: string (nullable = true)\n",
      " |-- stop_duration2: string (nullable = true)\n",
      " |-- stop_duration_h1: string (nullable = true)\n",
      " |-- stop_duration_m1: string (nullable = true)\n",
      " |-- stop_duration_h2: string (nullable = true)\n",
      " |-- stop_duration_m2: string (nullable = true)\n",
      " |-- duration_minutes: integer (nullable = true)\n",
      " |-- stop1_minutes: integer (nullable = true)\n",
      " |-- stop2_minutes: integer (nullable = true)\n",
      " |-- arr_time_group: string (nullable = true)\n",
      " |-- dep_time_group: string (nullable = true)\n",
      " |-- dep_weekday: string (nullable = true)\n",
      " |-- dep_weeknum: integer (nullable = true)\n",
      " |-- lead_time: integer (nullable = true)\n",
      " |-- start_date_str: string (nullable = true)\n",
      " |-- last_holiday: string (nullable = true)\n",
      " |-- next_holiday: string (nullable = true)\n",
      " |-- days_to_last_holiday: integer (nullable = true)\n",
      " |-- days_to_next_holiday: integer (nullable = true)\n",
      " |-- future_min_price: double (nullable = true)\n",
      " |-- price_will_drop: boolean (nullable = true)\n",
      " |-- price_will_drop_num: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flight6.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+----------------+--------------+------------+----+---------+--------------+-----------+-------------------------+---------+----+-----+-----+-----+-----------------+--------------+--------------+---------+-----------+-----------+--------------------+--------------------+----------------+------------+\n",
      "|price_will_drop_num|price  |few_tickets_left|from_city_name|to_city_name|trip|stay_days|company       |flight_code|plane                    |span_days|stop|power|video|wifi |check_bag_not_inc|dep_time_group|arr_time_group|lead_time|dep_weekday|dep_weeknum|days_to_last_holiday|days_to_next_holiday|duration_minutes|stop_minutes|\n",
      "+-------------------+-------+----------------+--------------+------------+----+---------+--------------+-----------+-------------------------+---------+----+-----+-----+-----+-----------------+--------------+--------------+---------+-----------+-----------+--------------------+--------------------+----------------+------------+\n",
      "|1                  |1671.42|true            |sydney        |shanghai    |1   |0        |Qantas Airways|QF129      |AIRBUS INDUSTRIE A330-300|0        |0   |false|true |false|false            |morning       |evening       |3        |4          |14         |-3                  |24                  |655             |0           |\n",
      "|0                  |949.42 |true            |sydney        |shanghai    |1   |0        |Qantas Airways|QF129      |AIRBUS INDUSTRIE A330-300|0        |0   |false|true |false|false            |morning       |evening       |1        |4          |14         |-3                  |24                  |655             |0           |\n",
      "+-------------------+-------+----------------+--------------+------------+----+---------+--------------+-----------+-------------------------+---------+----+-----+-----+-----+-----------------+--------------+--------------+---------+-----------+-----------+--------------------+--------------------+----------------+------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flight7 = (flight6.withColumn('price_drop_amt', col('price') - col('future_min_price'))\n",
    "                  .withColumn('stop_minutes', col('stop1_minutes') + col('stop2_minutes'))\n",
    "                  .withColumn('few_tickets_left', col('ticket_left') != '0')\n",
    "                  .select('price_will_drop_num', 'price', 'few_tickets_left', 'from_city_name', 'to_city_name', 'trip', 'stay_days',\n",
    "                          'company', 'flight_code', 'plane', 'span_days', 'stop',\n",
    "                         'power', 'video', 'wifi', 'check_bag_not_inc', \n",
    "                         'dep_time_group', 'arr_time_group',  \n",
    "                         'lead_time', 'dep_weekday', 'dep_weeknum', \n",
    "                         'days_to_last_holiday', 'days_to_next_holiday',                         \n",
    "                         'duration_minutes', 'stop_minutes'\n",
    "                      ).filter(flight6.price > 0))\n",
    "flight7.dtypes\n",
    "flight7 = flight7.na.fill({'stop_minutes': 0, 'days_to_next_holiday': 999})\n",
    "flight7.show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to transform categorical variables? See\n",
    "http://stackoverflow.com/questions/32982425/encode-and-assemble-multiple-features-in-pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# One hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "column_vec_in = ['from_city_name', 'to_city_name', 'trip', 'company', 'flight_code', 'plane', \n",
    "                 'arr_time_group', 'dep_time_group', 'dep_weekday']\n",
    "column_vec_out = ['from_city_name_catVec','to_city_name_catVec', 'trip_catVec',\n",
    "                'company_catVec', 'flight_code_catVec', 'plane_catVec', \n",
    "                  'arr_time_group_catVec', 'dep_time_group_catVec', 'dep_weekday_catVec']\n",
    " \n",
    "indexers = [StringIndexer(inputCol=x, outputCol=x+'_tmp') for x in column_vec_in ]\n",
    " \n",
    "encoders = [OneHotEncoder(dropLast=False, inputCol=x+\"_tmp\", outputCol=y)\n",
    "            for x,y in zip(column_vec_in, column_vec_out)]\n",
    "\n",
    "tmp = [[i,j] for i,j in zip(indexers, encoders)]\n",
    "tmp = [i for sublist in tmp for i in sublist]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer, VectorAssembler, SQLTransformer\n",
    "\n",
    "# prepare labeled sets\n",
    "# 'price_will_drop',\n",
    "\n",
    "cols_now = ['price', 'few_tickets_left', 'stay_days',\n",
    "              'span_days', 'stop',\n",
    "             'power', 'video', 'wifi', 'check_bag_not_inc',                          \n",
    "             'lead_time', 'dep_weeknum', \n",
    "             'days_to_last_holiday', 'days_to_next_holiday',                         \n",
    "             'duration_minutes', 'stop_minutes',\n",
    "            'from_city_name_catVec','to_city_name_catVec', 'trip_catVec',\n",
    "            'company_catVec', 'flight_code_catVec', 'plane_catVec', \n",
    "            'arr_time_group_catVec', 'dep_time_group_catVec', 'dep_weekday_catVec']\n",
    "\n",
    "assembler_features = VectorAssembler(inputCols=cols_now, outputCol='features')\n",
    "# labelIndexer = StringIndexer(inputCol='price', outputCol=\"label\")\n",
    "# tmp += [assembler_features, labelIndexer]\n",
    "tmp += [assembler_features]\n",
    "pipeline = Pipeline(stages=tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# flight3.na.drop().count()\n",
    "\n",
    "# from pyspark.ml import Pipeline\n",
    "# from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# indexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\")\\\n",
    "#             .fit(flight3) for column in list(set(flight3.columns)-\\\n",
    "#                                              set(['stay_days', 'power', 'price', 'span_days',\\\n",
    "#                                                   'stop', 'video', 'wifi', 'duration_minutes',\\\n",
    "#                                                  'dep_weeknum', 'lead_time']))]\n",
    "\n",
    "# pipeline = Pipeline(stages=indexers)\n",
    "# flight4 = pipeline.fit(flight3).transform(flight3)\n",
    "\n",
    "# flight4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+\n",
      "|from_city_name|  count|\n",
      "+--------------+-------+\n",
      "|        sydney|1835752|\n",
      "+--------------+-------+\n",
      "\n",
      "+------------+-------+\n",
      "|to_city_name|  count|\n",
      "+------------+-------+\n",
      "|    shanghai|1835752|\n",
      "+------------+-------+\n",
      "\n",
      "+----+-------+\n",
      "|trip|  count|\n",
      "+----+-------+\n",
      "|   1| 295267|\n",
      "|   2|1540485|\n",
      "+----+-------+\n",
      "\n",
      "+--------------------+------+\n",
      "|             company| count|\n",
      "+--------------------+------+\n",
      "|      Cathay Pacific|482702|\n",
      "| Philippine Airlines|   714|\n",
      "|    Juneyao Airlines|   701|\n",
      "|      Japan Airlines| 41421|\n",
      "|    Virgin Australia| 80045|\n",
      "|China Southern Ai...|230960|\n",
      "|     Hainan Airlines| 46185|\n",
      "|Thai Airways Inte...| 52266|\n",
      "|        Fiji Airways| 38559|\n",
      "|   Malaysia Airlines| 47129|\n",
      "|    Vietnam Airlines| 21673|\n",
      "|     Asiana Airlines| 30163|\n",
      "|      China Airlines|  8390|\n",
      "|          Air Canada|    62|\n",
      "|           Air China|134287|\n",
      "|  All Nippon Airways| 42670|\n",
      "|  Singapore Airlines| 72912|\n",
      "|            AirAsiaX| 66254|\n",
      "|    Sichuan Airlines| 25138|\n",
      "|LATAM Airlines Group|   370|\n",
      "|             Jetstar|     3|\n",
      "|      Qantas Airways|175007|\n",
      "|   Shanghai Airlines|     6|\n",
      "|     Xiamen Airlines| 43283|\n",
      "|     Air New Zealand| 32618|\n",
      "|          Korean Air| 40956|\n",
      "|China Eastern Air...| 99724|\n",
      "|    Garuda Indonesia| 21554|\n",
      "+--------------------+------+\n",
      "\n",
      "+-----------+------+\n",
      "|flight_code| count|\n",
      "+-----------+------+\n",
      "|     CZ3072| 19217|\n",
      "|      MU715|    69|\n",
      "|     HO1211|   596|\n",
      "|      NZ710|   674|\n",
      "|      TG663|  9166|\n",
      "|     MU6153|   474|\n",
      "|     CA1832|   506|\n",
      "|      QF127|  3566|\n",
      "|     CA1550|  1123|\n",
      "|     3U8974|  2728|\n",
      "|     CA1532|   348|\n",
      "|      MU292|    43|\n",
      "|      QF431|   139|\n",
      "|      QF130|  5953|\n",
      "|      QF457|   209|\n",
      "|      SQ827|    20|\n",
      "|        QF5|   875|\n",
      "|      QF471|   100|\n",
      "|      NZ286|  2949|\n",
      "|      QF461|  1802|\n",
      "|      QF451|   483|\n",
      "|       CI52|  6090|\n",
      "|     CA1836|  2199|\n",
      "|      SQ242| 25763|\n",
      "|      D7223| 26233|\n",
      "|      QF304|     7|\n",
      "|      MF808|  7357|\n",
      "|      CI504|  2300|\n",
      "|     KE5858|    39|\n",
      "|      CA175|  4661|\n",
      "|     MU5269|     1|\n",
      "|     MF8512|   906|\n",
      "|     VA5510|  4912|\n",
      "|      PR212|   714|\n",
      "|      CX365|    62|\n",
      "|      VN786|  7005|\n",
      "|      SQ232|  4545|\n",
      "|      VA816|   725|\n",
      "|     VA5412|  7953|\n",
      "|      SQ212| 12840|\n",
      "|      TG665|  5185|\n",
      "|     VA5547|    64|\n",
      "|      QF409|  8660|\n",
      "|      NZ706|  1215|\n",
      "|      TG476| 17746|\n",
      "|      VA804|  1666|\n",
      "|      VN523|  8019|\n",
      "|     JL5604|     2|\n",
      "|      CZ302|132064|\n",
      "|      MU561|  8233|\n",
      "|      CX138|219852|\n",
      "|     CA4514|   548|\n",
      "|      OZ364|  1403|\n",
      "|      QF417|  1310|\n",
      "|     MU5107|     3|\n",
      "|     CZ3532|  1529|\n",
      "|     MU5121|   149|\n",
      "|     CX5857|    27|\n",
      "|      QF508|  6941|\n",
      "|      QF342|    13|\n",
      "|      QF439|   231|\n",
      "|      MH389| 13004|\n",
      "|      QF516|   969|\n",
      "|      QF449|    82|\n",
      "|     VA5508|  7442|\n",
      "|     CZ3576|   762|\n",
      "|      VA806|  1396|\n",
      "|      QF453|  1305|\n",
      "|      QF302|  3065|\n",
      "|      MU750|  3146|\n",
      "|     MF8586|  3738|\n",
      "|     JL5642|     2|\n",
      "|     CZ3548|   425|\n",
      "|      QF129| 17526|\n",
      "|     CX5809|   588|\n",
      "|     MU5157|    31|\n",
      "|     CA1856|  2183|\n",
      "|      QF459|  1690|\n",
      "|      NZ702|   355|\n",
      "|     CX5831|    54|\n",
      "|     VA5654|   729|\n",
      "|      QF117|  2264|\n",
      "|     MU6141|   419|\n",
      "|      SQ252|  7534|\n",
      "|      SQ825|  4172|\n",
      "|      QF510|  9612|\n",
      "|     CZ5281|  2192|\n",
      "|     CZ3572|  1862|\n",
      "|     MU6785|   152|\n",
      "|      NZ110|  1670|\n",
      "|      QF477|    10|\n",
      "|      QF502|  1047|\n",
      "|     MU8439|  1231|\n",
      "|      SQ831|   442|\n",
      "|     KE5862|     1|\n",
      "|      MU562| 17678|\n",
      "|      CX162| 50255|\n",
      "|      QF401|  3339|\n",
      "|      QF455|  1246|\n",
      "|      D7221| 17060|\n",
      "|      QF497|   191|\n",
      "|     MU5804|   402|\n",
      "|     MF8518|  1168|\n",
      "|     MU5151|     2|\n",
      "|     MU6783|    77|\n",
      "|     CZ3596|   381|\n",
      "|     MF8510|  2227|\n",
      "|     MF8574|  1947|\n",
      "|     CZ5275|   230|\n",
      "|     MU6083|   361|\n",
      "|      NZ716|     5|\n",
      "|     CA3293|    17|\n",
      "|      QF514|   495|\n",
      "|      QF493|   227|\n",
      "|      NZ288|  1593|\n",
      "|     MU6081|   543|\n",
      "|     CZ3538|  3718|\n",
      "|      OZ366|  5308|\n",
      "|     CA1590|   349|\n",
      "|     MU8419|  1154|\n",
      "|     MU2154|   443|\n",
      "|      D7333|  4765|\n",
      "|      CA156|   725|\n",
      "|      QF427|   118|\n",
      "|      CA176| 13108|\n",
      "|      CA174| 87402|\n",
      "|     MU8440| 16560|\n",
      "|      QF504|  4891|\n",
      "|     CZ3504|  2290|\n",
      "|     MU5810|   165|\n",
      "|     QF4097|   105|\n",
      "|     CA1884|  1751|\n",
      "|     MU5153|     1|\n",
      "|     CX5871|    29|\n",
      "|      QF469|    80|\n",
      "|     MU5111|     1|\n",
      "|       VA35|    11|\n",
      "|      QF147|    61|\n",
      "|     MU2506|   268|\n",
      "|     CX5877|  3896|\n",
      "|      MU565|     1|\n",
      "|      QF405|  3223|\n",
      "|      VA812|   440|\n",
      "|      JL876|  5035|\n",
      "|     MU4202|   502|\n",
      "|      KE896|   341|\n",
      "|      QF447|   402|\n",
      "|     CX5897|  5960|\n",
      "|     VA5514| 29219|\n",
      "|     MU5103|     1|\n",
      "|      QF467|   415|\n",
      "|      MF802| 20909|\n",
      "|      CZ326| 38138|\n",
      "|     MU2469|   373|\n",
      "|     CZ6752|   265|\n",
      "|     CZ3554|  2678|\n",
      "|      MU736|  6496|\n",
      "|      MU735|  4373|\n",
      "|     CA1518|   903|\n",
      "|     CA1516|  1302|\n",
      "|     CX5813|  2346|\n",
      "|      QF419|  1291|\n",
      "|      MU739|   233|\n",
      "|     CZ3966|  2040|\n",
      "|     MU2508|   404|\n",
      "|      QF433|   301|\n",
      "|     VA5545|  2823|\n",
      "|      CZ380|   743|\n",
      "|     HU7994| 11693|\n",
      "|      CX369|  9248|\n",
      "|     MU5271|    67|\n",
      "|      QF443|   266|\n",
      "|     CZ3612|  1204|\n",
      "|     MU6113|   524|\n",
      "|       VA37|    20|\n",
      "|     VA5512| 11306|\n",
      "|      JL874|  1151|\n",
      "|     CZ3175|  2331|\n",
      "|     MF8568|  1053|\n",
      "|      QF429|    68|\n",
      "|      NZ718|  6621|\n",
      "|     MU5117|     3|\n",
      "|     KE5894|  2356|\n",
      "|      MU778| 21870|\n",
      "|      SQ833|  6915|\n",
      "|      QF506|  6243|\n",
      "|     QF5006|    94|\n",
      "|     MU5109|     5|\n",
      "|      MU545|     1|\n",
      "|     CA1947|   218|\n",
      "|      QF300|  5991|\n",
      "|     CZ3526|  6344|\n",
      "|      QF301| 16625|\n",
      "|      QF500|   959|\n",
      "|      VN772|  5407|\n",
      "|     CZ3645|    58|\n",
      "|      QF556|    14|\n",
      "|      D7331| 18196|\n",
      "|      MU728|  1308|\n",
      "|      NZ118|  2389|\n",
      "|     CZ3610|   535|\n",
      "|      SQ835|    72|\n",
      "|      QF407|  7568|\n",
      "|     MU5369|    52|\n",
      "|      TG472| 20164|\n",
      "|     CX5805|  4088|\n",
      "|     MU4204|   120|\n",
      "|     MU6087|  1314|\n",
      "|      MH122| 16777|\n",
      "|      QF437|   212|\n",
      "|      QF499|     5|\n",
      "|      QF411| 10543|\n",
      "|     VA5549|   691|\n",
      "|     CZ4010|   130|\n",
      "|      JL872|    74|\n",
      "|     FJ5322|  9081|\n",
      "|     CX5891|  7051|\n",
      "|      CX110|148250|\n",
      "|      QF441|   359|\n",
      "|      VA814|   542|\n",
      "|      QF512| 10498|\n",
      "|      QF403|  3291|\n",
      "|     CX5835|  5942|\n",
      "|      QF423|   118|\n",
      "|     CZ3582|  3120|\n",
      "|     MU5371|   166|\n",
      "|     HU7998| 32530|\n",
      "|     CX5865|  2692|\n",
      "|     CZ3236|  5040|\n",
      "|      QF367|   729|\n",
      "|      FM827|     6|\n",
      "|      KE898|  6598|\n",
      "|      OZ362|  1346|\n",
      "|      KE894|  1187|\n",
      "|      VA800|  2021|\n",
      "|      QF145|     6|\n",
      "|      MU737|   434|\n",
      "|     CA1520|   684|\n",
      "|      QF425|   145|\n",
      "|     QF5001|     1|\n",
      "|      JL772| 35157|\n",
      "|      QF435|   240|\n",
      "|      CX100| 10295|\n",
      "|      QF463|   462|\n",
      "|     CA3221|     2|\n",
      "|      NH880| 42670|\n",
      "|     MF8542|  1924|\n",
      "|     MU5123|    36|\n",
      "|      OZ602| 21762|\n",
      "|     MU6143|   355|\n",
      "|     MU5812|    68|\n",
      "|       QF81|  1574|\n",
      "|     MU5119|    13|\n",
      "|     QF1333|    11|\n",
      "|     CX5893|  4302|\n",
      "|     FJ5320| 29478|\n",
      "|      QF415|  7023|\n",
      "|     MU6149|   648|\n",
      "|     VA5551|  3316|\n",
      "|      LA800|   370|\n",
      "|     NZ3892|  4779|\n",
      "|     CA4504|  1689|\n",
      "|     KE5886|   473|\n",
      "|      GA895|  2306|\n",
      "|     KE5864|    19|\n",
      "|     3U8972|   930|\n",
      "|     NZ3891|   229|\n",
      "|      QF479|  1401|\n",
      "|     KE5808|    50|\n",
      "|     MU5802|    70|\n",
      "|      QF445|   103|\n",
      "|     TG4761|     5|\n",
      "|      QF491|    75|\n",
      "|     CA4516|   176|\n",
      "|      KE122| 29892|\n",
      "|      QF465|   180|\n",
      "|      3U606| 21480|\n",
      "|      CX367|  1706|\n",
      "|      QF389|     1|\n",
      "|      VN531|  1242|\n",
      "|     MU8438|   251|\n",
      "|     MU2692|  1053|\n",
      "|     MU6151|   513|\n",
      "|     CA4592|   656|\n",
      "|     CA1522|  1220|\n",
      "|      JQ503|     3|\n",
      "|     MU8450|   549|\n",
      "|     CX5803|   227|\n",
      "|     CA1584|   129|\n",
      "|      QF413|  6636|\n",
      "|      NZ104|  8700|\n",
      "|      QF473|  2379|\n",
      "|      QF489|    55|\n",
      "|     CZ3614|  2132|\n",
      "|     CZ3590|   686|\n",
      "|     CX5875|  5135|\n",
      "|      QF495|   626|\n",
      "|      GA715|  7810|\n",
      "|      CA177|    55|\n",
      "|       AC34|    62|\n",
      "|     CX5883|   697|\n",
      "|      QF303|  6127|\n",
      "|     HO1125|   105|\n",
      "|     MF8522|  1189|\n",
      "|     MU6781|    58|\n",
      "|     MF8502|   865|\n",
      "|      OZ368|   344|\n",
      "|      MU712|  6098|\n",
      "|     HU7816|  1962|\n",
      "|      VA808|  4637|\n",
      "|     CZ3524|   846|\n",
      "|      VA810|   132|\n",
      "|      CA430|  9850|\n",
      "|     MU2510|   156|\n",
      "|      NZ102|  1439|\n",
      "|     CA1502|   606|\n",
      "|      MH140| 17348|\n",
      "|      SQ222| 10609|\n",
      "|      GA713| 11438|\n",
      "|     CA1558|   731|\n",
      "|     CA1949|  1146|\n",
      "|      QF421|   135|\n",
      "|     MU5113|     3|\n",
      "+-----------+------+\n",
      "\n",
      "+--------------------+------+\n",
      "|               plane| count|\n",
      "+--------------------+------+\n",
      "|         Airbus A319|   166|\n",
      "|AIRBUS INDUSTRIE ...|215550|\n",
      "|NOTE: THIS IS TRA...|  5151|\n",
      "|BOEING 737-800 (W...|107928|\n",
      "|          Boeing 737|  1606|\n",
      "|    BOEING 777-300ER|323202|\n",
      "|          Boeing 757|   147|\n",
      "|         Airbus A321| 24167|\n",
      "|          Boeing 767|    18|\n",
      "|      Boeing 737-900|     1|\n",
      "|AIRBUS INDUSTRIE ...|  9388|\n",
      "|Airbus Industrie ...| 42211|\n",
      "|BOEING 777-300 PA...|  8516|\n",
      "|        BOEING 787-9| 89384|\n",
      "|      Boeing 767-300|     4|\n",
      "|         Embraer 190|   158|\n",
      "|772 - BOEING 777/200| 27564|\n",
      "|          Boeing 787| 42376|\n",
      "|AIRBUS INDUSTRIE ...|466299|\n",
      "|NOTE:  THIS IS BU...|   287|\n",
      "|         Airbus A320| 34000|\n",
      "|      Boeing 737-800| 19776|\n",
      "|BOEING 737-700 (W...|    54|\n",
      "|         Airbus A340|   457|\n",
      "|                    | 74199|\n",
      "|         Airbus A330|279193|\n",
      "|    BOEING 777-200LR| 15355|\n",
      "|          Boeing 747| 19401|\n",
      "|        Boeing 787-8|  5109|\n",
      "|      Boeing 747-400| 24085|\n",
      "+--------------------+------+\n",
      "\n",
      "+--------------+------+\n",
      "|arr_time_group| count|\n",
      "+--------------+------+\n",
      "|     afternoon|408728|\n",
      "|       morning|629869|\n",
      "|       evening|732401|\n",
      "| early morning| 64754|\n",
      "+--------------+------+\n",
      "\n",
      "+--------------+------+\n",
      "|dep_time_group| count|\n",
      "+--------------+------+\n",
      "|     afternoon|306784|\n",
      "|       morning|780710|\n",
      "|       evening|715389|\n",
      "| early morning| 32869|\n",
      "+--------------+------+\n",
      "\n",
      "+-----------+------+\n",
      "|dep_weekday| count|\n",
      "+-----------+------+\n",
      "|          3|259307|\n",
      "|          0|261083|\n",
      "|          5|261913|\n",
      "|          6|263861|\n",
      "|          1|263950|\n",
      "|          4|261794|\n",
      "|          2|263844|\n",
      "+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cols = ['from_city_name', 'to_city_name', 'trip', 'company', 'flight_code', 'plane', \n",
    "                 'arr_time_group', 'dep_time_group', 'dep_weekday']\n",
    "\n",
    "for c in cols:\n",
    "    flight7.groupBy(col(c)).count().show(50000)\n",
    "    \n",
    "    \n",
    "flight7 = flight7.na.replace('', 'unknown', 'plane')\n",
    "\n",
    "# http://stackoverflow.com/questions/40711229/pyspark-ml-error-urequirement-failed-cannot-have-an-empty-string-for-name\n",
    "# replacements = {\n",
    "#   'plane': 'unknown', 'another_col': 'another_replacement',\n",
    "#   'numeric_column_wont_be_replaced': 1.0\n",
    "# }\n",
    "\n",
    "# for k, v in replacements.items():\n",
    "#     # We can replace string only if target is string\n",
    "#     # In Python 2 str -> basestring\n",
    "#     if isinstance(v, str):\n",
    "#         df = df.na.replace(\"\", v, [k])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'price_will_drop :0'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'price :0'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'few_tickets_left :0'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'from_city_name :0'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'to_city_name :0'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'trip :0'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'stay_days :0'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'company :0'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'flight_code :0'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'plane :0'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'span_days :0'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'stop :0'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'power :0'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'video :0'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'wifi :0'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'check_bag_not_inc :0'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'dep_time_group :0'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'arr_time_group :0'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'lead_time :0'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'dep_weekday :0'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'dep_weeknum :0'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'days_to_last_holiday :0'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'days_to_next_holiday :0'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'duration_minutes :0'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'stop_minutes :0'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cols = ['price_will_drop', 'price', 'few_tickets_left', 'from_city_name', 'to_city_name', 'trip', 'stay_days',\n",
    "                          'company', 'flight_code', 'plane', 'span_days', 'stop',\n",
    "                         'power', 'video', 'wifi', 'check_bag_not_inc', \n",
    "                         'dep_time_group', 'arr_time_group',  \n",
    "                         'lead_time', 'dep_weekday', 'dep_weeknum', \n",
    "                         'days_to_last_holiday', 'days_to_next_holiday',                         \n",
    "                         'duration_minutes', 'stop_minutes']\n",
    "\n",
    "for c in cols:\n",
    "    display(c + \" :\" + str(flight7.where(col(c).isNull()).count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+----------------+--------------+------------+----+---------+--------------+-----------+--------------------+---------+----+-----+-----+-----+-----------------+--------------+--------------+---------+-----------+-----------+--------------------+--------------------+----------------+------------+\n",
      "|price_will_drop_num|  price|few_tickets_left|from_city_name|to_city_name|trip|stay_days|       company|flight_code|               plane|span_days|stop|power|video| wifi|check_bag_not_inc|dep_time_group|arr_time_group|lead_time|dep_weekday|dep_weeknum|days_to_last_holiday|days_to_next_holiday|duration_minutes|stop_minutes|\n",
      "+-------------------+-------+----------------+--------------+------------+----+---------+--------------+-----------+--------------------+---------+----+-----+-----+-----+-----------------+--------------+--------------+---------+-----------+-----------+--------------------+--------------------+----------------+------------+\n",
      "|                  1|1671.42|            true|        sydney|    shanghai|   1|        0|Qantas Airways|      QF129|AIRBUS INDUSTRIE ...|        0|   0|false| true|false|            false|       morning|       evening|        3|          4|         14|                  -3|                  24|             655|           0|\n",
      "|                  0| 949.42|            true|        sydney|    shanghai|   1|        0|Qantas Airways|      QF129|AIRBUS INDUSTRIE ...|        0|   0|false| true|false|            false|       morning|       evening|        1|          4|         14|                  -3|                  24|             655|           0|\n",
      "+-------------------+-------+----------------+--------------+------------+----+---------+--------------+-----------+--------------------+---------+----+-----+-----+-----+-----------------+--------------+--------------+---------+-----------+-----------+--------------------+--------------------+----------------+------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1835752"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flight7.show(2)\n",
    "flight7.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: int, price: double, few_tickets_left: boolean, from_city_name: string, to_city_name: string, trip: string, stay_days: int, company: string, flight_code: string, plane: string, span_days: int, stop: int, power: boolean, video: boolean, wifi: boolean, check_bag_not_inc: boolean, dep_time_group: string, arr_time_group: string, lead_time: int, dep_weekday: string, dep_weeknum: int, days_to_last_holiday: int, days_to_next_holiday: int, duration_minutes: int, stop_minutes: int, from_city_name_tmp: double, from_city_name_catVec: vector, to_city_name_tmp: double, to_city_name_catVec: vector, trip_tmp: double, trip_catVec: vector, company_tmp: double, company_catVec: vector, flight_code_tmp: double, flight_code_catVec: vector, plane_tmp: double, plane_catVec: vector, arr_time_group_tmp: double, arr_time_group_catVec: vector, dep_time_group_tmp: double, dep_time_group_catVec: vector, dep_weekday_tmp: double, dep_weekday_catVec: vector, features: vector]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# flight7 = flight7.na.drop()\n",
    "output = pipeline.fit(flight7).transform(flight7)\n",
    "output = output.withColumnRenamed('price_will_drop_num', 'label')\n",
    "output.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling o648.showString",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-107cdbd63b26>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[1;34m(self, n, truncate)\u001b[0m\n\u001b[0;32m    316\u001b[0m         \"\"\"\n\u001b[0;32m    317\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 318\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    319\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu/anaconda3/lib/python3.5/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1133\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1135\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu/anaconda3/lib/python3.5/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    325\u001b[0m             raise Py4JError(\n\u001b[0;32m    326\u001b[0m                 \u001b[1;34m\"An error occurred while calling {0}{1}{2}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 327\u001b[1;33m                 format(target_id, \".\", name))\n\u001b[0m\u001b[0;32m    328\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    329\u001b[0m         \u001b[0mtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPy4JError\u001b[0m: An error occurred while calling o648.showString"
     ]
    }
   ],
   "source": [
    "output.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# flight4.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# flight4.select('stay_days',\n",
    "#  'power', \n",
    "#  'span_days',\n",
    "#  'stop',\n",
    "#  'video',\n",
    "#  'wifi',\n",
    "#  'duration_minutes',\n",
    "#  'dep_weeknum',\n",
    "#  'lead_time',\n",
    "#  'trip_index',\n",
    "#  'arr_time_group_index',\n",
    "#  'to_city_name_index',\n",
    "#  'ticket_left_index',\n",
    "#  'company_index',\n",
    "#  'from_city_name_index',\n",
    "#  'airline_code_index',\n",
    "#  'plane_index',\n",
    "#  'check_bag_not_inc_index',\n",
    "#  'dep_weekday_index',\n",
    "#  'dep_time_group_index').dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from pyspark.ml.linalg import Vectors\n",
    "# from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# # from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# # ignore = ['id', 'label', 'binomial_label']\n",
    "# # assembler = VectorAssembler(\n",
    "# #     inputCols=[x for x in df.columns if x not in ignore],\n",
    "# #     outputCol='features')\n",
    "\n",
    "# # assembler.transform(df)\n",
    "# # arr_time_group\n",
    "\n",
    "# assembler = VectorAssembler(    \n",
    "#     inputCols=[\n",
    "#  'stay_days',\n",
    "#  'power', \n",
    "#  'span_days',\n",
    "#  'stop',\n",
    "#  'video',\n",
    "#  'wifi',\n",
    "#  'duration_minutes', \n",
    "#  'dep_weeknum',\n",
    "#  'lead_time',\n",
    "#  'trip_index',\n",
    "#  'arr_time_group_index',\n",
    "#  'to_city_name_index',\n",
    "#  'ticket_left_index',\n",
    "#  'company_index',\n",
    "#  'from_city_name_index',\n",
    "#  'airline_code_index',\n",
    "#  'plane_index',\n",
    "#  'check_bag_not_inc_index',\n",
    "#  'dep_weekday_index',\n",
    "#  'dep_time_group_index'],\n",
    "#     outputCol=\"features\")\n",
    "\n",
    "# output = assembler.transform(flight4)\n",
    "# # print(\"Assembled columns 'hour', 'mobile', 'userFeatures' to vector column 'features'\")\n",
    "# # output.select(\"features\", \"clicked\").show(truncate=False)\n",
    "# output = output.withColumnRenamed('price', 'label')\n",
    "# output.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JNetworkError",
     "evalue": "An error occurred while trying to connect to the Java server (127.0.0.1:40455)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/ubuntu/anaconda3/lib/python3.5/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m_get_connection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    826\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 827\u001b[1;33m             \u001b[0mconnection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeque\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    828\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: pop from an empty deque",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m/home/ubuntu/anaconda3/lib/python3.5/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36mstart\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    962\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 963\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddress\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    964\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_connected\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mPy4JNetworkError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-11129ef3146a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflight6\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflight7\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcount\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    378\u001b[0m         \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    379\u001b[0m         \"\"\"\n\u001b[1;32m--> 380\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    381\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    382\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu/anaconda3/lib/python3.5/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1129\u001b[0m             \u001b[0mproto\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1130\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1131\u001b[1;33m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m   1133\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[1;32m/home/ubuntu/anaconda3/lib/python3.5/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[1;34m(self, command, retry, binary)\u001b[0m\n\u001b[0;32m    879\u001b[0m          \u001b[1;32mif\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mis\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    880\u001b[0m         \"\"\"\n\u001b[1;32m--> 881\u001b[1;33m         \u001b[0mconnection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    882\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    883\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu/anaconda3/lib/python3.5/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m_get_connection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    827\u001b[0m             \u001b[0mconnection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeque\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    828\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 829\u001b[1;33m             \u001b[0mconnection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_connection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    830\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    831\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu/anaconda3/lib/python3.5/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m_create_connection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    833\u001b[0m         connection = GatewayConnection(\n\u001b[0;32m    834\u001b[0m             self.gateway_parameters, self.gateway_property)\n\u001b[1;32m--> 835\u001b[1;33m         \u001b[0mconnection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    836\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    837\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu/anaconda3/lib/python3.5/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36mstart\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    968\u001b[0m                 \u001b[1;34m\"server ({0}:{1})\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddress\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    969\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 970\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mPy4JNetworkError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    971\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    972\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPy4JNetworkError\u001b[0m: An error occurred while trying to connect to the Java server (127.0.0.1:40455)"
     ]
    }
   ],
   "source": [
    "display(output.count(), flight6.count(), flight7.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# output.coalesce(2).write.parquet(\"C:\\\\s3\\\\20170503_jsonl\\\\output.parquet\")\n",
    "output.coalesce(2).write.parquet(\"/home/ubuntu/parquet/output.parquet/output.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output = spark.read.parquet(\"/home/ubuntu/parquet/output.parquet/output.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18379"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output = output.sample(False, 0.1, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: integer (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- few_tickets_left: boolean (nullable = true)\n",
      " |-- from_city_name: string (nullable = true)\n",
      " |-- to_city_name: string (nullable = true)\n",
      " |-- trip: string (nullable = true)\n",
      " |-- stay_days: integer (nullable = true)\n",
      " |-- company: string (nullable = true)\n",
      " |-- flight_code: string (nullable = true)\n",
      " |-- plane: string (nullable = true)\n",
      " |-- span_days: integer (nullable = true)\n",
      " |-- stop: integer (nullable = true)\n",
      " |-- power: boolean (nullable = true)\n",
      " |-- video: boolean (nullable = true)\n",
      " |-- wifi: boolean (nullable = true)\n",
      " |-- check_bag_not_inc: boolean (nullable = true)\n",
      " |-- dep_time_group: string (nullable = true)\n",
      " |-- arr_time_group: string (nullable = true)\n",
      " |-- lead_time: integer (nullable = true)\n",
      " |-- dep_weekday: string (nullable = true)\n",
      " |-- dep_weeknum: integer (nullable = true)\n",
      " |-- days_to_last_holiday: integer (nullable = true)\n",
      " |-- days_to_next_holiday: integer (nullable = true)\n",
      " |-- duration_minutes: integer (nullable = true)\n",
      " |-- stop_minutes: integer (nullable = true)\n",
      " |-- from_city_name_tmp: double (nullable = true)\n",
      " |-- from_city_name_catVec: vector (nullable = true)\n",
      " |-- to_city_name_tmp: double (nullable = true)\n",
      " |-- to_city_name_catVec: vector (nullable = true)\n",
      " |-- trip_tmp: double (nullable = true)\n",
      " |-- trip_catVec: vector (nullable = true)\n",
      " |-- company_tmp: double (nullable = true)\n",
      " |-- company_catVec: vector (nullable = true)\n",
      " |-- flight_code_tmp: double (nullable = true)\n",
      " |-- flight_code_catVec: vector (nullable = true)\n",
      " |-- plane_tmp: double (nullable = true)\n",
      " |-- plane_catVec: vector (nullable = true)\n",
      " |-- arr_time_group_tmp: double (nullable = true)\n",
      " |-- arr_time_group_catVec: vector (nullable = true)\n",
      " |-- dep_time_group_tmp: double (nullable = true)\n",
      " |-- dep_time_group_catVec: vector (nullable = true)\n",
      " |-- dep_weekday_tmp: double (nullable = true)\n",
      " |-- dep_weekday_catVec: vector (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n",
      "+-----+-------+----------------+--------------+------------+----+---------+--------------------+-----------+--------------------+---------+----+-----+-----+-----+-----------------+--------------+--------------+---------+-----------+-----------+--------------------+--------------------+----------------+------------+------------------+---------------------+----------------+-------------------+--------+-------------+-----------+--------------+---------------+------------------+---------+--------------+------------------+---------------------+------------------+---------------------+---------------+------------------+--------------------+\n",
      "|label|  price|few_tickets_left|from_city_name|to_city_name|trip|stay_days|             company|flight_code|               plane|span_days|stop|power|video| wifi|check_bag_not_inc|dep_time_group|arr_time_group|lead_time|dep_weekday|dep_weeknum|days_to_last_holiday|days_to_next_holiday|duration_minutes|stop_minutes|from_city_name_tmp|from_city_name_catVec|to_city_name_tmp|to_city_name_catVec|trip_tmp|  trip_catVec|company_tmp|company_catVec|flight_code_tmp|flight_code_catVec|plane_tmp|  plane_catVec|arr_time_group_tmp|arr_time_group_catVec|dep_time_group_tmp|dep_time_group_catVec|dep_weekday_tmp|dep_weekday_catVec|            features|\n",
      "+-----+-------+----------------+--------------+------------+----+---------+--------------------+-----------+--------------------+---------+----+-----+-----+-----+-----------------+--------------+--------------+---------+-----------+-----------+--------------------+--------------------+----------------+------------+------------------+---------------------+----------------+-------------------+--------+-------------+-----------+--------------+---------------+------------------+---------+--------------+------------------+---------------------+------------------+---------------------+---------------+------------------+--------------------+\n",
      "|    0| 689.52|            true|        sydney|    shanghai|   1|        0|China Southern Ai...|      CZ302|         Airbus A330|        0|   1|false|false|false|            false|       evening|     afternoon|       47|          5|         23|                 -11|                 113|            1060|           0|               0.0|        (1,[0],[1.0])|             0.0|      (1,[0],[1.0])|     1.0|(2,[1],[1.0])|        1.0|(28,[1],[1.0])|            2.0|   (323,[2],[1.0])|      2.0|(30,[2],[1.0])|               2.0|        (4,[2],[1.0])|               1.0|        (4,[1],[1.0])|            3.0|     (7,[3],[1.0])|(415,[0,1,4,9,10,...|\n",
      "|    1|1025.42|            true|        sydney|    shanghai|   1|        0|   Malaysia Airlines|      MH140|AIRBUS INDUSTRIE ...|        0|   1|false|false|false|            false|       evening|     afternoon|       64|          3|         26|                 -30|                  94|            1085|           0|               0.0|        (1,[0],[1.0])|             0.0|      (1,[0],[1.0])|     1.0|(2,[1],[1.0])|        9.0|(28,[9],[1.0])|           24.0|  (323,[24],[1.0])|      0.0|(30,[0],[1.0])|               2.0|        (4,[2],[1.0])|               1.0|        (4,[1],[1.0])|            6.0|     (7,[6],[1.0])|(415,[0,1,4,9,10,...|\n",
      "+-----+-------+----------------+--------------+------------+----+---------+--------------------+-----------+--------------------+---------+----+-----+-----+-----+-----------------+--------------+--------------+---------+-----------+-----------+--------------------+--------------------+----------------+------------+------------------+---------------------+----------------+-------------------+--------+-------------+-----------+--------------+---------------+------------------+---------+--------------+------------------+---------------------+------------------+---------------------+---------------+------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output.count()\n",
    "output.printSchema()\n",
    "output.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# interlude to model in h2o (not sparkling water)  \n",
    "When converting the whole dataset to pandas dataframe, there was not enough RAM, JAVA was forced to shutdown!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: integer (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- few_tickets_left: boolean (nullable = true)\n",
      " |-- from_city_name: string (nullable = true)\n",
      " |-- to_city_name: string (nullable = true)\n",
      " |-- trip: string (nullable = true)\n",
      " |-- stay_days: integer (nullable = true)\n",
      " |-- company: string (nullable = true)\n",
      " |-- flight_code: string (nullable = true)\n",
      " |-- plane: string (nullable = true)\n",
      " |-- span_days: integer (nullable = true)\n",
      " |-- stop: integer (nullable = true)\n",
      " |-- power: boolean (nullable = true)\n",
      " |-- video: boolean (nullable = true)\n",
      " |-- wifi: boolean (nullable = true)\n",
      " |-- check_bag_not_inc: boolean (nullable = true)\n",
      " |-- dep_time_group: string (nullable = true)\n",
      " |-- arr_time_group: string (nullable = true)\n",
      " |-- lead_time: integer (nullable = true)\n",
      " |-- dep_weekday: string (nullable = true)\n",
      " |-- dep_weeknum: integer (nullable = true)\n",
      " |-- days_to_last_holiday: integer (nullable = true)\n",
      " |-- days_to_next_holiday: integer (nullable = true)\n",
      " |-- duration_minutes: integer (nullable = true)\n",
      " |-- stop_minutes: integer (nullable = true)\n",
      " |-- from_city_name_tmp: double (nullable = true)\n",
      " |-- from_city_name_catVec: vector (nullable = true)\n",
      " |-- to_city_name_tmp: double (nullable = true)\n",
      " |-- to_city_name_catVec: vector (nullable = true)\n",
      " |-- trip_tmp: double (nullable = true)\n",
      " |-- trip_catVec: vector (nullable = true)\n",
      " |-- company_tmp: double (nullable = true)\n",
      " |-- company_catVec: vector (nullable = true)\n",
      " |-- flight_code_tmp: double (nullable = true)\n",
      " |-- flight_code_catVec: vector (nullable = true)\n",
      " |-- plane_tmp: double (nullable = true)\n",
      " |-- plane_catVec: vector (nullable = true)\n",
      " |-- arr_time_group_tmp: double (nullable = true)\n",
      " |-- arr_time_group_catVec: vector (nullable = true)\n",
      " |-- dep_time_group_tmp: double (nullable = true)\n",
      " |-- dep_time_group_catVec: vector (nullable = true)\n",
      " |-- dep_weekday_tmp: double (nullable = true)\n",
      " |-- dep_weekday_catVec: vector (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output.printSchema()\n",
    "output2 = output.select('label', 'features')\n",
    "# output2 = output.select('label', 'features').rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18379"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>price</th>\n",
       "      <th>few_tickets_left</th>\n",
       "      <th>from_city_name</th>\n",
       "      <th>to_city_name</th>\n",
       "      <th>trip</th>\n",
       "      <th>stay_days</th>\n",
       "      <th>company</th>\n",
       "      <th>flight_code</th>\n",
       "      <th>plane</th>\n",
       "      <th>...</th>\n",
       "      <th>check_bag_not_inc</th>\n",
       "      <th>dep_time_group</th>\n",
       "      <th>arr_time_group</th>\n",
       "      <th>lead_time</th>\n",
       "      <th>dep_weekday</th>\n",
       "      <th>dep_weeknum</th>\n",
       "      <th>days_to_last_holiday</th>\n",
       "      <th>days_to_next_holiday</th>\n",
       "      <th>duration_minutes</th>\n",
       "      <th>stop_minutes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>689.52</td>\n",
       "      <td>True</td>\n",
       "      <td>sydney</td>\n",
       "      <td>shanghai</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>China Southern Airlines</td>\n",
       "      <td>CZ302</td>\n",
       "      <td>Airbus A330</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>evening</td>\n",
       "      <td>afternoon</td>\n",
       "      <td>47</td>\n",
       "      <td>5</td>\n",
       "      <td>23</td>\n",
       "      <td>-11</td>\n",
       "      <td>113</td>\n",
       "      <td>1060</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1025.42</td>\n",
       "      <td>True</td>\n",
       "      <td>sydney</td>\n",
       "      <td>shanghai</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Malaysia Airlines</td>\n",
       "      <td>MH140</td>\n",
       "      <td>AIRBUS INDUSTRIE A330-300</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>evening</td>\n",
       "      <td>afternoon</td>\n",
       "      <td>64</td>\n",
       "      <td>3</td>\n",
       "      <td>26</td>\n",
       "      <td>-30</td>\n",
       "      <td>94</td>\n",
       "      <td>1085</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1283.27</td>\n",
       "      <td>True</td>\n",
       "      <td>sydney</td>\n",
       "      <td>shanghai</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Qantas Airways</td>\n",
       "      <td>QF508</td>\n",
       "      <td>BOEING 737-800 (WINGLETS) PASSENGER</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>morning</td>\n",
       "      <td>evening</td>\n",
       "      <td>71</td>\n",
       "      <td>3</td>\n",
       "      <td>27</td>\n",
       "      <td>-37</td>\n",
       "      <td>87</td>\n",
       "      <td>860</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>723.82</td>\n",
       "      <td>True</td>\n",
       "      <td>sydney</td>\n",
       "      <td>shanghai</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Cathay Pacific</td>\n",
       "      <td>CX138</td>\n",
       "      <td>BOEING 777-300ER</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>evening</td>\n",
       "      <td>evening</td>\n",
       "      <td>91</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>-43</td>\n",
       "      <td>81</td>\n",
       "      <td>1540</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>663.82</td>\n",
       "      <td>True</td>\n",
       "      <td>sydney</td>\n",
       "      <td>shanghai</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>China Eastern Airlines</td>\n",
       "      <td>MU8440</td>\n",
       "      <td>AIRBUS INDUSTRIE A330-300</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>morning</td>\n",
       "      <td>evening</td>\n",
       "      <td>84</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>-48</td>\n",
       "      <td>76</td>\n",
       "      <td>655</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label    price few_tickets_left from_city_name to_city_name trip  \\\n",
       "0      0   689.52             True         sydney     shanghai    1   \n",
       "1      1  1025.42             True         sydney     shanghai    1   \n",
       "2      1  1283.27             True         sydney     shanghai    1   \n",
       "3      0   723.82             True         sydney     shanghai    1   \n",
       "4      0   663.82             True         sydney     shanghai    1   \n",
       "\n",
       "   stay_days                  company flight_code  \\\n",
       "0          0  China Southern Airlines       CZ302   \n",
       "1          0        Malaysia Airlines       MH140   \n",
       "2          0           Qantas Airways       QF508   \n",
       "3          0           Cathay Pacific       CX138   \n",
       "4          0   China Eastern Airlines      MU8440   \n",
       "\n",
       "                                 plane     ...       check_bag_not_inc  \\\n",
       "0                          Airbus A330     ...                   False   \n",
       "1            AIRBUS INDUSTRIE A330-300     ...                   False   \n",
       "2  BOEING 737-800 (WINGLETS) PASSENGER     ...                   False   \n",
       "3                     BOEING 777-300ER     ...                   False   \n",
       "4            AIRBUS INDUSTRIE A330-300     ...                   False   \n",
       "\n",
       "   dep_time_group arr_time_group lead_time dep_weekday dep_weeknum  \\\n",
       "0         evening      afternoon        47           5          23   \n",
       "1         evening      afternoon        64           3          26   \n",
       "2         morning        evening        71           3          27   \n",
       "3         evening        evening        91           2          28   \n",
       "4         morning        evening        84           0          29   \n",
       "\n",
       "  days_to_last_holiday days_to_next_holiday  duration_minutes stop_minutes  \n",
       "0                  -11                  113              1060            0  \n",
       "1                  -30                   94              1085            0  \n",
       "2                  -37                   87               860            0  \n",
       "3                  -43                   81              1540            0  \n",
       "4                  -48                   76               655            0  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output2.count()\n",
    "flight_pd = output.select('label', 'price', 'few_tickets_left', \n",
    "                          'from_city_name', 'to_city_name', 'trip', 'stay_days',\n",
    "                          'company', 'flight_code', 'plane', 'span_days', 'stop',\n",
    "                         'power', 'video', 'wifi', 'check_bag_not_inc', \n",
    "                         'dep_time_group', 'arr_time_group',  \n",
    "                         'lead_time', 'dep_weekday', 'dep_weeknum', \n",
    "                         'days_to_last_holiday', 'days_to_next_holiday',                         \n",
    "                         'duration_minutes', 'stop_minutes').toPandas()\n",
    "flight_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# flight_pd.to_csv('C:\\\\s3\\\\20170503_jsonl\\\\flight_pd.csv', sep='\\t')\n",
    "flight_pd.to_csv('/home/ubuntu/parquet/flight_pd.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether there is an H2O instance running at http://localhost:54321..... not found.\n",
      "Attempting to start a local H2O server...\n",
      "  Java Version: openjdk version \"1.8.0_131\"; OpenJDK Runtime Environment (build 1.8.0_131-8u131-b11-0ubuntu1.16.04.2-b11); OpenJDK 64-Bit Server VM (build 25.131-b11, mixed mode)\n",
      "  Starting server from /home/ubuntu/anaconda3/lib/python3.5/site-packages/h2o/backend/bin/h2o.jar\n",
      "  Ice root: /tmp/tmp2iu8aa2f\n",
      "  JVM stdout: /tmp/tmp2iu8aa2f/h2o_ubuntu_started_from_python.out\n",
      "  JVM stderr: /tmp/tmp2iu8aa2f/h2o_ubuntu_started_from_python.err\n",
      "  Server is running at http://127.0.0.1:54321\n",
      "Connecting to H2O server at http://127.0.0.1:54321... successful.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O cluster uptime:</td>\n",
       "<td>05 secs</td></tr>\n",
       "<tr><td>H2O cluster version:</td>\n",
       "<td>3.10.4.6</td></tr>\n",
       "<tr><td>H2O cluster version age:</td>\n",
       "<td>16 days </td></tr>\n",
       "<tr><td>H2O cluster name:</td>\n",
       "<td>H2O_from_python_ubuntu_x1de28</td></tr>\n",
       "<tr><td>H2O cluster total nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O cluster free memory:</td>\n",
       "<td>232.5 Mb</td></tr>\n",
       "<tr><td>H2O cluster total cores:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O cluster allowed cores:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O cluster status:</td>\n",
       "<td>accepting new members, healthy</td></tr>\n",
       "<tr><td>H2O connection url:</td>\n",
       "<td>http://127.0.0.1:54321</td></tr>\n",
       "<tr><td>H2O connection proxy:</td>\n",
       "<td>None</td></tr>\n",
       "<tr><td>H2O internal security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>Python version:</td>\n",
       "<td>3.5.2 final</td></tr></table></div>"
      ],
      "text/plain": [
       "--------------------------  ------------------------------\n",
       "H2O cluster uptime:         05 secs\n",
       "H2O cluster version:        3.10.4.6\n",
       "H2O cluster version age:    16 days\n",
       "H2O cluster name:           H2O_from_python_ubuntu_x1de28\n",
       "H2O cluster total nodes:    1\n",
       "H2O cluster free memory:    232.5 Mb\n",
       "H2O cluster total cores:    1\n",
       "H2O cluster allowed cores:  1\n",
       "H2O cluster status:         accepting new members, healthy\n",
       "H2O connection url:         http://127.0.0.1:54321\n",
       "H2O connection proxy:\n",
       "H2O internal security:      False\n",
       "Python version:             3.5.2 final\n",
       "--------------------------  ------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "h2o.init()\n",
    "# from h2o.estimators.gbm import H2OGradientBoostingEstimator\n",
    "\n",
    "# hf = h2o.H2OFrame(flight_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "flight_pd=pd.read_csv(\"/home/ubuntu/parquet/flight_pd.csv\", delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |âââââââââââââââââââââââââââââââââââââââââââââââââââââââââ| 100%\n"
     ]
    }
   ],
   "source": [
    "# flight_pd.head()\n",
    "# help(pd.read_csv)\n",
    "flight_hex = h2o.H2OFrame(flight_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, valid, test = flight_hex.split_frame([0.6, 0.2], seed=1234)\n",
    "\n",
    "flight_X = flight_hex.col_names[2:]\n",
    "flight_y = flight_hex.col_names[1]    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drf Model Build progress: |âââââââââââââââââââââââââââââââââââââââââââââââ| 100%\n",
      "/home/ubuntu/.jupyter/flight_project/C:\\s3\\20170503_jsonl/rf_v1\n"
     ]
    }
   ],
   "source": [
    "from h2o.estimators.gbm import H2OGradientBoostingEstimator\n",
    "from h2o.estimators.random_forest import H2ORandomForestEstimator\n",
    "\n",
    "rf_v1 = H2ORandomForestEstimator(\n",
    "    model_id=\"rf_v1\",\n",
    "    ntrees=200,\n",
    "    stopping_rounds=2,\n",
    "    score_each_iteration=True,\n",
    "    seed=1000000)\n",
    "\n",
    "rf_v1.train(flight_X, flight_y, training_frame=train, validation_frame=valid)\n",
    "rf_v1\n",
    "rf_v1.score_history()\n",
    "model_path = h2o.save_model(model=rf_v1, path=\"C:\\\\s3\\\\20170503_jsonl\", force=True)\n",
    "print(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Details\n",
      "=============\n",
      "H2ORandomForestEstimator :  Distributed Random Forest\n",
      "Model Key:  rf_v1\n",
      "\n",
      "\n",
      "ModelMetricsRegression: drf\n",
      "** Reported on train data. **\n",
      "\n",
      "MSE: 0.16125699307615135\n",
      "RMSE: 0.4015681674089112\n",
      "MAE: 0.29576985113399795\n",
      "RMSLE: 0.2812286384019287\n",
      "Mean Residual Deviance: 0.16125699307615135\n",
      "\n",
      "ModelMetricsRegression: drf\n",
      "** Reported on validation data. **\n",
      "\n",
      "MSE: 0.14772317434168744\n",
      "RMSE: 0.3843477258182848\n",
      "MAE: 0.2896960866913029\n",
      "RMSLE: 0.26993483527383644\n",
      "Mean Residual Deviance: 0.14772317434168744\n",
      "Scoring History: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>timestamp</b></td>\n",
       "<td><b>duration</b></td>\n",
       "<td><b>number_of_trees</b></td>\n",
       "<td><b>training_rmse</b></td>\n",
       "<td><b>training_mae</b></td>\n",
       "<td><b>training_deviance</b></td>\n",
       "<td><b>validation_rmse</b></td>\n",
       "<td><b>validation_mae</b></td>\n",
       "<td><b>validation_deviance</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>2017-05-13 12:52:38</td>\n",
       "<td> 0.052 sec</td>\n",
       "<td>0.0</td>\n",
       "<td>nan</td>\n",
       "<td>nan</td>\n",
       "<td>nan</td>\n",
       "<td>nan</td>\n",
       "<td>nan</td>\n",
       "<td>nan</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2017-05-13 12:52:39</td>\n",
       "<td> 0.773 sec</td>\n",
       "<td>1.0</td>\n",
       "<td>0.5375521</td>\n",
       "<td>0.2919111</td>\n",
       "<td>0.2889622</td>\n",
       "<td>0.5433976</td>\n",
       "<td>0.2976626</td>\n",
       "<td>0.2952809</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2017-05-13 12:52:39</td>\n",
       "<td> 1.137 sec</td>\n",
       "<td>2.0</td>\n",
       "<td>0.5221194</td>\n",
       "<td>0.2976923</td>\n",
       "<td>0.2726086</td>\n",
       "<td>0.4712592</td>\n",
       "<td>0.2990894</td>\n",
       "<td>0.2220852</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2017-05-13 12:52:39</td>\n",
       "<td> 1.453 sec</td>\n",
       "<td>3.0</td>\n",
       "<td>0.5050030</td>\n",
       "<td>0.2936024</td>\n",
       "<td>0.2550280</td>\n",
       "<td>0.4416678</td>\n",
       "<td>0.2929564</td>\n",
       "<td>0.1950704</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2017-05-13 12:52:39</td>\n",
       "<td> 1.713 sec</td>\n",
       "<td>4.0</td>\n",
       "<td>0.4906770</td>\n",
       "<td>0.2913865</td>\n",
       "<td>0.2407639</td>\n",
       "<td>0.4279878</td>\n",
       "<td>0.2926597</td>\n",
       "<td>0.1831736</td></tr>\n",
       "<tr><td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2017-05-13 12:52:44</td>\n",
       "<td> 6.563 sec</td>\n",
       "<td>27.0</td>\n",
       "<td>0.4036201</td>\n",
       "<td>0.2946506</td>\n",
       "<td>0.1629092</td>\n",
       "<td>0.3850148</td>\n",
       "<td>0.2889101</td>\n",
       "<td>0.1482364</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2017-05-13 12:52:45</td>\n",
       "<td> 6.746 sec</td>\n",
       "<td>28.0</td>\n",
       "<td>0.4029845</td>\n",
       "<td>0.2951376</td>\n",
       "<td>0.1623965</td>\n",
       "<td>0.3846213</td>\n",
       "<td>0.2890857</td>\n",
       "<td>0.1479335</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2017-05-13 12:52:45</td>\n",
       "<td> 6.912 sec</td>\n",
       "<td>29.0</td>\n",
       "<td>0.4023413</td>\n",
       "<td>0.2951724</td>\n",
       "<td>0.1618785</td>\n",
       "<td>0.3843001</td>\n",
       "<td>0.2891586</td>\n",
       "<td>0.1476865</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2017-05-13 12:52:45</td>\n",
       "<td> 7.214 sec</td>\n",
       "<td>30.0</td>\n",
       "<td>0.4023116</td>\n",
       "<td>0.2960082</td>\n",
       "<td>0.1618546</td>\n",
       "<td>0.3844270</td>\n",
       "<td>0.2898464</td>\n",
       "<td>0.1477841</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2017-05-13 12:52:45</td>\n",
       "<td> 7.521 sec</td>\n",
       "<td>31.0</td>\n",
       "<td>0.4015682</td>\n",
       "<td>0.2957699</td>\n",
       "<td>0.1612570</td>\n",
       "<td>0.3843477</td>\n",
       "<td>0.2896961</td>\n",
       "<td>0.1477232</td></tr></table></div>"
      ],
      "text/plain": [
       "     timestamp            duration    number_of_trees    training_rmse        training_mae         training_deviance    validation_rmse      validation_mae       validation_deviance\n",
       "---  -------------------  ----------  -----------------  -------------------  -------------------  -------------------  -------------------  -------------------  ---------------------\n",
       "     2017-05-13 12:52:38  0.052 sec   0.0                nan                  nan                  nan                  nan                  nan                  nan\n",
       "     2017-05-13 12:52:39  0.773 sec   1.0                0.5375520890856962   0.2919110682984847   0.28896224848039626  0.5433975849996553   0.29766256910283867  0.29528093538345757\n",
       "     2017-05-13 12:52:39  1.137 sec   2.0                0.5221193567336646   0.29769227043338264  0.27260862267597574  0.47125920853624204  0.2990893967157407   0.22208524163020527\n",
       "     2017-05-13 12:52:39  1.453 sec   3.0                0.505002970539796    0.29360235806128787  0.25502800025401806  0.4416677627010321   0.2929564319082589   0.1950704126093352\n",
       "     2017-05-13 12:52:39  1.713 sec   4.0                0.4906770015861094   0.2913865270721407   0.2407639198855348   0.4279878104120086   0.2926596799762274   0.18317356586126543\n",
       "---  ---                  ---         ---                ---                  ---                  ---                  ---                  ---                  ---\n",
       "     2017-05-13 12:52:44  6.563 sec   27.0               0.40362007440923525  0.29465060071844923  0.16290916446611658  0.3850147865336862   0.28891006232890354  0.14823638584957996\n",
       "     2017-05-13 12:52:45  6.746 sec   28.0               0.402984498333078    0.2951376407251492   0.16239650589676255  0.3846212910569458   0.2890856678756642   0.14793353753431182\n",
       "     2017-05-13 12:52:45  6.912 sec   29.0               0.4023412633786928   0.2951724486234276   0.16187849221716266  0.3843000528027514   0.2891585751420258   0.14768653058419753\n",
       "     2017-05-13 12:52:45  7.214 sec   30.0               0.4023115810357749   0.2960082168404483   0.16185460823550488  0.3844270063492678   0.2898464050616378   0.14778412321065998\n",
       "     2017-05-13 12:52:45  7.521 sec   31.0               0.4015681674089112   0.29576985113399795  0.16125699307615135  0.3843477258182848   0.2896960866913029   0.14772317434168744"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "See the whole table with table.as_data_frame()\n",
      "Variable Importances: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>variable</b></td>\n",
       "<td><b>relative_importance</b></td>\n",
       "<td><b>scaled_importance</b></td>\n",
       "<td><b>percentage</b></td></tr>\n",
       "<tr><td>price</td>\n",
       "<td>8528.5175781</td>\n",
       "<td>1.0</td>\n",
       "<td>0.1670183</td></tr>\n",
       "<tr><td>flight_code</td>\n",
       "<td>8061.0830078</td>\n",
       "<td>0.9451916</td>\n",
       "<td>0.1578643</td></tr>\n",
       "<tr><td>lead_time</td>\n",
       "<td>6366.9272461</td>\n",
       "<td>0.7465456</td>\n",
       "<td>0.1246868</td></tr>\n",
       "<tr><td>days_to_last_holiday</td>\n",
       "<td>3866.3369141</td>\n",
       "<td>0.4533422</td>\n",
       "<td>0.0757164</td></tr>\n",
       "<tr><td>few_tickets_left</td>\n",
       "<td>3569.9750977</td>\n",
       "<td>0.4185927</td>\n",
       "<td>0.0699126</td></tr>\n",
       "<tr><td>dep_weeknum</td>\n",
       "<td>3275.1225586</td>\n",
       "<td>0.3840201</td>\n",
       "<td>0.0641384</td></tr>\n",
       "<tr><td>stay_days</td>\n",
       "<td>3227.5683594</td>\n",
       "<td>0.3784442</td>\n",
       "<td>0.0632071</td></tr>\n",
       "<tr><td>dep_weekday</td>\n",
       "<td>2826.2167969</td>\n",
       "<td>0.3313843</td>\n",
       "<td>0.0553472</td></tr>\n",
       "<tr><td>duration_minutes</td>\n",
       "<td>2584.9619141</td>\n",
       "<td>0.3030963</td>\n",
       "<td>0.0506226</td></tr>\n",
       "<tr><td>days_to_next_holiday</td>\n",
       "<td>2132.4052734</td>\n",
       "<td>0.2500323</td>\n",
       "<td>0.0417600</td></tr>\n",
       "<tr><td>company</td>\n",
       "<td>2103.7602539</td>\n",
       "<td>0.2466736</td>\n",
       "<td>0.0411990</td></tr>\n",
       "<tr><td>plane</td>\n",
       "<td>1542.7763672</td>\n",
       "<td>0.1808962</td>\n",
       "<td>0.0302130</td></tr>\n",
       "<tr><td>trip</td>\n",
       "<td>1051.5755615</td>\n",
       "<td>0.1233011</td>\n",
       "<td>0.0205935</td></tr>\n",
       "<tr><td>arr_time_group</td>\n",
       "<td>930.6566772</td>\n",
       "<td>0.1091229</td>\n",
       "<td>0.0182255</td></tr>\n",
       "<tr><td>dep_time_group</td>\n",
       "<td>459.9393311</td>\n",
       "<td>0.0539296</td>\n",
       "<td>0.0090072</td></tr>\n",
       "<tr><td>video</td>\n",
       "<td>300.2182312</td>\n",
       "<td>0.0352017</td>\n",
       "<td>0.0058793</td></tr>\n",
       "<tr><td>power</td>\n",
       "<td>154.5631561</td>\n",
       "<td>0.0181231</td>\n",
       "<td>0.0030269</td></tr>\n",
       "<tr><td>stop</td>\n",
       "<td>73.6209717</td>\n",
       "<td>0.0086323</td>\n",
       "<td>0.0014418</td></tr>\n",
       "<tr><td>stop_minutes</td>\n",
       "<td>7.1566486</td>\n",
       "<td>0.0008391</td>\n",
       "<td>0.0001402</td></tr></table></div>"
      ],
      "text/plain": [
       "variable              relative_importance    scaled_importance    percentage\n",
       "--------------------  ---------------------  -------------------  ------------\n",
       "price                 8528.52                1                    0.167018\n",
       "flight_code           8061.08                0.945192             0.157864\n",
       "lead_time             6366.93                0.746546             0.124687\n",
       "days_to_last_holiday  3866.34                0.453342             0.0757164\n",
       "few_tickets_left      3569.98                0.418593             0.0699126\n",
       "dep_weeknum           3275.12                0.38402              0.0641384\n",
       "stay_days             3227.57                0.378444             0.0632071\n",
       "dep_weekday           2826.22                0.331384             0.0553472\n",
       "duration_minutes      2584.96                0.303096             0.0506226\n",
       "days_to_next_holiday  2132.41                0.250032             0.04176\n",
       "company               2103.76                0.246674             0.041199\n",
       "plane                 1542.78                0.180896             0.030213\n",
       "trip                  1051.58                0.123301             0.0205935\n",
       "arr_time_group        930.657                0.109123             0.0182255\n",
       "dep_time_group        459.939                0.0539296            0.00900722\n",
       "video                 300.218                0.0352017            0.00587933\n",
       "power                 154.563                0.0181231            0.00302689\n",
       "stop                  73.621                 0.00863233           0.00144176\n",
       "stop_minutes          7.15665                0.000839143          0.000140152"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# output2.take(2)\n",
    "(trainingData, testData) = output2.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stopped here! 20170513"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier trained in 117.26345801353455 seconds.\n",
      "Coefficients: (415,[1,17,18],[-0.636888486254,0.02752806023,-0.0236746971456])\n",
      "Intercept: -0.15746604930887775\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Load training data\n",
    "# training = spark.read.format(\"libsvm\").load(\"data/mllib/sample_libsvm_data.txt\")\n",
    "\n",
    "lr = LogisticRegression(maxIter=100, regParam=0.1, elasticNetParam=0.8)\n",
    "\n",
    "# Fit the model\n",
    "t0 = time()\n",
    "lrModel = lr.fit(trainingData)\n",
    "tt = time() - t0\n",
    "\n",
    "# Print the coefficients and intercept for logistic regression\n",
    "print(\"Classifier trained in \" + str(tt) + \" seconds.\")\n",
    "print(\"Coefficients: \" + str(lrModel.coefficients))\n",
    "print(\"Intercept: \" + str(lrModel.intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objectiveHistory:\n",
      "0.65965578316531\n",
      "0.65711778074993\n",
      "0.6526917948919334\n",
      "0.6500999301284001\n",
      "0.6492599933704122\n",
      "0.6489641671631153\n",
      "0.6485216921434988\n",
      "0.6484842065443268\n",
      "0.6483063221990527\n",
      "0.6482723762111214\n",
      "0.6482516218060789\n",
      "0.6482254209192646\n",
      "0.6481976474933386\n",
      "0.6481650074625309\n",
      "0.6481069533543712\n",
      "0.6477526451025746\n",
      "0.6475130339686226\n",
      "0.6474578323572088\n",
      "0.647396090473081\n",
      "0.6473465984142018\n",
      "0.6472936896040737\n",
      "0.6472569602439301\n",
      "0.6472170722058855\n",
      "0.6471895139254257\n",
      "0.6471591800353315\n",
      "0.6471434501387139\n",
      "0.6470705812041906\n",
      "0.6470686210102691\n",
      "0.6470669959768485\n",
      "0.6470618925480771\n",
      "0.647060755627432\n",
      "0.6470597286031927\n",
      "0.6470580906753532\n",
      "0.6470578826597967\n",
      "0.64705783197502\n",
      "0.6470577567273563\n",
      "0.6470576866332658\n",
      "0.6470576435581784\n",
      "0.6470576237233439\n",
      "0.6470575811617785\n",
      "0.6470575110695101\n",
      "0.6470572963566015\n",
      "0.6470572552241214\n",
      "0.6470571417411584\n",
      "0.6470571010875171\n",
      "0.647057036177969\n",
      "0.6470570151283103\n",
      "0.6470569585685809\n",
      "0.6470568210794492\n",
      "0.6470567704462217\n",
      "0.6470567477581171\n",
      "0.6470566341047528\n",
      "0.6470566096080839\n",
      "0.6470564978490837\n",
      "0.6470564467918238\n",
      "0.6470564407736182\n",
      "0.6470564322263362\n",
      "0.6470563961325915\n",
      "0.647056383688619\n",
      "0.6470563557068195\n",
      "0.647056329659766\n",
      "0.6470563043774867\n",
      "0.6470562874789451\n",
      "0.6470562642642851\n",
      "0.6470562542147861\n",
      "0.647056237214919\n",
      "0.6470562284385118\n",
      "0.6470562112589087\n",
      "0.6470562039309504\n",
      "0.6470562028423321\n",
      "0.6470561987141671\n",
      "0.6470561970695624\n",
      "0.647056193259162\n",
      "0.6470561924139273\n",
      "0.6470561891902595\n",
      "0.6470561875893502\n",
      "0.6470561852618576\n",
      "0.6470561835460706\n",
      "0.6470561819200972\n",
      "0.6470561799415141\n",
      "0.647056178638611\n",
      "0.6470561768038378\n",
      "0.6470561757344877\n",
      "0.6470561739912609\n",
      "0.6470561727749966\n",
      "0.6470561718790174\n",
      "0.6470561706510145\n",
      "0.647056169713758\n",
      "0.6470561687872372\n",
      "0.6470561678742499\n",
      "0.6470561671880762\n",
      "0.647056166319005\n",
      "0.6470561656378336\n",
      "0.6470561646383202\n",
      "0.6470561644815959\n",
      "0.6470561632939417\n",
      "0.6470561629643342\n",
      "0.6470561620783383\n",
      "0.6470561616501709\n",
      "0.6470561609396622\n",
      "0.6470561603209987\n",
      "+-------------------+------------------+\n",
      "|                FPR|               TPR|\n",
      "+-------------------+------------------+\n",
      "|                0.0|               0.0|\n",
      "| 0.2442847811887655|0.5750950351342011|\n",
      "|0.24883592621182432|0.5803311306824727|\n",
      "| 0.7814003028323734|0.9500518373459279|\n",
      "|                1.0|               1.0|\n",
      "|                1.0|               1.0|\n",
      "+-------------------+------------------+\n",
      "\n",
      "areaUnderROC: 0.6935268346641583\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression_4eefbed75b8eca6922ce"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Extract the summary from the returned LogisticRegressionModel instance trained\n",
    "# in the earlier example\n",
    "trainingSummary = lrModel.summary\n",
    "\n",
    "# Obtain the objective per iteration\n",
    "objectiveHistory = trainingSummary.objectiveHistory\n",
    "print(\"objectiveHistory:\")\n",
    "for objective in objectiveHistory:\n",
    "    print(objective)\n",
    "\n",
    "# Obtain the receiver-operating characteristic as a dataframe and areaUnderROC.\n",
    "trainingSummary.roc.show()\n",
    "print(\"areaUnderROC: \" + str(trainingSummary.areaUnderROC))\n",
    "\n",
    "# Set the model threshold to maximize F-Measure\n",
    "fMeasure = trainingSummary.fMeasureByThreshold\n",
    "maxFMeasure = fMeasure.groupBy().max('F-Measure').select('max(F-Measure)').head()\n",
    "bestThreshold = fMeasure.where(fMeasure['F-Measure'] == maxFMeasure['max(F-Measure)']) \\\n",
    "    .select('threshold').head()['threshold']\n",
    "lr.setThreshold(bestThreshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+--------------------+\n",
      "|prediction|label|            features|\n",
      "+----------+-----+--------------------+\n",
      "|       0.0|    0|(415,[0,1,2,4,5,6...|\n",
      "|       0.0|    0|(415,[0,1,2,4,5,6...|\n",
      "|       0.0|    0|(415,[0,1,2,4,5,6...|\n",
      "|       0.0|    0|(415,[0,1,2,4,5,6...|\n",
      "|       0.0|    0|(415,[0,1,2,4,5,6...|\n",
      "+----------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o884.csv.\n: java.lang.UnsupportedOperationException: CSV data source does not support struct<type:tinyint,size:int,indices:array<int>,values:array<double>> data type.\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.org$apache$spark$sql$execution$datasources$csv$CSVFileFormat$$verifyType$1(CSVFileFormat.scala:233)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anonfun$verifySchema$1.apply(CSVFileFormat.scala:237)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anonfun$verifySchema$1.apply(CSVFileFormat.scala:237)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\r\n\tat scala.collection.IterableLike$class.foreach(IterableLike.scala:72)\r\n\tat org.apache.spark.sql.types.StructType.foreach(StructType.scala:96)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.verifySchema(CSVFileFormat.scala:237)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.prepareWrite(CSVFileFormat.scala:121)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:108)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:101)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:113)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:87)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:87)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.write(DataSource.scala:492)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:215)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:198)\r\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:579)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:280)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-019628324be8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Select example rows to display.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"prediction\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"label\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"features\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mpredictions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'prediction'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'features'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoalesce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'D://Data Science//pySpark//check_pred8.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\spark-2.1.0-bin-hadoop2.7\\spark-2.1.0-bin-hadoop2.7\\python\\pyspark\\sql\\readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[1;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat)\u001b[0m\n\u001b[0;32m    709\u001b[0m                        \u001b[0mnullValue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnullValue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mescapeQuotes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mescapeQuotes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquoteAll\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mquoteAll\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    710\u001b[0m                        dateFormat=dateFormat, timestampFormat=timestampFormat)\n\u001b[1;32m--> 711\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    712\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    713\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0msince\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-2.1.0-bin-hadoop2.7\\spark-2.1.0-bin-hadoop2.7\\python\\lib\\py4j-0.10.4-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1133\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1135\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-2.1.0-bin-hadoop2.7\\spark-2.1.0-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-2.1.0-bin-hadoop2.7\\spark-2.1.0-bin-hadoop2.7\\python\\lib\\py4j-0.10.4-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    318\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 319\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    320\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o884.csv.\n: java.lang.UnsupportedOperationException: CSV data source does not support struct<type:tinyint,size:int,indices:array<int>,values:array<double>> data type.\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.org$apache$spark$sql$execution$datasources$csv$CSVFileFormat$$verifyType$1(CSVFileFormat.scala:233)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anonfun$verifySchema$1.apply(CSVFileFormat.scala:237)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anonfun$verifySchema$1.apply(CSVFileFormat.scala:237)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\r\n\tat scala.collection.IterableLike$class.foreach(IterableLike.scala:72)\r\n\tat org.apache.spark.sql.types.StructType.foreach(StructType.scala:96)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.verifySchema(CSVFileFormat.scala:237)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.prepareWrite(CSVFileFormat.scala:121)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:108)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:101)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:113)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:87)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:87)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.write(DataSource.scala:492)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:215)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:198)\r\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:579)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:280)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n"
     ]
    }
   ],
   "source": [
    "# Make predictions.\n",
    "predictions = lrModel.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "predictions.select('label', 'prediction').coalesce(1).write.csv('D://Data Science//pySpark//check_pred7.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "# from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "# from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Load and parse the data file, converting it to a DataFrame.\n",
    "# data = spark.read.format(\"libsvm\").load(\"data/mllib/sample_libsvm_data.txt\")\n",
    "\n",
    "# Automatically identify categorical features, and index them.\n",
    "# Set maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "featureIndexer =\\\n",
    "    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=100).fit(output)\n",
    "    \n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = output.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train a RandomForest model.\n",
    "# rf = RandomForestRegressor(featuresCol=\"indexedFeatures\", numTrees=1000, featureSubsetStrategy=\"auto\",\n",
    "#                                     impurity='variance', maxDepth=4, maxBins=32)\n",
    "gbt = GBTClassifier(labelCol=\"label\", featuresCol=\"indexedFeatures\", maxIter=10)\n",
    "\n",
    "# Chain indexer and forest in a Pipeline\n",
    "pipeline = Pipeline(stages=[featureIndexer, gbt])\n",
    "\n",
    "# Train model.  This also runs the indexer.\n",
    "model = pipeline.fit(trainingData)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "# evaluator = RegressionEvaluator(\n",
    "#     labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\")\n",
    "auc = evaluator.evaluate(predictions)\n",
    "print(\"Area under the curve (AUC) on test data = %g\" % auc)\n",
    "\n",
    "gbtModel = model.stages[1]\n",
    "print(gbtModel)  # summary only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Load training data\n",
    "# training = spark.read.format(\"libsvm\").load(\"data/mllib/sample_libsvm_data.txt\")\n",
    "flight7.withColumnRenamed('price_will_drop_num', 'label')\n",
    "\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "# Fit the model\n",
    "lrModel = lr.fit(training)\n",
    "\n",
    "# Print the coefficients and intercept for logistic regression\n",
    "print(\"Coefficients: \" + str(lrModel.coefficients))\n",
    "print(\"Intercept: \" + str(lrModel.intercept))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "how to score model manually: http://stackoverflow.com/questions/35731140/apply-model-scores-to-spark-dataframe-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'path file:/D:/Data Science/pySpark/check_pred6.csv already exists.;'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mC:\\spark-2.1.0-bin-hadoop2.7\\spark-2.1.0-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-2.1.0-bin-hadoop2.7\\spark-2.1.0-bin-hadoop2.7\\python\\lib\\py4j-0.10.4-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    318\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 319\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    320\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o734.csv.\n: org.apache.spark.sql.AnalysisException: path file:/D:/Data Science/pySpark/check_pred6.csv already exists.;\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:80)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:113)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:87)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:87)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.write(DataSource.scala:492)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:215)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:198)\r\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:579)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:280)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-116dff4a21c7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpredictions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'prediction'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoalesce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'D://Data Science//pySpark//check_pred6.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\spark-2.1.0-bin-hadoop2.7\\spark-2.1.0-bin-hadoop2.7\\python\\pyspark\\sql\\readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[1;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat)\u001b[0m\n\u001b[0;32m    709\u001b[0m                        \u001b[0mnullValue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnullValue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mescapeQuotes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mescapeQuotes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquoteAll\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mquoteAll\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    710\u001b[0m                        dateFormat=dateFormat, timestampFormat=timestampFormat)\n\u001b[1;32m--> 711\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    712\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    713\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0msince\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-2.1.0-bin-hadoop2.7\\spark-2.1.0-bin-hadoop2.7\\python\\lib\\py4j-0.10.4-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1133\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1135\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-2.1.0-bin-hadoop2.7\\spark-2.1.0-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[0;32m     68\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: 'path file:/D:/Data Science/pySpark/check_pred6.csv already exists.;'"
     ]
    }
   ],
   "source": [
    "predictions.select('label', 'prediction').coalesce(1).write.csv('D://Data Science//pySpark//check_pred6.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Load and parse the data file, converting it to a DataFrame.\n",
    "data = flight3\n",
    "\n",
    "# Automatically identify categorical features, and index them.\n",
    "# Set maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "# featureIndexer =\\\n",
    "#     VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(data)\n",
    "\n",
    "\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = data.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train a GBT model.\n",
    "gbt = GBTRegressor(featuresCol=\"indexedFeatures\", maxIter=10)\n",
    "\n",
    "# Chain indexer and GBT in a Pipeline\n",
    "pipeline = Pipeline(stages=[featureIndexer, gbt])\n",
    "\n",
    "# Train model.  This also runs the indexer.\n",
    "model = pipeline.fit(trainingData)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
    "\n",
    "gbtModel = model.stages[1]\n",
    "print(gbtModel)  # summary only\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# # Load and parse the data file, converting it to a DataFrame.\n",
    "# data = spark.read.format(\"libsvm\").load(\"data/mllib/sample_libsvm_data.txt\")\n",
    "\n",
    "# # Automatically identify categorical features, and index them.\n",
    "# # Set maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "featureIndexer =\\\n",
    "    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(flight3)\n",
    "\n",
    "# # Split the data into training and test sets (30% held out for testing)\n",
    "# (trainingData, testData) = data.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train a GBT model.\n",
    "gbt = GBTRegressor(featuresCol=\"indexedFeatures\", maxIter=10)\n",
    "\n",
    "# Chain indexer and GBT in a Pipeline\n",
    "pipeline = Pipeline(stages=[featureIndexer, gbt])\n",
    "\n",
    "# Train model.  This also runs the indexer.\n",
    "model = pipeline.fit(trainingData)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
    "\n",
    "gbtModel = model.stages[1]\n",
    "print(gbtModel)  # summary only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# display(flight.select(\"duration\").show())\n",
    "# display(flight2.select(\"duration_h\", 'duration_m').show())\n",
    "display(flight2.show(5))\n",
    "# flight2.toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def getHours(x):\n",
    "  return re.match('([0-9]+(?=h))', x)\n",
    "temp = flight.select(\"duration\").rdd.map(lambda x:getHours(x[0])).toDF()\n",
    "temp.select(\"duration\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flight.select(\"arr_time\").show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# flight2 = (flight.withColumn('arr_time_local', flight.arr_time.substr(1, 23).cast('date'))                 \n",
    "#           )\n",
    "\n",
    "# flight3 = flight2.withColumn('arr_time', flight2.arr_time.cast('timestamp'))\n",
    "\n",
    "# .select('arr_time').show(2, truncate=False)\n",
    "\n",
    "\n",
    "# flight2 = (flight.withColumn('duration_minutes', flight.duration.substr(1, 23).cast('date'))                 \n",
    "#           )\n",
    "\n",
    "# flight.selectExpr(\"duration\", \"regexp_extract(duration,'([0-9]+(?=h))', 1) as duration_hour\", \"regexp_extract(duration,'([0-9]+(?=m))', 1)\", ).show()\n",
    "flight.selectExpr(\"regexp_extract(duration,'([0-9]+(?=h))', 1) as duration_hour\").duration_hour.cast(\"double\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flight2.select('arr_time', 'arr_time_local').dtypes\n",
    "flight2.select('arr_time', 'arr_time_local').show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql import crosstab\n",
    "# from pyspark.sql.functions import *\n",
    "\n",
    "flight.crosstab('start_date','from_city_name').show()\n",
    "# flight.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "3\n",
    "down vote\n",
    "In order to simply cast a string column to a timestamp, the string column must be properly formatted.\n",
    "\n",
    "To retrieve the \"createdAt\" column as a timestamp, you can write the UDF function that would convert the string\n",
    "\n",
    "\"2016-07-01T16:37:41-0400\"\n",
    "to\n",
    "\n",
    "\"2016-07-01 16:37:41\"\n",
    "and convert the \"createdAt\" column to a new format (don't forget to handle the timezone field).\n",
    "\n",
    "Once you have a column containing timestamps as strings like \"2016-07-01 16:37:41\", a simple cast to timestamp would do the job, as you have it in your code.\n",
    "\n",
    "You can read more about Date/Time/String Handling in Spark here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "display(flight.select(\"arr_time\").take(3))\n",
    "display(flight2.select(\"arr_time\").take(3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flight2.withColumn('local', F.from_utc_timestamp(flight2.arr_time, \"AEST\")).select(\"local\").show()\n",
    "flight2.withColumn('local', F.from_utc_timestamp(flight2.arr_time, \"CTZ\")).select(\"local\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import pytz # $ pip install pytz\n",
    "from tzlocal import get_localzone # $ pip install tzlocal\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get local timezone    \n",
    "local_tz = get_localzone() \n",
    "print(local_tz)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test it\n",
    "# utc_now, now = datetime.utcnow(), datetime.now()\n",
    "ts = time.time()\n",
    "utc_now, now = datetime.utcfromtimestamp(ts), datetime.fromtimestamp(ts)\n",
    "print(utc_now)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "local_now = utc_now.replace(tzinfo=pytz.utc).astimezone(local_tz) # utc -> local\n",
    "print(local_now)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert local_now.replace(tzinfo=None) == now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test it\n",
    "# utc_now, now = datetime.utcnow(), datetime.now()\n",
    "ts = time.time()\n",
    "utc_now, now = datetime.utcfromtimestamp(ts), datetime.fromtimestamp(ts)\n",
    "\n",
    "local_now = utc_now.replace(tzinfo=pytz.utc).astimezone(local_tz) # utc -> local\n",
    "assert local_now.replace(tzinfo=None) == now\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Spark-Sql doesn't support date-time, and nor timezones\n",
    "* Using timestamp is the only solution\n",
    "* from_unixtime(at) parses the epoch time correctly, just that the printing of it as a string changes it due to timezone. It is safe to assume that the  from_unixtime will convert it correctly ( although printing it might show different results)\n",
    "* from_utc_timestamp will shift ( not just convert) the timestamp to that timezone, in this case it will subtract 8 hours to the time since (-08:00)\n",
    "* printing sql results messes up the times with respect to timezone param\n",
    "  \t \t\n",
    "* from_unixtime(at) does what from_utc_timestamp does too, it will parse a Unix timestamp integer (seconds since midnight 1970-01-01), and convert the time instant parsed from UTC to the system's default timezone. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flight2.selectExpr(\"from_utc_timestamp(arr_time, 'AEST') as testthis\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flight2.dtypes\n",
    "display(flight2.select('arr_time').take(3))\n",
    "display(flight.select('arr_time').take(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, month, dayofmonth, hour, minute, weekofyear, crosstab\n",
    "\n",
    "flight2.select(year(\"arr_time\").alias('year'), \n",
    "               month(\"arr_time\").alias('month'),\n",
    "               dayofmonth(\"arr_time\").alias('day'),\n",
    "               hour(\"arr_time\").alias('hour'),\n",
    "               minute(\"arr_time\").alias('minute'),\n",
    "               weekofyear('arr_time').alias('week_no'),\n",
    "               \"arr_time_zone\"\n",
    "              ).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_extract, col\n",
    "\n",
    "#flight.select(regexp_extract('arr_time', r'[+-][0-9]{2}:[0-9]{2}\\b', 1)).alias('d').collect\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([('2017-04-09T07:15:00.000+08:00',)], ['str'])\n",
    "df.select(regexp_extract('str', '(\\d+)-(\\d+)', 1).alias('d')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flight2.describe('price').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "# flight2.dtypes\n",
    "# flight2.first()\n",
    "display(flight2.select(max(\"start_date\")).show())\n",
    "display(flight2.select(min(\"start_date\")).show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flight2 = flight2.withColumn('zero_price', flight2.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# flight.describe().show()\n",
    "flight.describe(\"from_city_name\").show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import colflight.filter(flight.price > 0).groupby(flight.search_date_x, flight.from_city_name, flight.to_city_name).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "# flight_to_brisbane = flight.where(col(\"price\") > 0 & col(\"to_city_name\") == \"brisbane\").groupby(flight.search_date_x).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.where(col(\"v\").isin({\"foo\", \"bar\"})).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numeric = sqlContext.createDataFrame([\n",
    "    ('3.5,', '5.0', 'null'), ('2.0', '14.0', 'null'),  ('null', '38.0', 'null'),\n",
    "    ('null', 'null', 'null'),  ('1.0', 'null', '4.0')],\n",
    "    ('low', 'high', 'normal'))\n",
    "\n",
    "numeric_filtered_1 = numeric.where(numeric['LOW'] != 'null')\n",
    "numeric_filtered_1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flight.select('from_city_name', 'price').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "h2o tutorial \n",
    "https://github.com/h2oai/h2o-tutorials/blob/master/tutorials/pysparkling/Chicago_Crime_Demo.md"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

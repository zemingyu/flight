{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resources list**\n",
    "1. pyspark tutorial https://github.com/pydatasg/Pydata_meetup_Nov_16/blob/master/RF_modeling_2.3_business_weimin.ipynb\n",
    "2. follow this tutorial: https://www.analyticsvidhya.com/blog/2016/10/spark-dataframe-and-operations/\n",
    "3. and this one: https://www.dezyre.com/apache-spark-tutorial/pyspark-tutorial\n",
    "4. data engineer trick: http://nadbordrozd.github.io/blog/2016/05/22/one-weird-trick-that-will-fix-your-pyspark-schemas/\n",
    "5. git http://rogerdudler.github.io/git-guide/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get data\n",
    "## Downloand data from s3\n",
    "version 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! cd /home/ubuntu/s3/flight_1_5\n",
    "\n",
    "! aws s3 sync s3://flight.price/flight_1_5 . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "version 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! aws s3 ls s3://flight.price.11/flight_1_5 --recursive\n",
    "! aws s3 sync s3://flight.price.11/flight_1_5 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! du -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract\n",
    "Note: version 1.1 from 2017-05-11 onwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "# dir_in = 'C:\\\\s3\\\\20170503\\\\flight_1_6' # sydney to shanghai\n",
    "# dir_in = 'C:\\\\s3\\\\20170503\\\\flight_5_1' # beijing to sydney\n",
    "# dir_in = 'C:\\\\s3\\\\20170503'\n",
    "dir_in = '/home/ubuntu/s3/flight_1_5'\n",
    "\n",
    "# dir_out = 'C:\\\\s3\\\\20170503_extracted\\\\flight_1_6'\n",
    "dir_out = '/home/ubuntu/s3/flight_1_5/extracted'\n",
    "extension = \".zip\"\n",
    "\n",
    "os.chdir(dir_in) # change directory from working dir to dir with files\n",
    "\n",
    "for subdir, dirs, files in os.walk(dir_in):\n",
    "    for item in files:\n",
    "        if item.endswith(extension): # check for \".zip\" extension\n",
    "            file_name = os.path.join(subdir, item)\n",
    "#             file_name = os.path.abspath(item) # get full path of files\n",
    "            zip_ref = zipfile.ZipFile(file_name) # create zipfile object\n",
    "            zip_ref.extractall(dir_out) # extract file to dir\n",
    "            zip_ref.close() # close file  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move files in final_results to the main folder and delete the folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ! cd /home/ubuntu/s3/flight_1_5/extracted\n",
    "# ! mv /home/ubuntu/s3/flight_1_5/extracted/final_results/*.txt .\n",
    "! find /home/ubuntu/s3/flight_1_5/extracted/final_results -name '*.txt' -exec mv {} /home/ubuntu/s3/flight_1_5/extracted \\;\n",
    "# -exec runs any command,  {} inserts the filename found, \\; marks the end of the exec command.\n",
    "! rmdir /home/ubuntu/s3/flight_1_5/extracted/final_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://stackoverflow.com/questions/9157138/recursively-counting-files-in-a-linux-directory\n",
    "* -type f to include only files.  \n",
    "* | (and not Â¦) redirects find command's standard output to wc command's standard input.  \n",
    "* wc (short for word count) counts newlines, words and bytes on its input (docs).  \n",
    "* -l to count just newlines.\n",
    "* You can also remove the -type f to include directories (and symlinks) in the count.\n",
    "* It's possible this command will overcount if filenames can contain newline characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! find /home/ubuntu/s3/flight_1_5/extracted -type f | wc -l\n",
    "! du -sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Launch spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import h2o\n",
    "# import pysparkling\n",
    "import zipfile\n",
    "import os\n",
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "from IPython.display import display\n",
    "from pyspark.sql.functions import regexp_extract, col, split, udf, trim, when, from_unixtime, unix_timestamp, minute, hour, datediff\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import IntegerType, StringType, BooleanType\n",
    "# from pyspark.sql.types import *\n",
    "import datetime\n",
    "import argparse\n",
    "import json\n",
    "import glob, os, shutil\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize\n",
    "\n",
    "pd.options.display.max_columns = 99\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext()\n",
    "\n",
    "# # # spark_home = os.environ.get('SPARK_HOME', None)\n",
    "# spark_home = \"C:\\spark-2.1.0-bin-hadoop2.7\\spark-2.1.0-bin-hadoop2.7\"\n",
    "\n",
    "# if not spark_home:\n",
    "\n",
    "#     raise ValueError('SPARK_HOME environment variable is not set')\n",
    "\n",
    "# sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "\n",
    "# sys.path.insert(0, os.path.join(spark_home, 'C:\\spark-2.1.0-bin-hadoop2.7\\spark-2.1.0-bin-hadoop2.7\\python\\lib\\py4j-0.10.4-src.zip')) ## may need to adjust on your system depending on which Spark version you're using and where you installed it.\n",
    "\n",
    "# exec(open(os.path.join(spark_home, 'python/pyspark/shell.py')).read())\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"Python Spark SQL basic example\") \\\n",
    "        .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "        .getOrCreate()\n",
    "        \n",
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# hc= H2OContext(sc).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# h2o not working yet\n",
    "# # Start H2O Context\n",
    "# from pysparkling import *\n",
    "# sc\n",
    "# hc= H2OContext(sc).start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in json files and aggregate into a dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy version 1.1 files to a separate folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mkfldr(fldr):\n",
    "    try:\n",
    "      os.makedirs(fldr)\n",
    "    except:\n",
    "      print(\"Folder already exist or some error\")\n",
    "    \n",
    "def read_from_json_file(filename):\n",
    "    with open(filename,'r') as f:\n",
    "        d = json.load(f)   \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "src_fldr = \"/home/ubuntu/s3/flight_1_5/extracted\"\n",
    "dst_fldr = \"/home/ubuntu/s3/flight_1_5/extracted11\"\n",
    "\n",
    "mkfldr(dst_fldr)\n",
    "\n",
    "for txt_file in glob.glob(os.path.join(parent_dir, 'flight_1_5_price_2017-05-1[1-4]*.txt')):\n",
    "    shutil.move(txt_file, dst_fldr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Append multiple json files to a single jsonl file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! find -type f -name \"flight_1_5_price_2017-05-10_2017-06-10*.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "comb_fldr = '/home/ubuntu/s3/comb'\n",
    "mkfldr(comb_fldr)\n",
    "\n",
    "pq_fldr = '/home/ubuntu/s3/pq'\n",
    "mkfldr(pq_fldr)\n",
    "\n",
    "os.chdir('/home/ubuntu/s3/flight_1_5/extracted')\n",
    "\n",
    "# json_file_name = 'flight_1_5_price_2017-05-10_2017-11-06_2_7.txt'\n",
    "# pq_file_name = os.path.join(pq_fldr, json_file_name.replace('.jsonl','.parquet'))\n",
    "\n",
    "json_file_name = 'flight_1_5_price_2017-05-10_2017-06-10*.txt'\n",
    "jsonl_file_name = os.path.join(comb_fldr, json_file_name.replace('.txt','.jsonl'))\n",
    "\n",
    "# 'flight_1_5_price_2017-05-[8|9|10]*.txt'\n",
    "\n",
    "with open(jsonl_file_name, 'w') as outfile:\n",
    "    for file in glob.glob(json_file_name):                \n",
    "        d = read_from_json_file(file)                \n",
    "        json.dump(d, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in jsonl file, flatten it, and save as a new flat jsonl file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set parquet schema and create empty parquet file to append to\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def chunkIt(seq, num):\n",
    "  avg = len(seq) / float(num)\n",
    "  index_start = []\n",
    "  index_end = []\n",
    "  last = 0.0\n",
    "\n",
    "  while last < len(seq):\n",
    "#     out.append(seq[int(last):int(last + avg)])\n",
    "    index_start.append(int(last))\n",
    "    index_end.append(int(last + avg))\n",
    "    last += avg\n",
    "\n",
    "  return list(zip(index_start, index_end))\n",
    "\n",
    "# files = [os.path.join(ext_fldr, \"flight_1_5_price_2017-04-06_2017-04-07_1_0.txt\"), \n",
    "#         os.path.join(ext_fldr, \"flight_1_5_price_2017-04-06_2017-04-07_2_7.txt\")]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set chunk size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22665\n",
      "[(0, 226), (226, 453), (453, 679), (679, 906), (906, 1133), (1133, 1359), (1359, 1586), (1586, 1813), (1813, 2039), (2039, 2266), (2266, 2493), (2493, 2719), (2719, 2946), (2946, 3173), (3173, 3399), (3399, 3626), (3626, 3853), (3853, 4079), (4079, 4306), (4306, 4533), (4533, 4759), (4759, 4986), (4986, 5212), (5212, 5439), (5439, 5666), (5666, 5892), (5892, 6119), (6119, 6346), (6346, 6572), (6572, 6799), (6799, 7026), (7026, 7252), (7252, 7479), (7479, 7706), (7706, 7932), (7932, 8159), (8159, 8386), (8386, 8612), (8612, 8839), (8839, 9065), (9065, 9292), (9292, 9519), (9519, 9745), (9745, 9972), (9972, 10199), (10199, 10425), (10425, 10652), (10652, 10879), (10879, 11105), (11105, 11332), (11332, 11559), (11559, 11785), (11785, 12012), (12012, 12239), (12239, 12465), (12465, 12692), (12692, 12919), (12919, 13145), (13145, 13372), (13372, 13598), (13598, 13825), (13825, 14052), (14052, 14278), (14278, 14505), (14505, 14732), (14732, 14958), (14958, 15185), (15185, 15412), (15412, 15638), (15638, 15865), (15865, 16092), (16092, 16318), (16318, 16545), (16545, 16772), (16772, 16998), (16998, 17225), (17225, 17452), (17452, 17678), (17678, 17905), (17905, 18131), (18131, 18358), (18358, 18585), (18585, 18811), (18811, 19038), (19038, 19265), (19265, 19491), (19491, 19718), (19718, 19945), (19945, 20171), (20171, 20398), (20398, 20625), (20625, 20851), (20851, 21078), (21078, 21305), (21305, 21531), (21531, 21758), (21758, 21985), (21985, 22211), (22211, 22438), (22438, 22665)]\n",
      "226\n"
     ]
    }
   ],
   "source": [
    "ext_fldr = '/home/ubuntu/s3/flight_1_5/extracted'\n",
    "file_count = len(glob.glob(os.path.join(ext_fldr, \"*.txt\")))\n",
    "range_temp = chunkIt(range(file_count), 100)\n",
    "\n",
    "print(file_count)\n",
    "print(range_temp)\n",
    "print(range_temp[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create empty parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/s3/flight_1_5/extracted/flight_1_5_price_2017-04-23_2017-04-30_2_21.txt completed. Count: 1\n"
     ]
    }
   ],
   "source": [
    "flat_table_list = []  # a list\n",
    "\n",
    "ext_fldr = '/home/ubuntu/s3/flight_1_5/extracted'\n",
    "comb_fldr = '/home/ubuntu/s3/comb'\n",
    "# os.chdir(comb_fldr)\n",
    "# file_list = os.listdir(comb_fldr)\n",
    "\n",
    "aggDF = pd.DataFrame() #aggregate dataframe to hold all individual dataframes\n",
    "count = 0\n",
    "\n",
    "for file in glob.glob(os.path.join(ext_fldr, \"*.txt\"))[0:1]:\n",
    "    with open(file) as f:               \n",
    "        flat_table = []  # a list\n",
    "        d = json.load(f)\n",
    "        flight_list = json_normalize(d['flight_list'])\n",
    "        basic = json_normalize(d['basic'])\n",
    "        \n",
    "        # create dataframe\n",
    "        basic = basic.drop('search_date', 1)\n",
    "        basic['tmp'] = 1\n",
    "        flight_list = flight_list.drop('id', 1)\n",
    "        flight_list['tmp'] = 1\n",
    "        DF = pd.merge(basic, flight_list, on=['tmp'])\n",
    "        DF=DF.drop('tmp', 1)\n",
    "        \n",
    "        # append\n",
    "        aggDF = aggDF.append(DF)\n",
    "        \n",
    "        count = count + 1\n",
    "        \n",
    "        print(file+\" completed. Count: \" + str(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(114, 29)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggDF.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['from_city_name', 'start_date', 'stay_days', 'table_name', 'task_id',\n",
       "       'to_city_name', 'trip', 'version', 'airline_code', 'airline_codes',\n",
       "       'arr_time', 'check_bag_inc', 'company', 'dep_time', 'duration',\n",
       "       'flight_code', 'flight_number', 'index', 'plane', 'power', 'price',\n",
       "       'price_code', 'search_date', 'span_days', 'stop', 'stop_info',\n",
       "       'ticket_left', 'video', 'wifi'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggDF.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# force to string\n",
    "# aggDF['id'] = 'Unknown'\n",
    "# aggDF['video'] = False\n",
    "# aggDF['wifi'] = False\n",
    "# aggDF['power'] = False\n",
    "# aggDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+---------+----------------+-------+------------+----+-------+------------+-------------+-----------------------------+-------------+--------------+-----------------------------+--------+-----------+-------------+-----+-------------------------+-----+-------+----------+-----------+---------+----+--------------------+-----------+-----+-----+\n",
      "|from_city_name|start_date|stay_days|table_name      |task_id|to_city_name|trip|version|airline_code|airline_codes|arr_time                     |check_bag_inc|company       |dep_time                     |duration|flight_code|flight_number|index|plane                    |power|price  |price_code|search_date|span_days|stop|stop_info           |ticket_left|video|wifi |\n",
      "+--------------+----------+---------+----------------+-------+------------+----+-------+------------+-------------+-----------------------------+-------------+--------------+-----------------------------+--------+-----------+-------------+-----+-------------------------+-----+-------+----------+-----------+---------+----+--------------------+-----------+-----+-----+\n",
      "|sydney        |2017-04-30|21       |flight_1_5_price|34     |beijing     |2   |1.0    |CX          |[CX, KA]     |2017-04-30T22:30:00.000+08:00|false        |Cathay Pacific|2017-04-30T10:05:00.000+10:00|14h25m  |CX162      |162          |50   |AIRBUS INDUSTRIE A330-300|true |1001.79|AUD       |2017-04-23 |0        |1   |Hong Kong(HKG):1h10m|0          |true |false|\n",
      "|sydney        |2017-04-30|21       |flight_1_5_price|34     |beijing     |2   |1.0    |CX          |[CX, KA]     |2017-04-30T23:25:00.000+08:00|false        |Cathay Pacific|2017-04-30T10:05:00.000+10:00|15h20m  |CX162      |162          |77   |AIRBUS INDUSTRIE A330-300|false|1001.79|AUD       |2017-04-23 |0        |1   |Hong Kong(HKG):2h10m|0          |false|false|\n",
      "+--------------+----------+---------+----------------+-------+------------+----+-------+------------+-------------+-----------------------------+-------------+--------------+-----------------------------+--------+-----------+-------------+-----+-------------------------+-----+-------+----------+-----------+---------+----+--------------------+-----------+-----+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sp_df_test = spark.createDataFrame(aggDF)\n",
    "sp_df_test.show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create empty dataframe and save to empty parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# if necessary delete the parquet file\n",
    "! rm -rf ~/s3/comb/flight_v1_0.pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "struct_v1_0 = sp_df_test.schema\n",
    "emptyDF = spark.createDataFrame(sc.emptyRDD(), struct_v1_0)\n",
    "# emptyDF = spark.createDataFrame(aggDF, struct_v1_0)\n",
    "emptyDF.write.parquet(os.path.join(comb_fldr, \"flight_v1_0.pq\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 226\n",
      "226 453\n"
     ]
    }
   ],
   "source": [
    "len(range_temp)\n",
    "for chunk_num in range(0, 2):\n",
    "    print(str(range_temp[chunk_num][0]), str(range_temp[chunk_num][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopped here 20170521.  \n",
    "  \n",
    "**Need to**:   \n",
    "* test generating multip pandas df and read into spark and then parquet - DONE\n",
    "* build loop within flight_1_5 - DONE\n",
    "* build loop for different sub folders on s3\n",
    "\n",
    "**Problem**:   \n",
    "* need to figure out how to flatten nested spark dataframe - for now just do this in pandas\n",
    "\n",
    "Check out https://www.youtube.com/watch?v=noFkYVkixPA  \n",
    "and https://spark.apache.org/docs/2.1.1/sql-programming-guide.html#save-modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ext_fldr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-0bed59a2ddcc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mfile_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mglob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mext_fldr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"*.txt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mrange_temp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchunkIt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_count\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ext_fldr' is not defined"
     ]
    }
   ],
   "source": [
    "def append_pq(df):    \n",
    "    spDF = spark.createDataFrame(df, struct_v1_0)\n",
    "    spDF.repartition(1).write.mode('append').parquet(os.path.join(comb_fldr, \"flight_v1_0.pq\"))\n",
    "    print(os.path.join(comb_fldr, \"flight_v1_0.pq\") + \" saved!\")    \n",
    "    \n",
    "    \n",
    "file_count = len(glob.glob(os.path.join(ext_fldr, \"*.txt\")))\n",
    "range_temp = chunkIt(range(file_count), 100)\n",
    "\n",
    "count = 0\n",
    "\n",
    "for chunk_num in range(0, 100):    \n",
    "    aggDF = pd.DataFrame() #aggregate dataframe to hold all individual dataframes\n",
    "\n",
    "    for file in glob.glob(os.path.join(ext_fldr, \"*.txt\"))[range_temp[chunk_num][0]:range_temp[chunk_num][1]]:        \n",
    "        with open(file) as f:                           \n",
    "            d = json.load(f)\n",
    "            flight_list = json_normalize(d['flight_list'])\n",
    "            basic = json_normalize(d['basic'])\n",
    "\n",
    "            # create dataframe\n",
    "            basic = basic.drop('search_date', 1)\n",
    "            basic['tmp'] = 1\n",
    "            flight_list = flight_list.drop('id', 1)\n",
    "            flight_list['tmp'] = 1\n",
    "            DF = pd.merge(basic, flight_list, on=['tmp'])\n",
    "            DF=DF.drop('tmp', 1)\n",
    "            \n",
    "\n",
    "            # append\n",
    "            aggDF = aggDF.append(DF)\n",
    "    count = count + 1\n",
    "    print(\"Count: \" + str(count))        \n",
    "    append_pq(aggDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flight = spark.read.parquet(\"/home/ubuntu/s3/comb/flight_v1_0.pq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(from_city_name='sydney', start_date='2017-09-09', stay_days='28', table_name='flight_1_5_price', task_id='620', to_city_name='beijing', trip='2', version='1.0', airline_code='QF', airline_codes=['QF', 'CA'], arr_time='2017-09-10T04:10:00.000+08:00', check_bag_inc=False, company='Qantas Airways', dep_time='2017-09-09T11:15:00.000+10:00', duration='18h55m', flight_code='QF145', flight_number='145', index=55, plane='BOEING 737-800 (WINGLETS) PASSENGER', power=None, price=0.0, price_code='', search_date='2017-05-08', span_days=0, stop=1, stop_info='Auckland(AKL):2h35m', ticket_left=0, video=None, wifi=None),\n",
       " Row(from_city_name='sydney', start_date='2017-09-09', stay_days='28', table_name='flight_1_5_price', task_id='620', to_city_name='beijing', trip='2', version='1.0', airline_code='MU', airline_codes=['MU', 'MU'], arr_time='2017-09-10T17:25:00.000+08:00', check_bag_inc=False, company='China Eastern Airlines', dep_time='2017-09-09T20:50:00.000+10:00', duration='22h35m', flight_code='MU778', flight_number='778', index=4, plane='AIRBUS INDUSTRIE A330-200', power=None, price=472.14, price_code='AUD', search_date='2017-05-08', span_days=0, stop=1, stop_info='Kunming(KMG):8h30m', ticket_left=4, video=None, wifi=None)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flight.count()\n",
    "flight.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flight.repartition(1).write.parquet(os.path.join(comb_fldr, \"flight_v1_0_comb.pq\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get size of the s3 bucket\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "def get_folder_size(bucket, prefix):\n",
    "    total_size = 0\n",
    "    for obj in boto3.resource('s3').Bucket(bucket).objects.filter(Prefix=prefix):\n",
    "        total_size += obj.size\n",
    "    return total_size\n",
    "\n",
    "# get_folder_size('flight_1_14', 'flight.price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import boto3 \n",
    "s3_client = boto3.client(\"s3\")\n",
    "all_objects = s3_client.list_objects(Bucket = 'flight.price') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1332\r\n",
      "-rw-rw-r-- 1 ubuntu ubuntu  14396 May 16 11:39 cleanData.Rmd\r\n",
      "-rw-rw-r-- 1 ubuntu ubuntu  84798 May 20 13:23 hs_err_pid2004.log\r\n",
      "-rw-rw-r-- 1 ubuntu ubuntu  84176 May 17 12:49 hs_err_pid9072.log\r\n",
      "-rw-rw-r-- 1 ubuntu ubuntu  81011 May 17 13:09 hs_err_pid9338.log\r\n",
      "-rw-rw-r-- 1 ubuntu ubuntu 466518 May 20 13:23 replay_pid2004.log\r\n",
      "drwxrwxr-x 2 ubuntu ubuntu   4096 May 16 14:05 spark-warehouse\r\n",
      "-rwxrwxr-x 1 ubuntu ubuntu 619093 May 25 10:05 test_pyspark.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "! cd ~/s3/comb/zip\n",
    "! ls -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "120/20*1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'ETag': '\"7ddc5ebf67b4f8b64deb75b23b0f1b32-7\"',\n",
       "  'Key': 'flight_15_10/flight_15_10_price_2017-04-25.zip',\n",
       "  'LastModified': datetime.datetime(2017, 4, 26, 4, 22, 58, tzinfo=tzlocal()),\n",
       "  'Owner': {'DisplayName': 'jingwangian',\n",
       "   'ID': '5e5078ad36b7435fdafdb000491d465746defe398a4f8f1c4b841323b53cc316'},\n",
       "  'Size': 56971822,\n",
       "  'StorageClass': 'STANDARD'},\n",
       " {'ETag': '\"6439938860b1f61d1f4ff87b190dedf1-8\"',\n",
       "  'Key': 'flight_15_10/flight_15_10_price_2017-04-26.zip',\n",
       "  'LastModified': datetime.datetime(2017, 4, 27, 13, 15, 33, tzinfo=tzlocal()),\n",
       "  'Owner': {'DisplayName': 'jingwangian',\n",
       "   'ID': '5e5078ad36b7435fdafdb000491d465746defe398a4f8f1c4b841323b53cc316'},\n",
       "  'Size': 58827784,\n",
       "  'StorageClass': 'STANDARD'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_objects['Contents'][0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_objects' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-2a8e8606ad89>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_objects\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Contents'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'all_objects' is not defined"
     ]
    }
   ],
   "source": [
    "len(all_objects['Contents'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did a test using route 1_5  \n",
    "* 38 zip files  \n",
    "* 1.5 GB txt file\n",
    "* 20MB parquet\n",
    "* Optimal parquet say 120MB\n",
    "* 120/20 * 38 = 228 zip files\n",
    "* 120/20 * 1.5 = 9 GB\n",
    "so process 228 zip file at a time  \n",
    "1000 zip files, so do 5 chunks  \n",
    "within each chunk, process 5 zip files at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dir_in = '/home/ubuntu/s3/flight_1_5'\n",
    "# dir_out = '/home/ubuntu/s3/flight_1_5/extracted'\n",
    "# extension = \".zip\"\n",
    "\n",
    "from os.path import join\n",
    "from os import listdir, rmdir\n",
    "from shutil import move\n",
    "\n",
    "\n",
    "def unzip_files(dir_in, dir_out, extension):\n",
    "    os.chdir(dir_in) # change directory from working dir to dir with files\n",
    "    for subdir, dirs, files in os.walk(dir_in):\n",
    "        for item in files:\n",
    "            if item.endswith(extension): # check for \".zip\" extension\n",
    "                file_name = os.path.join(subdir, item)\n",
    "                zip_ref = zipfile.ZipFile(file_name) # create zipfile object\n",
    "                zip_ref.extractall(dir_out) # extract file to dir\n",
    "                zip_ref.close() # close file              \n",
    "                \n",
    "def chunker(seq, size):\n",
    "    return (seq[pos:pos + size] for pos in range(0, len(seq), size))\n",
    "\n",
    "\n",
    "def append_pq(df, pq_folder, pq_file_name):    \n",
    "    spDF = spark.createDataFrame(df, struct_v1_0)\n",
    "    \n",
    "    # if exists then append, otherwise create new\n",
    "    if os.path.isdir(os.path.join(pq_folder, pq_file_name)):     \n",
    "        spDF.repartition(1).write.mode('append').parquet(os.path.join(pq_folder, pq_file_name))\n",
    "        print(os.path.join(pq_folder, pq_file_name) + \" saved!\")\n",
    "    else:\n",
    "        spDF.repartition(1).write.parquet(os.path.join(pq_folder, pq_file_name))\n",
    "        \n",
    "\n",
    "def txt_to_parquet(ext_fldr, pq_folder, pq_file_name):\n",
    "    aggDF = pd.DataFrame() #aggregate dataframe to hold all individual dataframes        \n",
    "    count = 0\n",
    "    \n",
    "#     file_count = len(glob.glob(os.path.join(ext_fldr, \"*.txt\")))\n",
    "#     range_temp = chunkIt(range(file_count), 100)\n",
    "\n",
    "#     for chunk_num in range(0, 100):       \n",
    "#         for file in glob.glob(os.path.join(ext_fldr, \"*.txt\"))[range_temp[chunk_num][0]:range_temp[chunk_num][1]]:        \n",
    "    for file in glob.glob(os.path.join(ext_fldr, \"*.txt\")):        \n",
    "        with open(file) as f:                           \n",
    "            d = json.load(f)\n",
    "            try:\n",
    "                flight_list = json_normalize(d['flight_list'])\n",
    "                basic = json_normalize(d['basic'])\n",
    "            except Exception as e:                                \n",
    "                copyFile(file, file.replace('/txt/', '/txt_exception/'))\n",
    "#                 print(\"Not version 1.0. Copied to txt_exception folder: \" + file)\n",
    "                continue\n",
    "\n",
    "\n",
    "            # create dataframe\n",
    "            basic = basic.drop('search_date', 1)\n",
    "            basic['tmp'] = 1\n",
    "            flight_list = flight_list.drop('id', 1)\n",
    "            flight_list['tmp'] = 1\n",
    "            DF = pd.merge(basic, flight_list, on=['tmp'])\n",
    "            DF=DF.drop('tmp', 1)            \n",
    "\n",
    "            # append\n",
    "            aggDF = aggDF.append(DF)\n",
    "\n",
    "            # counter\n",
    "            count += 1\n",
    "            if count % 1000 == 0:\n",
    "                print(\"processed \" + str(count) + \" files\")\n",
    "    if not(aggDF.empty):\n",
    "        append_pq(aggDF, pq_folder, pq_file_name)    \n",
    "    #     count = count + 1\n",
    "        #     print(\"Count: \" + str(count))     \n",
    "\n",
    "    \n",
    "def clear_folder(folder):\n",
    "    for the_file in os.listdir(folder):\n",
    "        file_path = os.path.join(folder, the_file)\n",
    "        try:\n",
    "            if os.path.isfile(file_path):\n",
    "                os.unlink(file_path)\n",
    "            #elif os.path.isdir(file_path): shutil.rmtree(file_path)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "        # recreate the folder after deletion\n",
    "        if not os.path.exists(folder):\n",
    "            os.makedirs(folder)\n",
    "            \n",
    "def copyFile(src, dest):\n",
    "    try:\n",
    "        shutil.copy(src, dest)\n",
    "    except shutil.Error as e:\n",
    "        print('Error: %s' % e)\n",
    "    except IOError as e:\n",
    "        print('Error: %s' % e.strerror)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "big_loop_count = 0\n",
    "small_loop_count = 0\n",
    "\n",
    "zip_folder = '/home/ubuntu/s3/comb/zip/'\n",
    "txt_folder = '/home/ubuntu/s3/comb/txt/'\n",
    "txt_exception_folder = '/home/ubuntu/s3/comb/txt_exception/'\n",
    "pq_folder = \"/home/ubuntu/s3/comb/parquet_temp/\"\n",
    "pq_file_name = \"flight_v1_0.pq\"    \n",
    "\n",
    "flight = spark.read.parquet(\"/home/ubuntu/s3/comb/flight_v1_0.pq\")\n",
    "struct_v1_0 = flight.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# clear folders before starting\n",
    "clear_folder(zip_folder)\n",
    "clear_folder(txt_folder)\n",
    "clear_folder(pq_folder)\n",
    "clear_folder(txt_exception_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "big loop: 1\n",
      "big loop: 2\n",
      "/home/ubuntu/s3/comb/parquet_temp/flight_v1_0.pq saved!\n",
      "big loop: 3\n",
      "/home/ubuntu/s3/comb/parquet_temp/flight_v1_0.pq saved!\n",
      "big loop: 4\n"
     ]
    }
   ],
   "source": [
    "for chunk in chunker(all_objects['Contents'], 1):    \n",
    "    \n",
    "    # download zip files\n",
    "    for item in chunk:\n",
    "        s3_client.download_file('flight.price', item['Key'], zip_folder + item['Key'].replace('/', '__'))\n",
    "        \n",
    "        big_loop_count += 1\n",
    "        print(\"big loop: \" + str(big_loop_count))\n",
    "\n",
    "        # extract to txt\n",
    "        unzip_files('/home/ubuntu/s3/comb/zip/', '/home/ubuntu/s3/comb/txt/', '.zip')   \n",
    "\n",
    "        # if necessary move subfolder contents to parent folder        \n",
    "        try:\n",
    "            for filename in listdir(join(txt_folder, 'final_results')):\n",
    "                move(join(txt_folder, 'final_results', filename), join(txt_folder, filename))\n",
    "            rmdir(join(txt_folder, 'final_results'))\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "        # create or append to parquet\n",
    "        txt_to_parquet(txt_folder, pq_folder, pq_file_name)\n",
    "\n",
    "        # clean up folder to save space\n",
    "        clear_folder('/home/ubuntu/s3/comb/zip/')\n",
    "        clear_folder('/home/ubuntu/s3/comb/txt/')    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flight_15_10/flight_15_10_price_2017-04-25.zip\n",
      "flight_15_10/flight_15_10_price_2017-04-26.zip\n",
      "flight_15_10/flight_15_10_price_2017-04-27.zip\n",
      "flight_15_10/flight_15_10_price_2017-04-28.zip\n",
      "flight_15_10/flight_15_10_price_2017-04-29.zip\n",
      "flight_15_10/flight_15_10_price_2017-04-30.zip\n",
      "flight_15_10/flight_15_10_price_2017-05-01.zip\n",
      "flight_15_10/flight_15_10_price_2017-05-02.zip\n",
      "flight_15_10/flight_15_10_price_2017-05-03.zip\n",
      "flight_15_10/flight_15_10_price_2017-05-06.zip\n",
      "flight_15_10/flight_15_10_price_2017-05-07.zip\n",
      "flight_15_10/flight_15_10_price_2017-05-08.zip\n",
      "flight_15_10/flight_15_10_price_2017-05-09.zip\n",
      "flight_15_10/flight_15_10_price_2017-05-10.zip\n",
      "flight_15_10/flight_15_10_price_2017-05-11.zip\n",
      "flight_15_11/flight_15_11_price_2017-04-25.zip\n",
      "flight_15_11/flight_15_11_price_2017-04-26.zip\n",
      "flight_15_11/flight_15_11_price_2017-04-27.zip\n",
      "flight_15_11/flight_15_11_price_2017-04-28.zip\n",
      "flight_15_11/flight_15_11_price_2017-04-29.zip\n",
      "flight_15_11/flight_15_11_price_2017-04-30.zip\n",
      "flight_15_11/flight_15_11_price_2017-05-01.zip\n",
      "flight_15_11/flight_15_11_price_2017-05-02.zip\n",
      "flight_15_11/flight_15_11_price_2017-05-03.zip\n",
      "flight_15_11/flight_15_11_price_2017-05-06.zip\n",
      "flight_15_11/flight_15_11_price_2017-05-07.zip\n",
      "flight_15_11/flight_15_11_price_2017-05-08.zip\n",
      "flight_15_11/flight_15_11_price_2017-05-09.zip\n",
      "flight_15_11/flight_15_11_price_2017-05-10.zip\n",
      "flight_15_11/flight_15_11_price_2017-05-11.zip\n",
      "flight_15_12/flight_15_12_price_2017-04-25.zip\n",
      "flight_15_12/flight_15_12_price_2017-04-26.zip\n",
      "flight_15_12/flight_15_12_price_2017-04-27.zip\n",
      "flight_15_12/flight_15_12_price_2017-04-28.zip\n",
      "flight_15_12/flight_15_12_price_2017-04-29.zip\n",
      "flight_15_12/flight_15_12_price_2017-04-30.zip\n",
      "flight_15_12/flight_15_12_price_2017-05-01.zip\n",
      "flight_15_12/flight_15_12_price_2017-05-02.zip\n",
      "flight_15_12/flight_15_12_price_2017-05-03.zip\n",
      "flight_15_12/flight_15_12_price_2017-05-06.zip\n",
      "flight_15_12/flight_15_12_price_2017-05-07.zip\n",
      "flight_15_12/flight_15_12_price_2017-05-08.zip\n",
      "flight_15_12/flight_15_12_price_2017-05-09.zip\n",
      "flight_15_12/flight_15_12_price_2017-05-10.zip\n",
      "flight_15_12/flight_15_12_price_2017-05-11.zip\n",
      "flight_15_13/flight_15_13_price_2017-04-25.zip\n",
      "flight_15_13/flight_15_13_price_2017-04-26.zip\n",
      "flight_15_13/flight_15_13_price_2017-04-27.zip\n",
      "flight_15_13/flight_15_13_price_2017-04-28.zip\n",
      "flight_15_13/flight_15_13_price_2017-04-29.zip\n",
      "flight_15_13/flight_15_13_price_2017-04-30.zip\n",
      "flight_15_13/flight_15_13_price_2017-05-01.zip\n",
      "flight_15_13/flight_15_13_price_2017-05-02.zip\n",
      "flight_15_13/flight_15_13_price_2017-05-03.zip\n",
      "flight_15_13/flight_15_13_price_2017-05-06.zip\n",
      "flight_15_13/flight_15_13_price_2017-05-07.zip\n",
      "flight_15_13/flight_15_13_price_2017-05-08.zip\n",
      "flight_15_13/flight_15_13_price_2017-05-09.zip\n",
      "flight_15_13/flight_15_13_price_2017-05-10.zip\n",
      "flight_15_13/flight_15_13_price_2017-05-11.zip\n",
      "flight_15_14/flight_15_14_price_2017-04-25.zip\n",
      "flight_15_14/flight_15_14_price_2017-04-26.zip\n",
      "flight_15_14/flight_15_14_price_2017-04-27.zip\n",
      "flight_15_14/flight_15_14_price_2017-04-28.zip\n",
      "flight_15_14/flight_15_14_price_2017-04-29.zip\n",
      "flight_15_14/flight_15_14_price_2017-04-30.zip\n",
      "flight_15_14/flight_15_14_price_2017-05-01.zip\n",
      "flight_15_14/flight_15_14_price_2017-05-02.zip\n",
      "flight_15_14/flight_15_14_price_2017-05-03.zip\n",
      "flight_15_14/flight_15_14_price_2017-05-06.zip\n",
      "flight_15_14/flight_15_14_price_2017-05-07.zip\n",
      "flight_15_14/flight_15_14_price_2017-05-08.zip\n",
      "flight_15_14/flight_15_14_price_2017-05-09.zip\n",
      "flight_15_14/flight_15_14_price_2017-05-10.zip\n",
      "flight_15_14/flight_15_14_price_2017-05-11.zip\n",
      "flight_15_5/flight_15_5_price_2017-04-25.zip\n",
      "flight_15_5/flight_15_5_price_2017-04-26.zip\n",
      "flight_15_5/flight_15_5_price_2017-04-27.zip\n",
      "flight_15_5/flight_15_5_price_2017-04-28.zip\n",
      "flight_15_5/flight_15_5_price_2017-04-29.zip\n",
      "flight_15_5/flight_15_5_price_2017-04-30.zip\n",
      "flight_15_5/flight_15_5_price_2017-05-01.zip\n",
      "flight_15_5/flight_15_5_price_2017-05-02.zip\n",
      "flight_15_5/flight_15_5_price_2017-05-03.zip\n",
      "flight_15_5/flight_15_5_price_2017-05-06.zip\n",
      "flight_15_5/flight_15_5_price_2017-05-07.zip\n",
      "flight_15_5/flight_15_5_price_2017-05-08.zip\n",
      "flight_15_5/flight_15_5_price_2017-05-09.zip\n",
      "flight_15_5/flight_15_5_price_2017-05-10.zip\n",
      "flight_15_5/flight_15_5_price_2017-05-11.zip\n",
      "flight_15_6/flight_15_6_price_2017-04-25.zip\n",
      "flight_15_6/flight_15_6_price_2017-04-26.zip\n",
      "flight_15_6/flight_15_6_price_2017-04-27.zip\n",
      "flight_15_6/flight_15_6_price_2017-04-28.zip\n",
      "flight_15_6/flight_15_6_price_2017-04-29.zip\n",
      "flight_15_6/flight_15_6_price_2017-04-30.zip\n",
      "flight_15_6/flight_15_6_price_2017-05-01.zip\n",
      "flight_15_6/flight_15_6_price_2017-05-02.zip\n",
      "flight_15_6/flight_15_6_price_2017-05-03.zip\n",
      "flight_15_6/flight_15_6_price_2017-05-06.zip\n",
      "flight_15_6/flight_15_6_price_2017-05-07.zip\n",
      "flight_15_6/flight_15_6_price_2017-05-08.zip\n",
      "flight_15_6/flight_15_6_price_2017-05-09.zip\n",
      "flight_15_6/flight_15_6_price_2017-05-10.zip\n",
      "flight_15_6/flight_15_6_price_2017-05-11.zip\n",
      "flight_15_7/flight_15_7_price_2017-04-25.zip\n",
      "flight_15_7/flight_15_7_price_2017-04-26.zip\n",
      "flight_15_7/flight_15_7_price_2017-04-27.zip\n",
      "flight_15_7/flight_15_7_price_2017-04-28.zip\n",
      "flight_15_7/flight_15_7_price_2017-04-29.zip\n",
      "flight_15_7/flight_15_7_price_2017-04-30.zip\n",
      "flight_15_7/flight_15_7_price_2017-05-01.zip\n",
      "flight_15_7/flight_15_7_price_2017-05-02.zip\n",
      "flight_15_7/flight_15_7_price_2017-05-03.zip\n",
      "flight_15_7/flight_15_7_price_2017-05-06.zip\n",
      "flight_15_7/flight_15_7_price_2017-05-07.zip\n",
      "flight_15_7/flight_15_7_price_2017-05-08.zip\n",
      "flight_15_7/flight_15_7_price_2017-05-09.zip\n",
      "flight_15_7/flight_15_7_price_2017-05-10.zip\n",
      "flight_15_7/flight_15_7_price_2017-05-11.zip\n",
      "flight_15_8/flight_15_8_price_2017-04-25.zip\n",
      "flight_15_8/flight_15_8_price_2017-04-26.zip\n",
      "flight_15_8/flight_15_8_price_2017-04-27.zip\n",
      "flight_15_8/flight_15_8_price_2017-04-28.zip\n",
      "flight_15_8/flight_15_8_price_2017-04-29.zip\n",
      "flight_15_8/flight_15_8_price_2017-04-30.zip\n",
      "flight_15_8/flight_15_8_price_2017-05-01.zip\n",
      "flight_15_8/flight_15_8_price_2017-05-02.zip\n",
      "flight_15_8/flight_15_8_price_2017-05-03.zip\n",
      "flight_15_8/flight_15_8_price_2017-05-06.zip\n",
      "flight_15_8/flight_15_8_price_2017-05-07.zip\n",
      "flight_15_8/flight_15_8_price_2017-05-08.zip\n",
      "flight_15_8/flight_15_8_price_2017-05-09.zip\n",
      "flight_15_8/flight_15_8_price_2017-05-10.zip\n",
      "flight_15_8/flight_15_8_price_2017-05-11.zip\n",
      "flight_15_9/flight_15_9_price_2017-04-25.zip\n",
      "flight_15_9/flight_15_9_price_2017-04-26.zip\n",
      "flight_15_9/flight_15_9_price_2017-04-27.zip\n",
      "flight_15_9/flight_15_9_price_2017-04-28.zip\n",
      "flight_15_9/flight_15_9_price_2017-04-29.zip\n",
      "flight_15_9/flight_15_9_price_2017-04-30.zip\n",
      "flight_15_9/flight_15_9_price_2017-05-01.zip\n",
      "flight_15_9/flight_15_9_price_2017-05-02.zip\n",
      "flight_15_9/flight_15_9_price_2017-05-03.zip\n",
      "flight_15_9/flight_15_9_price_2017-05-06.zip\n",
      "flight_15_9/flight_15_9_price_2017-05-07.zip\n",
      "flight_15_9/flight_15_9_price_2017-05-08.zip\n",
      "flight_15_9/flight_15_9_price_2017-05-09.zip\n",
      "flight_15_9/flight_15_9_price_2017-05-10.zip\n",
      "flight_15_9/flight_15_9_price_2017-05-11.zip\n",
      "flight_16_10/flight_16_10_price_2017-04-25.zip\n",
      "flight_16_10/flight_16_10_price_2017-04-26.zip\n",
      "flight_16_10/flight_16_10_price_2017-04-27.zip\n",
      "flight_16_10/flight_16_10_price_2017-04-28.zip\n",
      "flight_16_10/flight_16_10_price_2017-04-29.zip\n",
      "flight_16_10/flight_16_10_price_2017-04-30.zip\n",
      "flight_16_10/flight_16_10_price_2017-05-01.zip\n",
      "flight_16_10/flight_16_10_price_2017-05-02.zip\n",
      "flight_16_10/flight_16_10_price_2017-05-06.zip\n",
      "flight_16_10/flight_16_10_price_2017-05-07.zip\n",
      "flight_16_10/flight_16_10_price_2017-05-08.zip\n",
      "flight_16_10/flight_16_10_price_2017-05-09.zip\n",
      "flight_16_10/flight_16_10_price_2017-05-10.zip\n",
      "flight_16_10/flight_16_10_price_2017-05-11.zip\n",
      "flight_16_11/flight_16_11_price_2017-04-25.zip\n",
      "flight_16_11/flight_16_11_price_2017-04-26.zip\n",
      "flight_16_11/flight_16_11_price_2017-04-27.zip\n",
      "flight_16_11/flight_16_11_price_2017-04-28.zip\n",
      "flight_16_11/flight_16_11_price_2017-04-29.zip\n",
      "flight_16_11/flight_16_11_price_2017-04-30.zip\n",
      "flight_16_11/flight_16_11_price_2017-05-01.zip\n",
      "flight_16_11/flight_16_11_price_2017-05-02.zip\n",
      "flight_16_11/flight_16_11_price_2017-05-06.zip\n",
      "flight_16_11/flight_16_11_price_2017-05-07.zip\n",
      "flight_16_11/flight_16_11_price_2017-05-08.zip\n",
      "flight_16_11/flight_16_11_price_2017-05-09.zip\n",
      "flight_16_11/flight_16_11_price_2017-05-10.zip\n",
      "flight_16_11/flight_16_11_price_2017-05-11.zip\n",
      "flight_16_12/flight_16_12_price_2017-04-25.zip\n",
      "flight_16_12/flight_16_12_price_2017-04-26.zip\n",
      "flight_16_12/flight_16_12_price_2017-04-27.zip\n",
      "flight_16_12/flight_16_12_price_2017-04-28.zip\n",
      "flight_16_12/flight_16_12_price_2017-04-29.zip\n",
      "flight_16_12/flight_16_12_price_2017-04-30.zip\n",
      "flight_16_12/flight_16_12_price_2017-05-01.zip\n",
      "flight_16_12/flight_16_12_price_2017-05-02.zip\n",
      "flight_16_12/flight_16_12_price_2017-05-06.zip\n",
      "flight_16_12/flight_16_12_price_2017-05-07.zip\n",
      "flight_16_12/flight_16_12_price_2017-05-08.zip\n",
      "flight_16_12/flight_16_12_price_2017-05-09.zip\n",
      "flight_16_12/flight_16_12_price_2017-05-10.zip\n",
      "flight_16_12/flight_16_12_price_2017-05-11.zip\n",
      "flight_16_13/flight_16_13_price_2017-04-25.zip\n",
      "flight_16_13/flight_16_13_price_2017-04-26.zip\n",
      "flight_16_13/flight_16_13_price_2017-04-27.zip\n",
      "flight_16_13/flight_16_13_price_2017-04-28.zip\n",
      "flight_16_13/flight_16_13_price_2017-04-29.zip\n",
      "flight_16_13/flight_16_13_price_2017-04-30.zip\n",
      "flight_16_13/flight_16_13_price_2017-05-01.zip\n",
      "flight_16_13/flight_16_13_price_2017-05-06.zip\n",
      "flight_16_13/flight_16_13_price_2017-05-07.zip\n",
      "flight_16_13/flight_16_13_price_2017-05-08.zip\n",
      "flight_16_13/flight_16_13_price_2017-05-09.zip\n",
      "flight_16_13/flight_16_13_price_2017-05-10.zip\n",
      "flight_16_13/flight_16_13_price_2017-05-11.zip\n",
      "flight_16_14/flight_16_14_price_2017-04-26.zip\n",
      "flight_16_14/flight_16_14_price_2017-04-27.zip\n",
      "flight_16_14/flight_16_14_price_2017-04-28.zip\n",
      "flight_16_14/flight_16_14_price_2017-04-29.zip\n",
      "flight_16_14/flight_16_14_price_2017-04-30.zip\n",
      "flight_16_14/flight_16_14_price_2017-05-01.zip\n",
      "flight_16_14/flight_16_14_price_2017-05-06.zip\n",
      "flight_16_14/flight_16_14_price_2017-05-07.zip\n",
      "flight_16_14/flight_16_14_price_2017-05-08.zip\n",
      "flight_16_14/flight_16_14_price_2017-05-09.zip\n",
      "flight_16_5/flight_16_5_price_2017-04-25.zip\n",
      "flight_16_5/flight_16_5_price_2017-04-26.zip\n",
      "flight_16_5/flight_16_5_price_2017-04-27.zip\n",
      "flight_16_5/flight_16_5_price_2017-04-28.zip\n",
      "flight_16_5/flight_16_5_price_2017-04-29.zip\n",
      "flight_16_5/flight_16_5_price_2017-04-30.zip\n",
      "flight_16_5/flight_16_5_price_2017-05-01.zip\n",
      "flight_16_5/flight_16_5_price_2017-05-02.zip\n",
      "flight_16_5/flight_16_5_price_2017-05-03.zip\n",
      "flight_16_5/flight_16_5_price_2017-05-06.zip\n",
      "flight_16_5/flight_16_5_price_2017-05-07.zip\n",
      "flight_16_5/flight_16_5_price_2017-05-08.zip\n",
      "flight_16_5/flight_16_5_price_2017-05-09.zip\n",
      "flight_16_5/flight_16_5_price_2017-05-10.zip\n",
      "flight_16_5/flight_16_5_price_2017-05-11.zip\n",
      "flight_16_6/flight_16_6_price_2017-04-25.zip\n",
      "flight_16_6/flight_16_6_price_2017-04-26.zip\n",
      "flight_16_6/flight_16_6_price_2017-04-27.zip\n",
      "flight_16_6/flight_16_6_price_2017-04-28.zip\n",
      "flight_16_6/flight_16_6_price_2017-04-29.zip\n",
      "flight_16_6/flight_16_6_price_2017-04-30.zip\n",
      "flight_16_6/flight_16_6_price_2017-05-01.zip\n",
      "flight_16_6/flight_16_6_price_2017-05-02.zip\n",
      "flight_16_6/flight_16_6_price_2017-05-03.zip\n",
      "flight_16_6/flight_16_6_price_2017-05-06.zip\n",
      "flight_16_6/flight_16_6_price_2017-05-07.zip\n",
      "flight_16_6/flight_16_6_price_2017-05-08.zip\n",
      "flight_16_6/flight_16_6_price_2017-05-09.zip\n",
      "flight_16_6/flight_16_6_price_2017-05-10.zip\n",
      "flight_16_6/flight_16_6_price_2017-05-11.zip\n",
      "flight_16_7/flight_16_7_price_2017-04-25.zip\n",
      "flight_16_7/flight_16_7_price_2017-04-26.zip\n",
      "flight_16_7/flight_16_7_price_2017-04-27.zip\n",
      "flight_16_7/flight_16_7_price_2017-04-28.zip\n",
      "flight_16_7/flight_16_7_price_2017-04-29.zip\n",
      "flight_16_7/flight_16_7_price_2017-04-30.zip\n",
      "flight_16_7/flight_16_7_price_2017-05-01.zip\n",
      "flight_16_7/flight_16_7_price_2017-05-02.zip\n",
      "flight_16_7/flight_16_7_price_2017-05-03.zip\n",
      "flight_16_7/flight_16_7_price_2017-05-06.zip\n",
      "flight_16_7/flight_16_7_price_2017-05-07.zip\n",
      "flight_16_7/flight_16_7_price_2017-05-08.zip\n",
      "flight_16_7/flight_16_7_price_2017-05-09.zip\n",
      "flight_16_7/flight_16_7_price_2017-05-10.zip\n",
      "flight_16_7/flight_16_7_price_2017-05-11.zip\n",
      "flight_16_8/flight_16_8_price_2017-04-25.zip\n",
      "flight_16_8/flight_16_8_price_2017-04-26.zip\n",
      "flight_16_8/flight_16_8_price_2017-04-27.zip\n",
      "flight_16_8/flight_16_8_price_2017-04-28.zip\n",
      "flight_16_8/flight_16_8_price_2017-04-29.zip\n",
      "flight_16_8/flight_16_8_price_2017-04-30.zip\n",
      "flight_16_8/flight_16_8_price_2017-05-01.zip\n",
      "flight_16_8/flight_16_8_price_2017-05-02.zip\n",
      "flight_16_8/flight_16_8_price_2017-05-06.zip\n",
      "flight_16_8/flight_16_8_price_2017-05-07.zip\n",
      "flight_16_8/flight_16_8_price_2017-05-08.zip\n",
      "flight_16_8/flight_16_8_price_2017-05-09.zip\n",
      "flight_16_8/flight_16_8_price_2017-05-10.zip\n",
      "flight_16_8/flight_16_8_price_2017-05-11.zip\n",
      "flight_16_9/flight_16_9_price_2017-04-25.zip\n",
      "flight_16_9/flight_16_9_price_2017-04-26.zip\n",
      "flight_16_9/flight_16_9_price_2017-04-27.zip\n",
      "flight_16_9/flight_16_9_price_2017-04-28.zip\n",
      "flight_16_9/flight_16_9_price_2017-04-29.zip\n",
      "flight_16_9/flight_16_9_price_2017-04-30.zip\n",
      "flight_16_9/flight_16_9_price_2017-05-01.zip\n",
      "flight_16_9/flight_16_9_price_2017-05-02.zip\n",
      "flight_16_9/flight_16_9_price_2017-05-06.zip\n",
      "flight_16_9/flight_16_9_price_2017-05-07.zip\n",
      "flight_16_9/flight_16_9_price_2017-05-08.zip\n",
      "flight_16_9/flight_16_9_price_2017-05-09.zip\n",
      "flight_16_9/flight_16_9_price_2017-05-10.zip\n",
      "flight_16_9/flight_16_9_price_2017-05-11.zip\n",
      "flight_1_10/flight_1_10_price_2017-04-23.zip\n",
      "flight_1_10/flight_1_10_price_2017-04-24.zip\n",
      "flight_1_10/flight_1_10_price_2017-04-25.zip\n",
      "flight_1_10/flight_1_10_price_2017-04-26.zip\n",
      "flight_1_10/flight_1_10_price_2017-04-27.zip\n",
      "flight_1_10/flight_1_10_price_2017-04-28.zip\n",
      "flight_1_10/flight_1_10_price_2017-04-29.zip\n",
      "flight_1_10/flight_1_10_price_2017-04-30.zip\n",
      "flight_1_10/flight_1_10_price_2017-05-01.zip\n",
      "flight_1_10/flight_1_10_price_2017-05-02.zip\n",
      "flight_1_10/flight_1_10_price_2017-05-03.zip\n",
      "flight_1_10/flight_1_10_price_2017-05-06.zip\n",
      "flight_1_10/flight_1_10_price_2017-05-07.zip\n",
      "flight_1_10/flight_1_10_price_2017-05-08.zip\n",
      "flight_1_10/flight_1_10_price_2017-05-09.zip\n",
      "flight_1_10/flight_1_10_price_2017-05-10.zip\n",
      "flight_1_10/flight_1_10_price_2017-05-11.zip\n",
      "flight_1_11/flight_1_11_price_2017-04-23.zip\n",
      "flight_1_11/flight_1_11_price_2017-04-24.zip\n",
      "flight_1_11/flight_1_11_price_2017-04-25.zip\n",
      "flight_1_11/flight_1_11_price_2017-04-26.zip\n",
      "flight_1_11/flight_1_11_price_2017-04-27.zip\n",
      "flight_1_11/flight_1_11_price_2017-04-28.zip\n",
      "flight_1_11/flight_1_11_price_2017-04-29.zip\n",
      "flight_1_11/flight_1_11_price_2017-04-30.zip\n",
      "flight_1_11/flight_1_11_price_2017-05-01.zip\n",
      "flight_1_11/flight_1_11_price_2017-05-02.zip\n",
      "flight_1_11/flight_1_11_price_2017-05-03.zip\n",
      "flight_1_11/flight_1_11_price_2017-05-06.zip\n",
      "flight_1_11/flight_1_11_price_2017-05-07.zip\n",
      "flight_1_11/flight_1_11_price_2017-05-08.zip\n",
      "flight_1_11/flight_1_11_price_2017-05-09.zip\n",
      "flight_1_11/flight_1_11_price_2017-05-10.zip\n",
      "flight_1_11/flight_1_11_price_2017-05-11.zip\n",
      "flight_1_12/flight_1_12_price_2017-04-23.zip\n",
      "flight_1_12/flight_1_12_price_2017-04-24.zip\n",
      "flight_1_12/flight_1_12_price_2017-04-25.zip\n",
      "flight_1_12/flight_1_12_price_2017-04-26.zip\n",
      "flight_1_12/flight_1_12_price_2017-04-27.zip\n",
      "flight_1_12/flight_1_12_price_2017-04-28.zip\n",
      "flight_1_12/flight_1_12_price_2017-04-29.zip\n",
      "flight_1_12/flight_1_12_price_2017-04-30.zip\n",
      "flight_1_12/flight_1_12_price_2017-05-01.zip\n",
      "flight_1_12/flight_1_12_price_2017-05-02.zip\n",
      "flight_1_12/flight_1_12_price_2017-05-03.zip\n",
      "flight_1_12/flight_1_12_price_2017-05-06.zip\n",
      "flight_1_12/flight_1_12_price_2017-05-07.zip\n",
      "flight_1_12/flight_1_12_price_2017-05-08.zip\n",
      "flight_1_12/flight_1_12_price_2017-05-09.zip\n",
      "flight_1_12/flight_1_12_price_2017-05-10.zip\n",
      "flight_1_12/flight_1_12_price_2017-05-11.zip\n",
      "flight_1_13/flight_1_13_price_2017-04-23.zip\n",
      "flight_1_13/flight_1_13_price_2017-04-24.zip\n",
      "flight_1_13/flight_1_13_price_2017-04-25.zip\n",
      "flight_1_13/flight_1_13_price_2017-04-26.zip\n",
      "flight_1_13/flight_1_13_price_2017-04-27.zip\n",
      "flight_1_13/flight_1_13_price_2017-04-28.zip\n",
      "flight_1_13/flight_1_13_price_2017-04-29.zip\n",
      "flight_1_13/flight_1_13_price_2017-04-30.zip\n",
      "flight_1_13/flight_1_13_price_2017-05-01.zip\n",
      "flight_1_13/flight_1_13_price_2017-05-02.zip\n",
      "flight_1_13/flight_1_13_price_2017-05-03.zip\n",
      "flight_1_13/flight_1_13_price_2017-05-06.zip\n",
      "flight_1_13/flight_1_13_price_2017-05-07.zip\n",
      "flight_1_13/flight_1_13_price_2017-05-08.zip\n",
      "flight_1_13/flight_1_13_price_2017-05-09.zip\n",
      "flight_1_13/flight_1_13_price_2017-05-10.zip\n",
      "flight_1_13/flight_1_13_price_2017-05-11.zip\n",
      "flight_1_14/flight_1_14_price_2017-04-23.zip\n",
      "flight_1_14/flight_1_14_price_2017-04-24.zip\n",
      "flight_1_14/flight_1_14_price_2017-04-25.zip\n",
      "flight_1_14/flight_1_14_price_2017-04-26.zip\n",
      "flight_1_14/flight_1_14_price_2017-04-27.zip\n",
      "flight_1_14/flight_1_14_price_2017-04-28.zip\n",
      "flight_1_14/flight_1_14_price_2017-04-29.zip\n",
      "flight_1_14/flight_1_14_price_2017-04-30.zip\n",
      "flight_1_14/flight_1_14_price_2017-05-01.zip\n",
      "flight_1_14/flight_1_14_price_2017-05-02.zip\n",
      "flight_1_14/flight_1_14_price_2017-05-03.zip\n",
      "flight_1_14/flight_1_14_price_2017-05-06.zip\n",
      "flight_1_14/flight_1_14_price_2017-05-07.zip\n",
      "flight_1_14/flight_1_14_price_2017-05-08.zip\n",
      "flight_1_14/flight_1_14_price_2017-05-09.zip\n",
      "flight_1_14/flight_1_14_price_2017-05-10.zip\n",
      "flight_1_14/flight_1_14_price_2017-05-11.zip\n",
      "flight_1_5/flight_1_5_price_2017-04-04.zip\n",
      "flight_1_5/flight_1_5_price_2017-04-06.zip\n",
      "flight_1_5/flight_1_5_price_2017-04-09.zip\n",
      "flight_1_5/flight_1_5_price_2017-04-12.zip\n",
      "flight_1_5/flight_1_5_price_2017-04-13.zip\n",
      "flight_1_5/flight_1_5_price_2017-04-14.zip\n",
      "flight_1_5/flight_1_5_price_2017-04-15.zip\n",
      "flight_1_5/flight_1_5_price_2017-04-16.zip\n",
      "flight_1_5/flight_1_5_price_2017-04-18.zip\n",
      "flight_1_5/flight_1_5_price_2017-04-19.zip\n",
      "flight_1_5/flight_1_5_price_2017-04-20.zip\n",
      "flight_1_5/flight_1_5_price_2017-04-21.zip\n",
      "flight_1_5/flight_1_5_price_2017-04-22.zip\n",
      "flight_1_5/flight_1_5_price_2017-04-23.zip\n",
      "flight_1_5/flight_1_5_price_2017-04-24.zip\n",
      "flight_1_5/flight_1_5_price_2017-04-25.zip\n",
      "flight_1_5/flight_1_5_price_2017-04-26.zip\n",
      "flight_1_5/flight_1_5_price_2017-04-27.zip\n",
      "flight_1_5/flight_1_5_price_2017-04-28.zip\n",
      "flight_1_5/flight_1_5_price_2017-04-29.zip\n",
      "flight_1_5/flight_1_5_price_2017-04-30.zip\n",
      "flight_1_5/flight_1_5_price_2017-05-01.zip\n",
      "flight_1_5/flight_1_5_price_2017-05-02.zip\n",
      "flight_1_5/flight_1_5_price_2017-05-03.zip\n",
      "flight_1_5/flight_1_5_price_2017-05-06.zip\n",
      "flight_1_5/flight_1_5_price_2017-05-07.zip\n",
      "flight_1_5/flight_1_5_price_2017-05-08.zip\n",
      "flight_1_5/flight_1_5_price_2017-05-09.zip\n",
      "flight_1_5/flight_1_5_price_2017-05-10.zip\n",
      "flight_1_5/flight_1_5_price_2017-05-11.zip\n",
      "flight_1_6/flight_1_6_price_2017-04-04.zip\n",
      "flight_1_6/flight_1_6_price_2017-04-06.zip\n",
      "flight_1_6/flight_1_6_price_2017-04-09.zip\n",
      "flight_1_6/flight_1_6_price_2017-04-12.zip\n",
      "flight_1_6/flight_1_6_price_2017-04-13.zip\n",
      "flight_1_6/flight_1_6_price_2017-04-15.zip\n",
      "flight_1_6/flight_1_6_price_2017-04-16.zip\n",
      "flight_1_6/flight_1_6_price_2017-04-18.zip\n",
      "flight_1_6/flight_1_6_price_2017-04-19.zip\n",
      "flight_1_6/flight_1_6_price_2017-04-20.zip\n",
      "flight_1_6/flight_1_6_price_2017-04-21.zip\n",
      "flight_1_6/flight_1_6_price_2017-04-22.zip\n",
      "flight_1_6/flight_1_6_price_2017-04-23.zip\n",
      "flight_1_6/flight_1_6_price_2017-04-24.zip\n",
      "flight_1_6/flight_1_6_price_2017-04-25.zip\n",
      "flight_1_6/flight_1_6_price_2017-04-26.zip\n",
      "flight_1_6/flight_1_6_price_2017-04-27.zip\n",
      "flight_1_6/flight_1_6_price_2017-04-28.zip\n",
      "flight_1_6/flight_1_6_price_2017-04-29.zip\n",
      "flight_1_6/flight_1_6_price_2017-04-30.zip\n",
      "flight_1_6/flight_1_6_price_2017-05-01.zip\n",
      "flight_1_6/flight_1_6_price_2017-05-02.zip\n",
      "flight_1_6/flight_1_6_price_2017-05-03.zip\n",
      "flight_1_6/flight_1_6_price_2017-05-06.zip\n",
      "flight_1_6/flight_1_6_price_2017-05-07.zip\n",
      "flight_1_6/flight_1_6_price_2017-05-08.zip\n",
      "flight_1_6/flight_1_6_price_2017-05-09.zip\n",
      "flight_1_6/flight_1_6_price_2017-05-10.zip\n",
      "flight_1_6/flight_1_6_price_2017-05-11.zip\n",
      "flight_1_7/flight_1_7_price_2017-04-04.zip\n",
      "flight_1_7/flight_1_7_price_2017-04-06.zip\n",
      "flight_1_7/flight_1_7_price_2017-04-09.zip\n",
      "flight_1_7/flight_1_7_price_2017-04-13.zip\n",
      "flight_1_7/flight_1_7_price_2017-04-15.zip\n",
      "flight_1_7/flight_1_7_price_2017-04-16.zip\n",
      "flight_1_7/flight_1_7_price_2017-04-19.zip\n",
      "flight_1_7/flight_1_7_price_2017-04-20.zip\n",
      "flight_1_7/flight_1_7_price_2017-04-21.zip\n",
      "flight_1_7/flight_1_7_price_2017-04-22.zip\n",
      "flight_1_7/flight_1_7_price_2017-04-23.zip\n",
      "flight_1_7/flight_1_7_price_2017-04-24.zip\n",
      "flight_1_7/flight_1_7_price_2017-04-25.zip\n",
      "flight_1_7/flight_1_7_price_2017-04-26.zip\n",
      "flight_1_7/flight_1_7_price_2017-04-27.zip\n",
      "flight_1_7/flight_1_7_price_2017-04-28.zip\n",
      "flight_1_7/flight_1_7_price_2017-04-29.zip\n",
      "flight_1_7/flight_1_7_price_2017-04-30.zip\n",
      "flight_1_7/flight_1_7_price_2017-05-01.zip\n",
      "flight_1_7/flight_1_7_price_2017-05-02.zip\n",
      "flight_1_7/flight_1_7_price_2017-05-03.zip\n",
      "flight_1_7/flight_1_7_price_2017-05-06.zip\n",
      "flight_1_7/flight_1_7_price_2017-05-07.zip\n",
      "flight_1_7/flight_1_7_price_2017-05-08.zip\n",
      "flight_1_7/flight_1_7_price_2017-05-09.zip\n",
      "flight_1_7/flight_1_7_price_2017-05-10.zip\n",
      "flight_1_7/flight_1_7_price_2017-05-11.zip\n",
      "flight_1_8/flight_1_8_price_2017-04-04.zip\n",
      "flight_1_8/flight_1_8_price_2017-04-06.zip\n",
      "flight_1_8/flight_1_8_price_2017-04-13.zip\n",
      "flight_1_8/flight_1_8_price_2017-04-15.zip\n",
      "flight_1_8/flight_1_8_price_2017-04-16.zip\n",
      "flight_1_8/flight_1_8_price_2017-04-19.zip\n",
      "flight_1_8/flight_1_8_price_2017-04-20.zip\n",
      "flight_1_8/flight_1_8_price_2017-04-21.zip\n",
      "flight_1_8/flight_1_8_price_2017-04-22.zip\n",
      "flight_1_8/flight_1_8_price_2017-04-23.zip\n",
      "flight_1_8/flight_1_8_price_2017-04-24.zip\n",
      "flight_1_8/flight_1_8_price_2017-04-25.zip\n",
      "flight_1_8/flight_1_8_price_2017-04-26.zip\n",
      "flight_1_8/flight_1_8_price_2017-04-27.zip\n",
      "flight_1_8/flight_1_8_price_2017-04-28.zip\n",
      "flight_1_8/flight_1_8_price_2017-04-29.zip\n",
      "flight_1_8/flight_1_8_price_2017-04-30.zip\n",
      "flight_1_8/flight_1_8_price_2017-05-01.zip\n",
      "flight_1_8/flight_1_8_price_2017-05-02.zip\n",
      "flight_1_8/flight_1_8_price_2017-05-03.zip\n",
      "flight_1_8/flight_1_8_price_2017-05-06.zip\n",
      "flight_1_8/flight_1_8_price_2017-05-07.zip\n",
      "flight_1_8/flight_1_8_price_2017-05-08.zip\n",
      "flight_1_8/flight_1_8_price_2017-05-09.zip\n",
      "flight_1_8/flight_1_8_price_2017-05-10.zip\n",
      "flight_1_8/flight_1_8_price_2017-05-11.zip\n",
      "flight_1_9/flight_1_9_price_2017-04-23.zip\n",
      "flight_1_9/flight_1_9_price_2017-04-24.zip\n",
      "flight_1_9/flight_1_9_price_2017-04-25.zip\n",
      "flight_1_9/flight_1_9_price_2017-04-26.zip\n",
      "flight_1_9/flight_1_9_price_2017-04-27.zip\n",
      "flight_1_9/flight_1_9_price_2017-04-28.zip\n",
      "flight_1_9/flight_1_9_price_2017-04-29.zip\n",
      "flight_1_9/flight_1_9_price_2017-04-30.zip\n",
      "flight_1_9/flight_1_9_price_2017-05-01.zip\n",
      "flight_1_9/flight_1_9_price_2017-05-02.zip\n",
      "flight_1_9/flight_1_9_price_2017-05-03.zip\n",
      "flight_1_9/flight_1_9_price_2017-05-06.zip\n",
      "flight_1_9/flight_1_9_price_2017-05-07.zip\n",
      "flight_1_9/flight_1_9_price_2017-05-08.zip\n",
      "flight_1_9/flight_1_9_price_2017-05-09.zip\n",
      "flight_1_9/flight_1_9_price_2017-05-10.zip\n",
      "flight_1_9/flight_1_9_price_2017-05-11.zip\n",
      "flight_2_10/flight_2_10_price_2017-04-23.zip\n",
      "flight_2_10/flight_2_10_price_2017-04-24.zip\n",
      "flight_2_10/flight_2_10_price_2017-04-25.zip\n",
      "flight_2_10/flight_2_10_price_2017-04-26.zip\n",
      "flight_2_10/flight_2_10_price_2017-04-27.zip\n",
      "flight_2_10/flight_2_10_price_2017-04-28.zip\n",
      "flight_2_10/flight_2_10_price_2017-04-29.zip\n",
      "flight_2_10/flight_2_10_price_2017-04-30.zip\n",
      "flight_2_10/flight_2_10_price_2017-05-01.zip\n",
      "flight_2_10/flight_2_10_price_2017-05-02.zip\n",
      "flight_2_10/flight_2_10_price_2017-05-03.zip\n",
      "flight_2_10/flight_2_10_price_2017-05-06.zip\n",
      "flight_2_10/flight_2_10_price_2017-05-07.zip\n",
      "flight_2_10/flight_2_10_price_2017-05-08.zip\n",
      "flight_2_10/flight_2_10_price_2017-05-09.zip\n",
      "flight_2_10/flight_2_10_price_2017-05-10.zip\n",
      "flight_2_10/flight_2_10_price_2017-05-11.zip\n",
      "flight_2_11/flight_2_11_price_2017-04-23.zip\n",
      "flight_2_11/flight_2_11_price_2017-04-24.zip\n",
      "flight_2_11/flight_2_11_price_2017-04-25.zip\n",
      "flight_2_11/flight_2_11_price_2017-04-26.zip\n",
      "flight_2_11/flight_2_11_price_2017-04-27.zip\n",
      "flight_2_11/flight_2_11_price_2017-04-28.zip\n",
      "flight_2_11/flight_2_11_price_2017-04-29.zip\n",
      "flight_2_11/flight_2_11_price_2017-04-30.zip\n",
      "flight_2_11/flight_2_11_price_2017-05-01.zip\n",
      "flight_2_11/flight_2_11_price_2017-05-02.zip\n",
      "flight_2_11/flight_2_11_price_2017-05-03.zip\n",
      "flight_2_11/flight_2_11_price_2017-05-06.zip\n",
      "flight_2_11/flight_2_11_price_2017-05-07.zip\n",
      "flight_2_11/flight_2_11_price_2017-05-08.zip\n",
      "flight_2_11/flight_2_11_price_2017-05-09.zip\n",
      "flight_2_11/flight_2_11_price_2017-05-10.zip\n",
      "flight_2_11/flight_2_11_price_2017-05-11.zip\n",
      "flight_2_12/flight_2_12_price_2017-04-23.zip\n",
      "flight_2_12/flight_2_12_price_2017-04-24.zip\n",
      "flight_2_12/flight_2_12_price_2017-04-25.zip\n",
      "flight_2_12/flight_2_12_price_2017-04-26.zip\n",
      "flight_2_12/flight_2_12_price_2017-04-27.zip\n",
      "flight_2_12/flight_2_12_price_2017-04-28.zip\n",
      "flight_2_12/flight_2_12_price_2017-04-29.zip\n",
      "flight_2_12/flight_2_12_price_2017-04-30.zip\n",
      "flight_2_12/flight_2_12_price_2017-05-01.zip\n",
      "flight_2_12/flight_2_12_price_2017-05-02.zip\n",
      "flight_2_12/flight_2_12_price_2017-05-03.zip\n",
      "flight_2_12/flight_2_12_price_2017-05-06.zip\n",
      "flight_2_12/flight_2_12_price_2017-05-07.zip\n",
      "flight_2_12/flight_2_12_price_2017-05-08.zip\n",
      "flight_2_12/flight_2_12_price_2017-05-09.zip\n",
      "flight_2_12/flight_2_12_price_2017-05-10.zip\n",
      "flight_2_12/flight_2_12_price_2017-05-11.zip\n",
      "flight_2_13/flight_2_13_price_2017-04-23.zip\n",
      "flight_2_13/flight_2_13_price_2017-04-24.zip\n",
      "flight_2_13/flight_2_13_price_2017-04-25.zip\n",
      "flight_2_13/flight_2_13_price_2017-04-26.zip\n",
      "flight_2_13/flight_2_13_price_2017-04-27.zip\n",
      "flight_2_13/flight_2_13_price_2017-04-28.zip\n",
      "flight_2_13/flight_2_13_price_2017-04-29.zip\n",
      "flight_2_13/flight_2_13_price_2017-04-30.zip\n",
      "flight_2_13/flight_2_13_price_2017-05-01.zip\n",
      "flight_2_13/flight_2_13_price_2017-05-02.zip\n",
      "flight_2_13/flight_2_13_price_2017-05-03.zip\n",
      "flight_2_13/flight_2_13_price_2017-05-06.zip\n",
      "flight_2_13/flight_2_13_price_2017-05-07.zip\n",
      "flight_2_13/flight_2_13_price_2017-05-08.zip\n",
      "flight_2_13/flight_2_13_price_2017-05-09.zip\n",
      "flight_2_13/flight_2_13_price_2017-05-10.zip\n",
      "flight_2_13/flight_2_13_price_2017-05-11.zip\n",
      "flight_2_14/flight_2_14_price_2017-04-23.zip\n",
      "flight_2_14/flight_2_14_price_2017-04-24.zip\n",
      "flight_2_14/flight_2_14_price_2017-04-25.zip\n",
      "flight_2_14/flight_2_14_price_2017-04-26.zip\n",
      "flight_2_14/flight_2_14_price_2017-04-27.zip\n",
      "flight_2_14/flight_2_14_price_2017-04-28.zip\n",
      "flight_2_14/flight_2_14_price_2017-04-29.zip\n",
      "flight_2_14/flight_2_14_price_2017-04-30.zip\n",
      "flight_2_14/flight_2_14_price_2017-05-01.zip\n",
      "flight_2_14/flight_2_14_price_2017-05-02.zip\n",
      "flight_2_14/flight_2_14_price_2017-05-06.zip\n",
      "flight_2_14/flight_2_14_price_2017-05-07.zip\n",
      "flight_2_14/flight_2_14_price_2017-05-08.zip\n",
      "flight_2_14/flight_2_14_price_2017-05-09.zip\n",
      "flight_2_14/flight_2_14_price_2017-05-10.zip\n",
      "flight_2_14/flight_2_14_price_2017-05-11.zip\n",
      "flight_2_5/flight_2_5_price_2017-04-13.zip\n",
      "flight_2_5/flight_2_5_price_2017-04-14.zip\n",
      "flight_2_5/flight_2_5_price_2017-04-15.zip\n",
      "flight_2_5/flight_2_5_price_2017-04-16.zip\n",
      "flight_2_5/flight_2_5_price_2017-04-19.zip\n",
      "flight_2_5/flight_2_5_price_2017-04-20.zip\n",
      "flight_2_5/flight_2_5_price_2017-04-21.zip\n",
      "flight_2_5/flight_2_5_price_2017-04-22.zip\n",
      "flight_2_5/flight_2_5_price_2017-04-23.zip\n",
      "flight_2_5/flight_2_5_price_2017-04-24.zip\n",
      "flight_2_5/flight_2_5_price_2017-04-25.zip\n",
      "flight_2_5/flight_2_5_price_2017-04-26.zip\n",
      "flight_2_5/flight_2_5_price_2017-04-27.zip\n",
      "flight_2_5/flight_2_5_price_2017-04-28.zip\n",
      "flight_2_5/flight_2_5_price_2017-04-29.zip\n",
      "flight_2_5/flight_2_5_price_2017-04-30.zip\n",
      "flight_2_5/flight_2_5_price_2017-05-01.zip\n",
      "flight_2_5/flight_2_5_price_2017-05-02.zip\n",
      "flight_2_5/flight_2_5_price_2017-05-03.zip\n",
      "flight_2_5/flight_2_5_price_2017-05-06.zip\n",
      "flight_2_5/flight_2_5_price_2017-05-07.zip\n",
      "flight_2_5/flight_2_5_price_2017-05-08.zip\n",
      "flight_2_5/flight_2_5_price_2017-05-09.zip\n",
      "flight_2_5/flight_2_5_price_2017-05-10.zip\n",
      "flight_2_5/flight_2_5_price_2017-05-11.zip\n",
      "flight_2_6/flight_2_6_price_2017-04-04.zip\n",
      "flight_2_6/flight_2_6_price_2017-04-06.zip\n",
      "flight_2_6/flight_2_6_price_2017-04-09.zip\n",
      "flight_2_6/flight_2_6_price_2017-04-13.zip\n",
      "flight_2_6/flight_2_6_price_2017-04-15.zip\n",
      "flight_2_6/flight_2_6_price_2017-04-16.zip\n",
      "flight_2_6/flight_2_6_price_2017-04-19.zip\n",
      "flight_2_6/flight_2_6_price_2017-04-20.zip\n",
      "flight_2_6/flight_2_6_price_2017-04-21.zip\n",
      "flight_2_6/flight_2_6_price_2017-04-22.zip\n",
      "flight_2_6/flight_2_6_price_2017-04-23.zip\n",
      "flight_2_6/flight_2_6_price_2017-04-24.zip\n",
      "flight_2_6/flight_2_6_price_2017-04-25.zip\n",
      "flight_2_6/flight_2_6_price_2017-04-26.zip\n",
      "flight_2_6/flight_2_6_price_2017-04-27.zip\n",
      "flight_2_6/flight_2_6_price_2017-04-28.zip\n",
      "flight_2_6/flight_2_6_price_2017-04-29.zip\n",
      "flight_2_6/flight_2_6_price_2017-04-30.zip\n",
      "flight_2_6/flight_2_6_price_2017-05-01.zip\n",
      "flight_2_6/flight_2_6_price_2017-05-02.zip\n",
      "flight_2_6/flight_2_6_price_2017-05-03.zip\n",
      "flight_2_6/flight_2_6_price_2017-05-06.zip\n",
      "flight_2_6/flight_2_6_price_2017-05-07.zip\n",
      "flight_2_6/flight_2_6_price_2017-05-08.zip\n",
      "flight_2_6/flight_2_6_price_2017-05-09.zip\n",
      "flight_2_6/flight_2_6_price_2017-05-10.zip\n",
      "flight_2_6/flight_2_6_price_2017-05-11.zip\n",
      "flight_2_7/flight_2_7_price_2017-04-04.zip\n",
      "flight_2_7/flight_2_7_price_2017-04-06.zip\n",
      "flight_2_7/flight_2_7_price_2017-04-13.zip\n",
      "flight_2_7/flight_2_7_price_2017-04-15.zip\n",
      "flight_2_7/flight_2_7_price_2017-04-16.zip\n",
      "flight_2_7/flight_2_7_price_2017-04-19.zip\n",
      "flight_2_7/flight_2_7_price_2017-04-20.zip\n",
      "flight_2_7/flight_2_7_price_2017-04-21.zip\n",
      "flight_2_7/flight_2_7_price_2017-04-22.zip\n",
      "flight_2_7/flight_2_7_price_2017-04-23.zip\n",
      "flight_2_7/flight_2_7_price_2017-04-24.zip\n",
      "flight_2_7/flight_2_7_price_2017-04-25.zip\n",
      "flight_2_7/flight_2_7_price_2017-04-26.zip\n",
      "flight_2_7/flight_2_7_price_2017-04-27.zip\n",
      "flight_2_7/flight_2_7_price_2017-04-28.zip\n",
      "flight_2_7/flight_2_7_price_2017-04-29.zip\n",
      "flight_2_7/flight_2_7_price_2017-04-30.zip\n",
      "flight_2_7/flight_2_7_price_2017-05-01.zip\n",
      "flight_2_7/flight_2_7_price_2017-05-02.zip\n",
      "flight_2_7/flight_2_7_price_2017-05-03.zip\n",
      "flight_2_7/flight_2_7_price_2017-05-06.zip\n",
      "flight_2_7/flight_2_7_price_2017-05-07.zip\n",
      "flight_2_7/flight_2_7_price_2017-05-08.zip\n",
      "flight_2_7/flight_2_7_price_2017-05-09.zip\n",
      "flight_2_7/flight_2_7_price_2017-05-10.zip\n",
      "flight_2_7/flight_2_7_price_2017-05-11.zip\n",
      "flight_2_8/flight_2_8_price_2017-04-04.zip\n",
      "flight_2_8/flight_2_8_price_2017-04-06.zip\n",
      "flight_2_8/flight_2_8_price_2017-04-09.zip\n",
      "flight_2_8/flight_2_8_price_2017-04-13.zip\n",
      "flight_2_8/flight_2_8_price_2017-04-14.zip\n",
      "flight_2_8/flight_2_8_price_2017-04-15.zip\n",
      "flight_2_8/flight_2_8_price_2017-04-16.zip\n",
      "flight_2_8/flight_2_8_price_2017-04-19.zip\n",
      "flight_2_8/flight_2_8_price_2017-04-20.zip\n",
      "flight_2_8/flight_2_8_price_2017-04-21.zip\n",
      "flight_2_8/flight_2_8_price_2017-04-22.zip\n",
      "flight_2_8/flight_2_8_price_2017-04-23.zip\n",
      "flight_2_8/flight_2_8_price_2017-04-24.zip\n",
      "flight_2_8/flight_2_8_price_2017-04-25.zip\n",
      "flight_2_8/flight_2_8_price_2017-04-26.zip\n",
      "flight_2_8/flight_2_8_price_2017-04-27.zip\n",
      "flight_2_8/flight_2_8_price_2017-04-28.zip\n",
      "flight_2_8/flight_2_8_price_2017-04-29.zip\n",
      "flight_2_8/flight_2_8_price_2017-04-30.zip\n",
      "flight_2_8/flight_2_8_price_2017-05-01.zip\n",
      "flight_2_8/flight_2_8_price_2017-05-02.zip\n",
      "flight_2_8/flight_2_8_price_2017-05-03.zip\n",
      "flight_2_8/flight_2_8_price_2017-05-06.zip\n",
      "flight_2_8/flight_2_8_price_2017-05-07.zip\n",
      "flight_2_8/flight_2_8_price_2017-05-08.zip\n",
      "flight_2_8/flight_2_8_price_2017-05-09.zip\n",
      "flight_2_8/flight_2_8_price_2017-05-10.zip\n",
      "flight_2_8/flight_2_8_price_2017-05-11.zip\n",
      "flight_2_9/flight_2_9_price_2017-04-23.zip\n",
      "flight_2_9/flight_2_9_price_2017-04-24.zip\n",
      "flight_2_9/flight_2_9_price_2017-04-25.zip\n",
      "flight_2_9/flight_2_9_price_2017-04-26.zip\n",
      "flight_2_9/flight_2_9_price_2017-04-27.zip\n",
      "flight_2_9/flight_2_9_price_2017-04-28.zip\n",
      "flight_2_9/flight_2_9_price_2017-04-29.zip\n",
      "flight_2_9/flight_2_9_price_2017-04-30.zip\n",
      "flight_2_9/flight_2_9_price_2017-05-01.zip\n",
      "flight_2_9/flight_2_9_price_2017-05-02.zip\n",
      "flight_2_9/flight_2_9_price_2017-05-03.zip\n",
      "flight_2_9/flight_2_9_price_2017-05-06.zip\n",
      "flight_2_9/flight_2_9_price_2017-05-07.zip\n",
      "flight_2_9/flight_2_9_price_2017-05-08.zip\n",
      "flight_2_9/flight_2_9_price_2017-05-09.zip\n",
      "flight_2_9/flight_2_9_price_2017-05-10.zip\n",
      "flight_2_9/flight_2_9_price_2017-05-11.zip\n",
      "flight_3_10/flight_3_10_price_2017-04-23.zip\n",
      "flight_3_10/flight_3_10_price_2017-04-24.zip\n",
      "flight_3_10/flight_3_10_price_2017-04-25.zip\n",
      "flight_3_10/flight_3_10_price_2017-04-26.zip\n",
      "flight_3_10/flight_3_10_price_2017-04-27.zip\n",
      "flight_3_10/flight_3_10_price_2017-04-28.zip\n",
      "flight_3_10/flight_3_10_price_2017-04-29.zip\n",
      "flight_3_10/flight_3_10_price_2017-04-30.zip\n",
      "flight_3_10/flight_3_10_price_2017-05-01.zip\n",
      "flight_3_10/flight_3_10_price_2017-05-02.zip\n",
      "flight_3_10/flight_3_10_price_2017-05-06.zip\n",
      "flight_3_10/flight_3_10_price_2017-05-07.zip\n",
      "flight_3_10/flight_3_10_price_2017-05-08.zip\n",
      "flight_3_10/flight_3_10_price_2017-05-09.zip\n",
      "flight_3_10/flight_3_10_price_2017-05-10.zip\n",
      "flight_3_10/flight_3_10_price_2017-05-11.zip\n",
      "flight_3_11/flight_3_11_price_2017-04-23.zip\n",
      "flight_3_11/flight_3_11_price_2017-04-24.zip\n",
      "flight_3_11/flight_3_11_price_2017-04-25.zip\n",
      "flight_3_11/flight_3_11_price_2017-04-26.zip\n",
      "flight_3_11/flight_3_11_price_2017-04-27.zip\n",
      "flight_3_11/flight_3_11_price_2017-04-28.zip\n",
      "flight_3_11/flight_3_11_price_2017-04-29.zip\n",
      "flight_3_11/flight_3_11_price_2017-04-30.zip\n",
      "flight_3_11/flight_3_11_price_2017-05-01.zip\n",
      "flight_3_11/flight_3_11_price_2017-05-02.zip\n",
      "flight_3_11/flight_3_11_price_2017-05-06.zip\n",
      "flight_3_11/flight_3_11_price_2017-05-07.zip\n",
      "flight_3_11/flight_3_11_price_2017-05-08.zip\n",
      "flight_3_11/flight_3_11_price_2017-05-09.zip\n",
      "flight_3_11/flight_3_11_price_2017-05-10.zip\n",
      "flight_3_11/flight_3_11_price_2017-05-11.zip\n",
      "flight_3_12/flight_3_12_price_2017-04-23.zip\n",
      "flight_3_12/flight_3_12_price_2017-04-24.zip\n",
      "flight_3_12/flight_3_12_price_2017-04-25.zip\n",
      "flight_3_12/flight_3_12_price_2017-04-26.zip\n",
      "flight_3_12/flight_3_12_price_2017-04-27.zip\n",
      "flight_3_12/flight_3_12_price_2017-04-28.zip\n",
      "flight_3_12/flight_3_12_price_2017-04-29.zip\n",
      "flight_3_12/flight_3_12_price_2017-04-30.zip\n",
      "flight_3_12/flight_3_12_price_2017-05-01.zip\n",
      "flight_3_12/flight_3_12_price_2017-05-02.zip\n",
      "flight_3_12/flight_3_12_price_2017-05-06.zip\n",
      "flight_3_12/flight_3_12_price_2017-05-07.zip\n",
      "flight_3_12/flight_3_12_price_2017-05-08.zip\n",
      "flight_3_12/flight_3_12_price_2017-05-09.zip\n",
      "flight_3_12/flight_3_12_price_2017-05-10.zip\n",
      "flight_3_12/flight_3_12_price_2017-05-11.zip\n",
      "flight_3_13/flight_3_13_price_2017-04-23.zip\n",
      "flight_3_13/flight_3_13_price_2017-04-24.zip\n",
      "flight_3_13/flight_3_13_price_2017-04-25.zip\n",
      "flight_3_13/flight_3_13_price_2017-04-26.zip\n",
      "flight_3_13/flight_3_13_price_2017-04-27.zip\n",
      "flight_3_13/flight_3_13_price_2017-04-28.zip\n",
      "flight_3_13/flight_3_13_price_2017-04-29.zip\n",
      "flight_3_13/flight_3_13_price_2017-04-30.zip\n",
      "flight_3_13/flight_3_13_price_2017-05-01.zip\n",
      "flight_3_13/flight_3_13_price_2017-05-02.zip\n",
      "flight_3_13/flight_3_13_price_2017-05-06.zip\n",
      "flight_3_13/flight_3_13_price_2017-05-07.zip\n",
      "flight_3_13/flight_3_13_price_2017-05-08.zip\n",
      "flight_3_13/flight_3_13_price_2017-05-09.zip\n",
      "flight_3_13/flight_3_13_price_2017-05-10.zip\n",
      "flight_3_13/flight_3_13_price_2017-05-11.zip\n",
      "flight_3_14/flight_3_14_price_2017-04-23.zip\n",
      "flight_3_14/flight_3_14_price_2017-04-24.zip\n",
      "flight_3_14/flight_3_14_price_2017-04-26.zip\n",
      "flight_3_14/flight_3_14_price_2017-04-27.zip\n",
      "flight_3_14/flight_3_14_price_2017-04-28.zip\n",
      "flight_3_14/flight_3_14_price_2017-04-29.zip\n",
      "flight_3_14/flight_3_14_price_2017-04-30.zip\n",
      "flight_3_14/flight_3_14_price_2017-05-01.zip\n",
      "flight_3_14/flight_3_14_price_2017-05-02.zip\n",
      "flight_3_14/flight_3_14_price_2017-05-06.zip\n",
      "flight_3_14/flight_3_14_price_2017-05-07.zip\n",
      "flight_3_14/flight_3_14_price_2017-05-08.zip\n",
      "flight_3_14/flight_3_14_price_2017-05-09.zip\n",
      "flight_3_14/flight_3_14_price_2017-05-10.zip\n",
      "flight_3_14/flight_3_14_price_2017-05-11.zip\n",
      "flight_3_5/flight_3_5_price_2017-04-04.zip\n",
      "flight_3_5/flight_3_5_price_2017-04-06.zip\n",
      "flight_3_5/flight_3_5_price_2017-04-13.zip\n",
      "flight_3_5/flight_3_5_price_2017-04-14.zip\n",
      "flight_3_5/flight_3_5_price_2017-04-15.zip\n",
      "flight_3_5/flight_3_5_price_2017-04-16.zip\n",
      "flight_3_5/flight_3_5_price_2017-04-19.zip\n",
      "flight_3_5/flight_3_5_price_2017-04-20.zip\n",
      "flight_3_5/flight_3_5_price_2017-04-21.zip\n",
      "flight_3_5/flight_3_5_price_2017-04-22.zip\n",
      "flight_3_5/flight_3_5_price_2017-04-23.zip\n",
      "flight_3_5/flight_3_5_price_2017-04-24.zip\n",
      "flight_3_5/flight_3_5_price_2017-04-25.zip\n",
      "flight_3_5/flight_3_5_price_2017-04-26.zip\n",
      "flight_3_5/flight_3_5_price_2017-04-27.zip\n",
      "flight_3_5/flight_3_5_price_2017-04-28.zip\n",
      "flight_3_5/flight_3_5_price_2017-04-29.zip\n",
      "flight_3_5/flight_3_5_price_2017-04-30.zip\n",
      "flight_3_5/flight_3_5_price_2017-05-01.zip\n",
      "flight_3_5/flight_3_5_price_2017-05-02.zip\n",
      "flight_3_5/flight_3_5_price_2017-05-06.zip\n",
      "flight_3_5/flight_3_5_price_2017-05-07.zip\n",
      "flight_3_5/flight_3_5_price_2017-05-08.zip\n",
      "flight_3_5/flight_3_5_price_2017-05-09.zip\n",
      "flight_3_5/flight_3_5_price_2017-05-10.zip\n",
      "flight_3_5/flight_3_5_price_2017-05-11.zip\n",
      "flight_3_6/flight_3_6_price_2017-04-04.zip\n",
      "flight_3_6/flight_3_6_price_2017-04-06.zip\n",
      "flight_3_6/flight_3_6_price_2017-04-13.zip\n",
      "flight_3_6/flight_3_6_price_2017-04-14.zip\n",
      "flight_3_6/flight_3_6_price_2017-04-15.zip\n",
      "flight_3_6/flight_3_6_price_2017-04-16.zip\n",
      "flight_3_6/flight_3_6_price_2017-04-19.zip\n",
      "flight_3_6/flight_3_6_price_2017-04-20.zip\n",
      "flight_3_6/flight_3_6_price_2017-04-21.zip\n",
      "flight_3_6/flight_3_6_price_2017-04-22.zip\n",
      "flight_3_6/flight_3_6_price_2017-04-23.zip\n",
      "flight_3_6/flight_3_6_price_2017-04-24.zip\n",
      "flight_3_6/flight_3_6_price_2017-04-25.zip\n",
      "flight_3_6/flight_3_6_price_2017-04-26.zip\n",
      "flight_3_6/flight_3_6_price_2017-04-27.zip\n",
      "flight_3_6/flight_3_6_price_2017-04-28.zip\n",
      "flight_3_6/flight_3_6_price_2017-04-29.zip\n",
      "flight_3_6/flight_3_6_price_2017-04-30.zip\n",
      "flight_3_6/flight_3_6_price_2017-05-01.zip\n",
      "flight_3_6/flight_3_6_price_2017-05-02.zip\n",
      "flight_3_6/flight_3_6_price_2017-05-06.zip\n",
      "flight_3_6/flight_3_6_price_2017-05-07.zip\n",
      "flight_3_6/flight_3_6_price_2017-05-08.zip\n",
      "flight_3_6/flight_3_6_price_2017-05-09.zip\n",
      "flight_3_6/flight_3_6_price_2017-05-10.zip\n",
      "flight_3_6/flight_3_6_price_2017-05-11.zip\n",
      "flight_3_7/flight_3_7_price_2017-04-04.zip\n",
      "flight_3_7/flight_3_7_price_2017-04-06.zip\n",
      "flight_3_7/flight_3_7_price_2017-04-13.zip\n",
      "flight_3_7/flight_3_7_price_2017-04-14.zip\n",
      "flight_3_7/flight_3_7_price_2017-04-15.zip\n",
      "flight_3_7/flight_3_7_price_2017-04-16.zip\n",
      "flight_3_7/flight_3_7_price_2017-04-18.zip\n",
      "flight_3_7/flight_3_7_price_2017-04-19.zip\n",
      "flight_3_7/flight_3_7_price_2017-04-20.zip\n",
      "flight_3_7/flight_3_7_price_2017-04-21.zip\n",
      "flight_3_7/flight_3_7_price_2017-04-22.zip\n",
      "flight_3_7/flight_3_7_price_2017-04-23.zip\n",
      "flight_3_7/flight_3_7_price_2017-04-24.zip\n",
      "flight_3_7/flight_3_7_price_2017-04-25.zip\n",
      "flight_3_7/flight_3_7_price_2017-04-26.zip\n",
      "flight_3_7/flight_3_7_price_2017-04-27.zip\n",
      "flight_3_7/flight_3_7_price_2017-04-28.zip\n",
      "flight_3_7/flight_3_7_price_2017-04-29.zip\n",
      "flight_3_7/flight_3_7_price_2017-04-30.zip\n",
      "flight_3_7/flight_3_7_price_2017-05-01.zip\n",
      "flight_3_7/flight_3_7_price_2017-05-02.zip\n",
      "flight_3_7/flight_3_7_price_2017-05-06.zip\n",
      "flight_3_7/flight_3_7_price_2017-05-07.zip\n",
      "flight_3_7/flight_3_7_price_2017-05-08.zip\n",
      "flight_3_7/flight_3_7_price_2017-05-09.zip\n",
      "flight_3_7/flight_3_7_price_2017-05-10.zip\n",
      "flight_3_7/flight_3_7_price_2017-05-11.zip\n",
      "flight_3_8/flight_3_8_price_2017-04-04.zip\n",
      "flight_3_8/flight_3_8_price_2017-04-06.zip\n",
      "flight_3_8/flight_3_8_price_2017-04-13.zip\n",
      "flight_3_8/flight_3_8_price_2017-04-14.zip\n",
      "flight_3_8/flight_3_8_price_2017-04-15.zip\n",
      "flight_3_8/flight_3_8_price_2017-04-16.zip\n",
      "flight_3_8/flight_3_8_price_2017-04-19.zip\n",
      "flight_3_8/flight_3_8_price_2017-04-20.zip\n",
      "flight_3_8/flight_3_8_price_2017-04-21.zip\n",
      "flight_3_8/flight_3_8_price_2017-04-22.zip\n",
      "flight_3_8/flight_3_8_price_2017-04-23.zip\n",
      "flight_3_8/flight_3_8_price_2017-04-24.zip\n",
      "flight_3_8/flight_3_8_price_2017-04-25.zip\n",
      "flight_3_8/flight_3_8_price_2017-04-26.zip\n",
      "flight_3_8/flight_3_8_price_2017-04-27.zip\n",
      "flight_3_8/flight_3_8_price_2017-04-28.zip\n",
      "flight_3_8/flight_3_8_price_2017-04-29.zip\n",
      "flight_3_8/flight_3_8_price_2017-04-30.zip\n",
      "flight_3_8/flight_3_8_price_2017-05-01.zip\n",
      "flight_3_8/flight_3_8_price_2017-05-02.zip\n",
      "flight_3_8/flight_3_8_price_2017-05-06.zip\n",
      "flight_3_8/flight_3_8_price_2017-05-07.zip\n",
      "flight_3_8/flight_3_8_price_2017-05-08.zip\n",
      "flight_3_8/flight_3_8_price_2017-05-09.zip\n",
      "flight_3_8/flight_3_8_price_2017-05-10.zip\n",
      "flight_3_8/flight_3_8_price_2017-05-11.zip\n",
      "flight_3_9/flight_3_9_price_2017-04-23.zip\n",
      "flight_3_9/flight_3_9_price_2017-04-24.zip\n",
      "flight_3_9/flight_3_9_price_2017-04-25.zip\n",
      "flight_3_9/flight_3_9_price_2017-04-26.zip\n",
      "flight_3_9/flight_3_9_price_2017-04-27.zip\n",
      "flight_3_9/flight_3_9_price_2017-04-28.zip\n",
      "flight_3_9/flight_3_9_price_2017-04-29.zip\n",
      "flight_3_9/flight_3_9_price_2017-04-30.zip\n",
      "flight_3_9/flight_3_9_price_2017-05-01.zip\n",
      "flight_3_9/flight_3_9_price_2017-05-02.zip\n",
      "flight_3_9/flight_3_9_price_2017-05-06.zip\n",
      "flight_3_9/flight_3_9_price_2017-05-07.zip\n",
      "flight_3_9/flight_3_9_price_2017-05-08.zip\n",
      "flight_3_9/flight_3_9_price_2017-05-09.zip\n",
      "flight_3_9/flight_3_9_price_2017-05-10.zip\n",
      "flight_3_9/flight_3_9_price_2017-05-11.zip\n",
      "flight_4_10/flight_4_10_price_2017-04-25.zip\n",
      "flight_4_10/flight_4_10_price_2017-04-26.zip\n",
      "flight_4_10/flight_4_10_price_2017-04-27.zip\n",
      "flight_4_10/flight_4_10_price_2017-04-28.zip\n",
      "flight_4_10/flight_4_10_price_2017-04-29.zip\n",
      "flight_4_10/flight_4_10_price_2017-04-30.zip\n",
      "flight_4_10/flight_4_10_price_2017-05-01.zip\n",
      "flight_4_10/flight_4_10_price_2017-05-02.zip\n",
      "flight_4_10/flight_4_10_price_2017-05-03.zip\n",
      "flight_4_10/flight_4_10_price_2017-05-06.zip\n",
      "flight_4_10/flight_4_10_price_2017-05-07.zip\n",
      "flight_4_10/flight_4_10_price_2017-05-08.zip\n",
      "flight_4_10/flight_4_10_price_2017-05-09.zip\n",
      "flight_4_10/flight_4_10_price_2017-05-10.zip\n",
      "flight_4_10/flight_4_10_price_2017-05-11.zip\n",
      "flight_4_11/flight_4_11_price_2017-04-25.zip\n",
      "flight_4_11/flight_4_11_price_2017-04-26.zip\n",
      "flight_4_11/flight_4_11_price_2017-04-27.zip\n",
      "flight_4_11/flight_4_11_price_2017-04-28.zip\n",
      "flight_4_11/flight_4_11_price_2017-04-29.zip\n",
      "flight_4_11/flight_4_11_price_2017-04-30.zip\n",
      "flight_4_11/flight_4_11_price_2017-05-01.zip\n",
      "flight_4_11/flight_4_11_price_2017-05-02.zip\n",
      "flight_4_11/flight_4_11_price_2017-05-03.zip\n",
      "flight_4_11/flight_4_11_price_2017-05-06.zip\n",
      "flight_4_11/flight_4_11_price_2017-05-07.zip\n",
      "flight_4_11/flight_4_11_price_2017-05-08.zip\n",
      "flight_4_11/flight_4_11_price_2017-05-09.zip\n",
      "flight_4_11/flight_4_11_price_2017-05-10.zip\n",
      "flight_4_11/flight_4_11_price_2017-05-11.zip\n",
      "flight_4_12/flight_4_12_price_2017-04-25.zip\n",
      "flight_4_12/flight_4_12_price_2017-04-26.zip\n",
      "flight_4_12/flight_4_12_price_2017-04-27.zip\n",
      "flight_4_12/flight_4_12_price_2017-04-28.zip\n",
      "flight_4_12/flight_4_12_price_2017-04-29.zip\n",
      "flight_4_12/flight_4_12_price_2017-04-30.zip\n",
      "flight_4_12/flight_4_12_price_2017-05-01.zip\n",
      "flight_4_12/flight_4_12_price_2017-05-02.zip\n",
      "flight_4_12/flight_4_12_price_2017-05-03.zip\n",
      "flight_4_12/flight_4_12_price_2017-05-06.zip\n",
      "flight_4_12/flight_4_12_price_2017-05-07.zip\n",
      "flight_4_12/flight_4_12_price_2017-05-08.zip\n",
      "flight_4_12/flight_4_12_price_2017-05-09.zip\n",
      "flight_4_12/flight_4_12_price_2017-05-10.zip\n",
      "flight_4_12/flight_4_12_price_2017-05-11.zip\n",
      "flight_4_13/flight_4_13_price_2017-04-25.zip\n",
      "flight_4_13/flight_4_13_price_2017-04-26.zip\n",
      "flight_4_13/flight_4_13_price_2017-04-27.zip\n",
      "flight_4_13/flight_4_13_price_2017-04-28.zip\n",
      "flight_4_13/flight_4_13_price_2017-04-29.zip\n",
      "flight_4_13/flight_4_13_price_2017-04-30.zip\n",
      "flight_4_13/flight_4_13_price_2017-05-01.zip\n",
      "flight_4_13/flight_4_13_price_2017-05-02.zip\n",
      "flight_4_13/flight_4_13_price_2017-05-03.zip\n",
      "flight_4_13/flight_4_13_price_2017-05-06.zip\n",
      "flight_4_13/flight_4_13_price_2017-05-07.zip\n",
      "flight_4_13/flight_4_13_price_2017-05-08.zip\n",
      "flight_4_13/flight_4_13_price_2017-05-09.zip\n",
      "flight_4_13/flight_4_13_price_2017-05-10.zip\n",
      "flight_4_13/flight_4_13_price_2017-05-11.zip\n",
      "flight_4_14/flight_4_14_price_2017-04-25.zip\n",
      "flight_4_14/flight_4_14_price_2017-04-26.zip\n",
      "flight_4_14/flight_4_14_price_2017-04-27.zip\n",
      "flight_4_14/flight_4_14_price_2017-04-28.zip\n",
      "flight_4_14/flight_4_14_price_2017-04-29.zip\n",
      "flight_4_14/flight_4_14_price_2017-04-30.zip\n",
      "flight_4_14/flight_4_14_price_2017-05-01.zip\n",
      "flight_4_14/flight_4_14_price_2017-05-02.zip\n",
      "flight_4_14/flight_4_14_price_2017-05-03.zip\n",
      "flight_4_14/flight_4_14_price_2017-05-06.zip\n",
      "flight_4_14/flight_4_14_price_2017-05-07.zip\n",
      "flight_4_14/flight_4_14_price_2017-05-08.zip\n",
      "flight_4_14/flight_4_14_price_2017-05-09.zip\n",
      "flight_4_14/flight_4_14_price_2017-05-10.zip\n",
      "flight_4_14/flight_4_14_price_2017-05-11.zip\n",
      "flight_4_5/flight_4_5_price_2017-04-04.zip\n",
      "flight_4_5/flight_4_5_price_2017-04-06.zip\n",
      "flight_4_5/flight_4_5_price_2017-04-13.zip\n",
      "flight_4_5/flight_4_5_price_2017-04-14.zip\n",
      "flight_4_5/flight_4_5_price_2017-04-15.zip\n",
      "flight_4_5/flight_4_5_price_2017-04-16.zip\n",
      "flight_4_5/flight_4_5_price_2017-04-19.zip\n",
      "flight_4_5/flight_4_5_price_2017-04-20.zip\n",
      "flight_4_5/flight_4_5_price_2017-04-21.zip\n",
      "flight_4_5/flight_4_5_price_2017-04-22.zip\n",
      "flight_4_5/flight_4_5_price_2017-04-25.zip\n",
      "flight_4_5/flight_4_5_price_2017-04-26.zip\n",
      "flight_4_5/flight_4_5_price_2017-04-27.zip\n",
      "flight_4_5/flight_4_5_price_2017-04-28.zip\n",
      "flight_4_5/flight_4_5_price_2017-04-29.zip\n",
      "flight_4_5/flight_4_5_price_2017-04-30.zip\n"
     ]
    }
   ],
   "source": [
    "for item in all_objects['Contents']:\n",
    "    print(item['Key'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "emptyDF.write.mode('append').parquet(os.path.join(comb_fldr, \"flight_v1_0.pq\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! rm -rf /home/ubuntu/s3/flight_1_5/extracted/flight_1_5_price_2017-05-10_.pq\n",
    "# -f = to ignore non-existent files, never prompt\n",
    "# -r = to remove directories and their contents recursively\n",
    "# -v = to explain what is being done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spark.createDataFrame(testDF2, struct_v1_1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "\n",
    "df1 = spark.read.json(os.path.join(comb_fldr, \"flight_1_5_price_2017-05-10_2017-06-10*.jsonl\"))\n",
    "df2 = spark.read.json(os.path.join(comb_fldr, \"flight_1_5_price_2017-05-10_2017-11-06_2_*.jsonl\"))\n",
    "df = df1.unionAll(df2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# df.printSchema()\n",
    "# df.select(\"basic.*\").show()\n",
    "df.select(F.explode(\"flight_list\")).show(200, truncate=False)\n",
    "# df.select[\"basic\"]\n",
    "# flat_table = []  # a list\n",
    "# d = json.load(f)\n",
    "# flight_list = json_normalize(d['flight_list'])\n",
    "# basic = json_normalize(d['basic'])\n",
    "\n",
    "# # create dataframe\n",
    "# basic2 = basic.drop('search_date', 1)\n",
    "# basic2['tmp'] = 1\n",
    "# flight_list['tmp'] = 1\n",
    "# DF = pd.merge(basic2, flight_list, on=['tmp'])\n",
    "# DF=DF.drop('tmp', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "basic2 = basic.drop('search_date', 1)\n",
    "basic2['tmp'] = 1\n",
    "flight_list['tmp'] = 1\n",
    "DF = pd.merge(basic2, flight_list, on=['tmp'])\n",
    "DF=DF.drop('tmp', 1)\n",
    "# DF.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = open(\"temp.jsonl\", \"w\")\n",
    "\n",
    "for row in DF.iterrows():\n",
    "    row[1].to_json(f)\n",
    "    f.write(\"\\n\")\n",
    "    \n",
    "f.close()\n",
    "\n",
    "open(\"temp.jsonl\").read()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flight = spark.read.json(\"/home/ubuntu/s3/comb/temp.jsonl\")\n",
    "flight.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flight.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "data = []\n",
    "with open('/home/ubuntu/s3/comb/flight_1_5_price_2017-05-10_2017-11-06_2_7.jsonl') as f:\n",
    "    for line in f:\n",
    "        d = json.load(f)\n",
    "        flight_list = json_normalize(d['flight_list'])\n",
    "        basic = json_normalize(d['flight_list'])\n",
    "        flat_table = dict()\n",
    "        flat_table['basic'] = basic\n",
    "        flat_table['flight'] = flight_list\n",
    "        flat_table_list.append(flat_table)    \n",
    "df = pd.DataFrame.from_records(data)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! cd /home/ubuntu/s3/comb\n",
    "! head -n10 *.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(flat_table_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_records(flat_table_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(jsonl_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having errors. Probably due to 1 Gb RAM being too small to hold the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flight = spark.read.json(jsonl_file_name)\n",
    "flight.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flight.show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flight.write.parquet(pq_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flight.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "extraction_path = \"/home/ubuntu/s3/flight_1_5/extracted\"\n",
    "\n",
    "\n",
    "\n",
    "d = read_from_json_file(json_file_name)\n",
    "\n",
    "jsonl_file_name = json_file_name.replace('.json','.jsonl')\n",
    "\n",
    "with open(jsonl_file_name,'w') as f:\n",
    "    json.dump(d, f)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# def spark_write(spark,jsonl_file_name,parquet_file_name):\n",
    "#     df = spark.read.json(jsonl_file_name)\n",
    "#     df.write.parquet(parquet_file_name)\n",
    "    \n",
    "# def spark_print_json(spark,json_file_name):\n",
    "#     d = read_from_json_file(json_file_name)\n",
    "    \n",
    "#     jsonl_file_name = json_file_name.replace('.json','.jsonl')\n",
    "    \n",
    "#     with open(jsonl_file_name,'w') as f:\n",
    "#         json.dump(d, f)\n",
    "    \n",
    "#     df = spark.read.json(jsonl_file_name)\n",
    "    \n",
    "#     df.createGlobalTempView(\"flight\")\n",
    "    \n",
    "#     spark.sql(\"SELECT fromCity,toCity,flight_leg1.departureTime, flight_leg1.departureLocation.airportLongName FROM global_temp.flight\").show()\n",
    "    \n",
    "def read_from_json_file(filename):\n",
    "    with open(filename,'r') as f:\n",
    "        d = json.load(f)\n",
    "        \n",
    "    return d\n",
    "\n",
    "# def pq_to_json_file(spark, pq_file_name, json_file_name):\n",
    "#     df = spark.read.load(pq_file_name)\n",
    "    \n",
    "#     df.write.json(json_file_name)\n",
    "    \n",
    "# def read_data_from_pq_file(spark,filename):\n",
    "#     df = spark.read.load(filename)\n",
    "    \n",
    "#     df.select('from_city_name','to_city_name').show()\n",
    "#     #spark.sql(\"SELECT from_city_name,to_city_name FROM global_temp.flight\").show()\n",
    "    \n",
    "# def main():\n",
    "#     full_path_pq_file_name = \"test/t.parquet\"\n",
    "\n",
    "#     spark = SparkSession \\\n",
    "#         .builder \\\n",
    "#         .appName(\"Python Spark SQL basic example\") \\\n",
    "#         .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "#         .getOrCreate()\n",
    "            \n",
    "#     parser = argparse.ArgumentParser()\n",
    "    \n",
    "#     parser.add_argument('cmd',help='[json,pq,json2pq,jsonprint]')\n",
    "#     parser.add_argument('--json',help='json file namejson,pq]')\n",
    "#     parser.add_argument('--pq',help='parquet file name')\n",
    "    \n",
    "#     args = parser.parse_args()\n",
    "    \n",
    "#     cmd_ind = args.cmd\n",
    "#     json_file_name = args.json\n",
    "#     pq_file_name = args.pq\n",
    "    \n",
    "#     if cmd_ind == 'json2pq':\n",
    "#         if json_file_name is None:\n",
    "#             print(\"Please set the json file name by --json\")\n",
    "#             quit(0)\n",
    "            \n",
    "#         if pq_file_name is None:\n",
    "#             print(\"Please set the pqrquet file name by --pq\")\n",
    "#             quit(0)\n",
    "#         d = read_from_json_file(json_file_name)\n",
    "        \n",
    "#         jsonl_file_name = json_file_name.replace('.json','.jsonl')\n",
    "        \n",
    "#         with open(jsonl_file_name,'w') as f:\n",
    "#             json.dump(d, f)\n",
    "            \n",
    "#         spark_write(spark, jsonl_file_name, pq_file_name)\n",
    "#     elif cmd_ind == 'pq':\n",
    "#         if pq_file_name is None:\n",
    "#             print(\"Please set the pqrquet file name by --pq\")\n",
    "#             quit(0)\n",
    "#         read_data_from_pq_file(spark,full_path_pq_file_name)\n",
    "#     elif cmd_ind == 'pq2json':\n",
    "#         if json_file_name is None:\n",
    "#             print(\"Please set the json file name by --json\")\n",
    "#             quit(0)\n",
    "            \n",
    "#         if pq_file_name is None:\n",
    "#             print(\"Please set the pqrquet file name by --pq\")\n",
    "#             quit(0)\n",
    "            \n",
    "#         pq_to_json_file(spark,pq_file_name,json_file_name)\n",
    "        \n",
    "#     elif cmd_ind== 'jsonprint':\n",
    "#         if json_file_name is None:\n",
    "#             print(\"Please set the json file name by --json\")\n",
    "#             quit(0)\n",
    "            \n",
    "#         spark_print_json(spark,json_file_name)\n",
    "#     else:\n",
    "#         print('invalid cmd command')\n",
    "        \n",
    "#     spark.stop()\n",
    "    \n",
    "    \n",
    "# if __name__== '__main__':\n",
    "#     main()\n",
    "    \n",
    "    \n",
    "# spark-submit sp.py jsonprint --json final_flight_result_format_v1.1.json\n",
    "\n",
    "\n",
    "# json_file_name = \"D:\\\\Data Science\\\\pySpark\\\\test_schema\\\\final_flight_result_format_v1.1.json\"\n",
    "# jsonl_file_name = \"C:\\\\s3\\\\20170503_jsonl\\\\flight_1_6.jsonl\"\n",
    "jsonl_file_name = \"C:\\\\s3\\\\a3.json\"\n",
    "\n",
    "# d = read_from_json_file(json_file_name)\n",
    "\n",
    "# jsonl_file_name = json_file_name.replace('.json','.jsonl')\n",
    "\n",
    "# with open(jsonl_file_name,'w') as f:\n",
    "#     json.dump(d, f)\n",
    "\n",
    "# flight = spark.read.json(jsonl_file_name)\n",
    "\n",
    "# df.createGlobalTempView(\"flight\")\n",
    "\n",
    "# spark.sql(\"SELECT fromCity,toCity,flight_leg1.departureTime, flight_leg1.departureLocation.airportLongName FROM global_temp.flight\").show()\n",
    "  \n",
    "# flight.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# flight.printSchema()\n",
    "# flight.drop('airline_codes').repartition(1).write.format('com.databricks.spark.csv').save('C:\\\\s3\\\\20170503_jsonl\\\\flight_1_6.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# flight.write.parquet(\"C:\\\\s3\\\\20170503_jsonl\\\\flight.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# flight = spark.read.parquet(\"C:\\\\s3\\\\20170503_jsonl\\\\flight.parquet\")\n",
    "flight = spark.read.parquet(\"/home/ubuntu/parquet/flight.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flight.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flight.printSchema()\n",
    "flight.show(2,truncate=False)\n",
    "display(flight.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# path_to_input = \"D:\\Data Science\\pySpark\\original_json\"\n",
    "# original_data = spark.read.json(sc.wholeTextFiles(path_to_input).values())\n",
    "\n",
    "# original_data = sqlContext.read.json(\"D:\\\\Data Science\\\\pySpark\\\\original_json\\\\original_flight_info.txt\")\n",
    "# original_data.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import json\n",
    "# from pprint import pprint\n",
    "\n",
    "# # with open(\"D:\\\\Data Science\\\\pySpark\\\\original_json\\\\original_flight_info.txt\") as data_file:    \n",
    "# #     data = json.load(data_file)\n",
    "\n",
    "# # pprint(data)\n",
    "# # # data.head()\n",
    "\n",
    "# df = spark.read.json(\"D:\\\\Data Science\\\\Flight v2\\\\python\\\\final_flight_result_format_v1.1.json\")\n",
    "# # Displays the content of the DataFrame to stdout\n",
    "# df.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# flight = sc.textFile(\"C:\\\\s3_temp\\\\summary\\\\flat_table.csv\").map(lambda line: (line.split(',')[5], line.split(',')[10])).collect()\n",
    "# flight = sc.textFile(\"C:\\\\s3_temp\\\\summary\\\\flat_table.csv\")\n",
    "# flight = spark.read.csv(\"C:\\\\s3_temp\\\\summary\\\\flat_table.csv\", header=True, mode=\"DROPMALFORMED\")\n",
    "\n",
    "# flight.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# flight.repartition(1).write.format('com.databricks.spark.csv').save('D://Data Science//pySpark//test2.csv', header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flight.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# flight.rdd.takeSample(False, 3, 1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flight.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import year, month, dayofmonth, hour, minute, weekofyear, crosstab\n",
    "# flight2.select(year(\"arr_time\").alias('year'), \n",
    "# from pyspark.sql.functions import regexp_extract, col, split\n",
    "# # import re\n",
    "\n",
    "# from pyspark.sql.functions import udf\n",
    "# from pyspark.sql.types import IntegerType, StringType, BooleanType\n",
    "\n",
    "def toBool(aString):\n",
    "    if aString.lower() == 'true':\n",
    "        return True\n",
    "    else: \n",
    "        return False\n",
    " \n",
    "udfToBool = udf(toBool, BooleanType())\n",
    "\n",
    "\n",
    "def groupTicketLeft(aString):\n",
    "    if aString == '0':\n",
    "        return 'plenty'\n",
    "    else: \n",
    "        return aString\n",
    " \n",
    "udfGroupTicketLeft = udf(groupTicketLeft, StringType())\n",
    "\n",
    "# def getMinutes(aString):\n",
    "#     return minute(unix_timestamp(aString, \"HH'h'mm'm'\").cast(\"timestamp\"))\n",
    "\n",
    "# udfGetMinutes = udf(getMinutes, IntegerType())\n",
    "\n",
    "# def getHours(x):\n",
    "#   return re.match('([0-9]+(?=h))', x)\n",
    "# getHours(\"14h\")\n",
    "# len(flight.columns)\n",
    "# flight.columns\n",
    "# flight.dtypes\n",
    "flight2 = (flight.withColumn('search_date', flight.search_date.cast('timestamp'))\n",
    "#           .withColumn('search_date_y', flight.search_date_y.cast('timestamp'))\n",
    "#           .withColumnRenamed('search_date_y', 'search_date')\n",
    "          .withColumn('stay_days', flight.stay_days.cast('int'))                                \n",
    "           #create local time first\n",
    "           .withColumn('dep_time_local', flight.dep_time.substr(1, 23).cast('timestamp'))                      \n",
    "          .withColumn('dep_time', flight.dep_time.cast('timestamp'))\n",
    "           #create local time first\n",
    "          .withColumn('arr_time_local', flight.arr_time.substr(1, 23).cast('timestamp'))                      \n",
    "          .withColumn('arr_time', flight.arr_time.cast('timestamp'))\n",
    "           #duration           \n",
    "           .withColumn('duration_h',split(flight.duration,'h').getItem(0))\n",
    "           .withColumn('duration_m',split(flight.duration,'h').getItem(1))\n",
    "           #stop info\n",
    "           .withColumn('stop_info1',split(flight.stop_info,';').getItem(0))\n",
    "           .withColumn('stop_info2',split(flight.stop_info,';').getItem(1))\n",
    "#            .withColumn('druation_hours', \n",
    "#                        flight.selectExpr(\"duration\", \"regexp_extract(duration,'([0-9]+(?=h))', 1) as duration_hours\")\n",
    "#                        .duration_hours\n",
    "#                        .cast('int'))\n",
    "          .withColumn('start_date', flight.start_date.cast('date'))           \n",
    "          .withColumn('price', flight.price.cast('double'))          \n",
    "           # to correct the name\n",
    "          .withColumnRenamed('check_bag_inc', 'check_bag_not_inc')\n",
    "#           .withColumn('check_bag_not_inc', udfToBool(flight.check_bag_inc))\n",
    "          .withColumn('power', flight.power.cast('boolean'))           \n",
    "          .withColumn('video', flight.video.cast('boolean'))\n",
    "          .withColumn('wifi', flight.wifi.cast('boolean'))           \n",
    "          .withColumn('stop', flight.stop.cast('int')) \n",
    "          .withColumn('span_days', flight.span_days.cast('int'))           \n",
    "           .withColumn('ticket_left', udfGroupTicketLeft(flight.ticket_left))\n",
    "           .drop('search_date_x', 'check_bag_inc')\n",
    "          )\n",
    "\n",
    "flight2 = (flight2.withColumn('stop_loc1',split(flight2.stop_info1,':').getItem(0))\n",
    "           .withColumn('stop_duration1',split(flight2.stop_info1, ':').getItem(1))\n",
    "           .withColumn('stop_loc2',split(flight2.stop_info2,':').getItem(0))\n",
    "           .withColumn('stop_duration2',split(flight2.stop_info2, ':').getItem(1)))\n",
    "\n",
    "flight2 = (flight2.withColumn('stop_duration_h1',split(flight2.stop_duration1,'h').getItem(0))\n",
    "           .withColumn('stop_duration_m1',split(flight2.stop_duration1,'h').getItem(1))\n",
    "           .withColumn('stop_duration_h2',split(flight2.stop_duration2,'h').getItem(0))\n",
    "           .withColumn('stop_duration_m2',split(flight2.stop_duration2,'h').getItem(1)))\n",
    "# .withColumn('arr_time_zone', flight.arr_time.substr(24, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define udf\n",
    "\n",
    "def getMinutes(hString, minString):\n",
    "    if (hString is not None) & (minString is not None): return int(hString) * 60 + int(minString[:-1])\n",
    "    else: return None\n",
    "#     if score >= 80: return 'A'\n",
    "#     elif score >= 60: return 'B'\n",
    "#     elif score >= 35: return 'C'\n",
    "#     else: return 'D'\n",
    " \n",
    "udfGetMinutes = udf(getMinutes, IntegerType())\n",
    "flight2 = (flight2.withColumn(\"duration_minutes\", udfGetMinutes(\"duration_h\", \"duration_m\"))\n",
    "                    .withColumn(\"stop1_minutes\", udfGetMinutes(\"stop_duration_h1\", \"stop_duration_m1\"))\n",
    "                   .withColumn(\"stop2_minutes\", udfGetMinutes(\"stop_duration_h2\", \"stop_duration_m2\")))\n",
    "                 \n",
    "flight2.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# flight2.groupby('search_date', 'from_city_name', 'to_city_name', 'trip').count().show()\n",
    "# flight2.groupby('table_name').count().show()\n",
    "flight2.groupby('stop_info', \n",
    "                'stop_info1', 'stop_loc1', \n",
    "                'stop_duration1', 'stop_duration_h1', 'stop_duration_m1',\n",
    "                'stop_info2', 'stop_loc2', \n",
    "                'stop_duration2',  'stop_duration_h2', 'stop_duration_m2').count().show(500, truncate=False)\n",
    "# flight2.groupby('stop_duration1').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flight2.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# flight2 = flight2.drop('stop_duration_h1', 'stop_duration_m1', 'stop_duration_h2', 'stop_duration_m2',\n",
    "#                       'stop_duration1', 'stop_duration2',\n",
    "#                       'duration_h', 'duration_m', \n",
    "#                        'stop_info', 'stop_info1', 'stop_info2')\n",
    "# display(flight2.show(2))\n",
    "# display(flight2.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flight2.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def groupTime(dt):\n",
    "    h = dt.hour\n",
    "    if h < 5 : return 'early morning'    \n",
    "    elif h < 12 : return 'morning'\n",
    "    elif h < 18 : return 'afternoon'    \n",
    "    else: return 'evening'\n",
    "#     if h >= 18 or h < 7 : return 'night'    \n",
    "#     elif 7 <= h < 12 : return 'morning'\n",
    "#     elif 12 <= h < 18 : return 'afternoon'    \n",
    "#     else: return 'error'\n",
    "    \n",
    "    \n",
    "udfGroupTime = udf(groupTime, StringType())\n",
    "flight2 = flight2.withColumn(\"arr_time_group\", udfGroupTime(\"arr_time_local\")) \\\n",
    "                .withColumn(\"dep_time_group\", udfGroupTime(\"dep_time_local\"))              \n",
    "flight2.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "# flight2.select(\"dep_time_local\", \"dep_time_group\", \"arr_time_local\", \"arr_time_group\").show(10)\n",
    "#flight2.describe(['price']).show()# groupBy(\"dep_time_group\", \"arr_time_group\")\n",
    "# flight2.freqItems(flight2.trip)\n",
    "# flight2.groupby(flight2.trip, flight2.stay_days, flight.dep_time_group).agg(func.mean('price')).show()\n",
    "# flight2.groupby(flight2.trip, flight2.dep_time_group).agg(func.mean('price')).orderBy(['trip', 'avg(price)'], ascending=[1, 0]).show()\n",
    "# flight2.groupby(flight2.trip, flight2.company).agg(F.mean('price'), F.stddev('price'), F.max('price'), F.min('price')).orderBy(['trip', 'stddev_samp(price)'], ascending=[1, 0]).show(10000)\n",
    "flight2.filter(flight2.price <= 0).count() / flight2.count()\n",
    "\n",
    "# drop rows with zero price\n",
    "flight2 = flight2.filter(flight2.price > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# duration_test.select(trim(col(\"duration\"))).show()\n",
    "# df\n",
    "# .withColumn('Created-formatted',when((df.Created.isNull() | (df.Created == '')) ,'0')\n",
    "# .otherwise(unix_timestamp(df.Created,'yyyy-MM-dd')))\n",
    "\n",
    "duration_test = flight2.select(\"stop_duration1\")\n",
    "duration_test.show()\n",
    "\n",
    "duration_test.withColumn('duration_h', when(duration_test.stop_duration1.isNull(), None)\n",
    "                          .otherwise(hour(unix_timestamp(duration_test.stop_duration1,\"HH'h'mm'm'\").cast(\"timestamp\")))).show(20)          \n",
    "    \n",
    "#     .withColumn('duration_m', minute(unix_timestamp(duration_test.duration,\"HH'h'mm'm'\").cast(\"timestamp\")))).show(2)\n",
    "\n",
    "\n",
    "# duration_test.withColumn('duration2',when((duration_test.duration.isNull() | (duration_test.duration == '')) , 0)\\\n",
    "#   .otherwise(unix_timestamp(duration_test.duration,\"HH'h'mm'm'\").cast(\"timestamp\"))).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from datetime import date\n",
    "\n",
    "# import holidays\n",
    "\n",
    "# us_holidays = holidays.UnitedStates()  # or holidays.US()\n",
    "\n",
    "# date(2015, 1, 1) in us_holidays  # True\n",
    "# date(2020, 12, 25) in us_holidays  # False\n",
    "\n",
    "# # print(us_holidays)\n",
    "\n",
    "\n",
    "# for date, name in sorted(holidays.AU(state='NSW', years=2017).items()):\n",
    "#      print(date, name)\n",
    "        \n",
    "# for date, name in sorted(holidays.NZ(state='AUK', years=2017).items()):\n",
    "#      print(date, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # The Holiday class will also recognize strings of any format\n",
    "# # and int/float representing a Unix timestamp\n",
    "# '2014-01-01' in us_holidays  # True\n",
    "# '1/1/2014' in us_holidays    # True\n",
    "# 1388597445 in us_holidays    # True\n",
    "\n",
    "# us_holidays.get('2014-01-01')  # \"New Year's Day\"\n",
    "\n",
    "# # Easily create custom Holiday objects with your own dates instead\n",
    "# # of using the pre-defined countries/states/provinces available\n",
    "# custom_holidays = holidays.HolidayBase()\n",
    "# # Append custom holiday dates by passing:\n",
    "# # 1) a dict with date/name key/value pairs,\n",
    "# custom_holidays.append({\"2015-01-01\": \"New Year's Day\"})\n",
    "# # 2) a list of dates (in any format: date, datetime, string, integer),\n",
    "# custom_holidays.append(['2015-07-01', '07/04/2015'])\n",
    "# # 3) a single date item\n",
    "# custom_holidays.append(date(2015, 12, 25))\n",
    "\n",
    "# date(2015, 1, 1) in custom_holidays  # True\n",
    "# date(2015, 1, 2) in custom_holidays  # False\n",
    "# '12/25/2015' in custom_holidays      # True\n",
    "\n",
    "# # For more complex logic like 4th Monday of January, you can inherit the\n",
    "# # HolidayBase class and define your own _populate(year) method. See below\n",
    "# # documentation for examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from datetime import date\n",
    "# from workalendar.europe import France\n",
    "# cal = France()\n",
    "# cal.holidays(2017)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# flight2.show(2)\n",
    "flight3 = flight2\n",
    "\n",
    "\n",
    "def getWeekday(dt):\n",
    "    return dt.weekday()\n",
    "    \n",
    "def getWeeknumber(dt):\n",
    "    return dt.isocalendar()[1]\n",
    "\n",
    "    \n",
    "udfGetWeekday = udf(getWeekday, StringType())\n",
    "udfGetWeeknumber = udf(getWeeknumber, IntegerType())\n",
    "\n",
    "flight3 = flight3.withColumn(\"dep_weekday\", udfGetWeekday(\"start_date\")) \\\n",
    "                .withColumn(\"dep_weeknum\", udfGetWeeknumber(\"start_date\")) \\\n",
    "                .withColumn(\"lead_time\", datediff(flight3.start_date, flight3.search_date)) \n",
    "#                 .drop('start_date', 'search_date')\n",
    "\n",
    "flight3.show(2)\n",
    "display(flight2.count(), flight3.count())\n",
    "flight3.describe('lead_time').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flight3.show(5)\n",
    "flight3.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get distance to nearest holiday: http://stackoverflow.com/questions/40752378/spark-sql-distance-to-nearest-holiday\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# datetime.datetime('2017-09-01')\n",
    "# date1 = datetime.datetime.strptime(\"2015-01-30\", \"%Y-%m-%d\").strftime(\"%d-%m-%Y\")\n",
    "# print(date1)\n",
    "# start_date_test = flight3.select(\"start_date\").distinct()\n",
    "flight3 = flight3.withColumn(\"start_date_str\", flight3.start_date.cast('string'))\n",
    "\n",
    "holidays = ['2017-01-01', '2017-01-02', '2017-01-27', '2017-01-28',\n",
    "            '2017-01-29', '2017-01-30', '2017-01-31', '2017-02-01', '2017-02-02',\n",
    "            '2017-04-02', '2017-04-03', '2017-04-04',\n",
    "            '2017-05-01', '2017-05-28', '2017-05-29', '2017-05-30',\n",
    "            '2017-10-01', '2017-10-02', '2017-10-03', '2017-10-04', '217-10-05', '2017-10-06']\n",
    " \n",
    "\n",
    "index = spark.sparkContext.broadcast(sorted(holidays))\n",
    "\n",
    "def last_holiday(date):\n",
    "    last_holiday = index.value[0]\n",
    "    for next_holiday in index.value:\n",
    "        if next_holiday >= date:\n",
    "            break\n",
    "        last_holiday = next_holiday\n",
    "    if last_holiday > date:\n",
    "        last_holiday = None\n",
    "    if next_holiday < date:\n",
    "        next_holiday = None        \n",
    "    return last_holiday\n",
    "\n",
    "def next_holiday(date):\n",
    "    last_holiday = index.value[0]\n",
    "    for next_holiday in index.value:\n",
    "        if next_holiday >= date:\n",
    "            break\n",
    "        last_holiday = next_holiday\n",
    "    if last_holiday > date:\n",
    "        last_holiday = None\n",
    "    if next_holiday < date:\n",
    "        next_holiday = None        \n",
    "    return next_holiday\n",
    "\n",
    "\n",
    "# return_type = StructType([StructField('last_holiday', StringType()), StructField('next_holiday', StringType())])\n",
    "\n",
    "last_holiday_udf = udf(last_holiday, StringType())\n",
    "next_holiday_udf = udf(next_holiday, StringType())\n",
    "\n",
    "flight4 = flight3.withColumn('last_holiday', last_holiday_udf('start_date_str'))\n",
    "flight4 = flight4.withColumn('next_holiday', next_holiday_udf('start_date_str'))\n",
    "flight4 = flight4.withColumn('days_to_last_holiday', datediff('last_holiday', 'start_date_str'))\n",
    "flight4 = flight4.withColumn('days_to_next_holiday', datediff('next_holiday', 'start_date_str'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(flight4\n",
    "    .select('days_to_last_holiday', 'days_to_next_holiday', 'last_holiday', 'next_holiday', 'start_date')    \n",
    "    .distinct().show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# flight4.show()\n",
    "#'table_name', 'trip', 'stay_days'\n",
    "# flight5 = (flight4.filter((flight4.trip==1) & (flight4.start_date=='2017-10-01') & (flight4.company=='AirAsiaX'))\n",
    "#          .select('start_date', 'company', 'dep_time_local', 'stop_info', 'duration', 'search_date', 'price')\n",
    "#          .sort(F.desc('start_date'), 'company', 'dep_time_local', 'stop_info', 'duration', 'search_date'))       \n",
    "         \n",
    "flight5 = flight4.sort('table_name', 'trip', 'stay_days', \n",
    "                       'start_date', 'company', 'dep_time_local',\n",
    "                       'stop_info', 'duration', 'search_date')       \n",
    "    \n",
    "byVar = ['table_name', 'trip', 'stay_days', 'start_date', 'company', 'dep_time_local', 'stop_info', 'duration']\n",
    "\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "Threshold = 5\n",
    "\n",
    "w =  Window.partitionBy(byVar).orderBy('search_date').rowsBetween(0, sys.maxsize)\n",
    "flight6 = (flight5.withColumn('future_min_price', F.min(col('price')).over(w))\n",
    "          .withColumn('price_will_drop',                       \n",
    "                      (col('price') - col('future_min_price')) > Threshold ))\n",
    "flight6 = flight6.withColumn('price_will_drop_num', flight6.price_will_drop.cast('int'))\n",
    "flight6.cache()\n",
    "# min_price = (flight5\n",
    "#                   .groupBy(byVar)\n",
    "#                   .agg(F.min(col(\"price\"))).alias(\"min_price\"))\n",
    "\n",
    "# flight6 = flight5.join(min_price, on=byVar, how='left')\n",
    "# # flight6.select('price', 'min(price)').show(10)\n",
    "# # flight6.select('price', 'min(price)').show()\n",
    "# flight6 = flight6.withColumn('price_diff', col('price')-col('min(price)'))\n",
    "# flight6.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flight6.filter((flight6.trip==1) & \\\n",
    "               (flight6.start_date=='2017-10-01') & \\\n",
    "               (flight6.company=='AirAsiaX')) \\\n",
    "    .select('duration', 'search_date', 'price', 'future_min_price', 'price_will_drop') \\\n",
    "    .show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flight2.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flight6.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ['table_name', 'trip', 'stay_days', 'start_date', 'company', 'dep_time_local', 'stop_info', 'duration']\n",
    "# groupby(flight2.trip, flight2.stay_days, flight.dep_time_group).agg(func.mean('price')).show()\n",
    "# flight6.groupBy(col('trip'), col('stay_days'), col('lead_time')).agg(F.mean('price_will_drop_num')).show()\n",
    "flight6.groupBy(col('trip'), col('stay_days'), col('lead_time')). \\\n",
    "    agg(F.mean('price_will_drop_num')).orderBy(['trip', 'stay_days', 'lead_time'], ascending=[1, 1, 0]).show(1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save to parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flight6.coalesce(2).write.parquet(\"C:\\\\s3\\\\20170503_jsonl\\\\flight6.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# flight6 = spark.read.parquet(\"C:\\\\s3\\\\20170503_jsonl\\\\flight6.parquet\")\n",
    "flight6 = spark.read.parquet(\"/home/ubuntu/parquet/flight6.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flight6.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flight6.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flight7 = (flight6.withColumn('price_drop_amt', col('price') - col('future_min_price'))\n",
    "                  .withColumn('stop_minutes', col('stop1_minutes') + col('stop2_minutes'))\n",
    "                  .withColumn('few_tickets_left', col('ticket_left') != '0')\n",
    "                  .select('price_will_drop_num', 'price', 'few_tickets_left', 'from_city_name', 'to_city_name', 'trip', 'stay_days',\n",
    "                          'company', 'flight_code', 'plane', 'span_days', 'stop',\n",
    "                         'power', 'video', 'wifi', 'check_bag_not_inc', \n",
    "                         'dep_time_group', 'arr_time_group',  \n",
    "                         'lead_time', 'dep_weekday', 'dep_weeknum', \n",
    "                         'days_to_last_holiday', 'days_to_next_holiday',                         \n",
    "                         'duration_minutes', 'stop_minutes'\n",
    "                      ).filter(flight6.price > 0))\n",
    "flight7.dtypes\n",
    "flight7 = flight7.na.fill({'stop_minutes': 0, 'days_to_next_holiday': 999})\n",
    "flight7.show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to transform categorical variables? See\n",
    "http://stackoverflow.com/questions/32982425/encode-and-assemble-multiple-features-in-pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# One hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "column_vec_in = ['from_city_name', 'to_city_name', 'trip', 'company', 'flight_code', 'plane', \n",
    "                 'arr_time_group', 'dep_time_group', 'dep_weekday']\n",
    "column_vec_out = ['from_city_name_catVec','to_city_name_catVec', 'trip_catVec',\n",
    "                'company_catVec', 'flight_code_catVec', 'plane_catVec', \n",
    "                  'arr_time_group_catVec', 'dep_time_group_catVec', 'dep_weekday_catVec']\n",
    " \n",
    "indexers = [StringIndexer(inputCol=x, outputCol=x+'_tmp') for x in column_vec_in ]\n",
    " \n",
    "encoders = [OneHotEncoder(dropLast=False, inputCol=x+\"_tmp\", outputCol=y)\n",
    "            for x,y in zip(column_vec_in, column_vec_out)]\n",
    "\n",
    "tmp = [[i,j] for i,j in zip(indexers, encoders)]\n",
    "tmp = [i for sublist in tmp for i in sublist]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer, VectorAssembler, SQLTransformer\n",
    "\n",
    "# prepare labeled sets\n",
    "# 'price_will_drop',\n",
    "\n",
    "cols_now = ['price', 'few_tickets_left', 'stay_days',\n",
    "              'span_days', 'stop',\n",
    "             'power', 'video', 'wifi', 'check_bag_not_inc',                          \n",
    "             'lead_time', 'dep_weeknum', \n",
    "             'days_to_last_holiday', 'days_to_next_holiday',                         \n",
    "             'duration_minutes', 'stop_minutes',\n",
    "            'from_city_name_catVec','to_city_name_catVec', 'trip_catVec',\n",
    "            'company_catVec', 'flight_code_catVec', 'plane_catVec', \n",
    "            'arr_time_group_catVec', 'dep_time_group_catVec', 'dep_weekday_catVec']\n",
    "\n",
    "assembler_features = VectorAssembler(inputCols=cols_now, outputCol='features')\n",
    "# labelIndexer = StringIndexer(inputCol='price', outputCol=\"label\")\n",
    "# tmp += [assembler_features, labelIndexer]\n",
    "tmp += [assembler_features]\n",
    "pipeline = Pipeline(stages=tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# flight3.na.drop().count()\n",
    "\n",
    "# from pyspark.ml import Pipeline\n",
    "# from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# indexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\")\\\n",
    "#             .fit(flight3) for column in list(set(flight3.columns)-\\\n",
    "#                                              set(['stay_days', 'power', 'price', 'span_days',\\\n",
    "#                                                   'stop', 'video', 'wifi', 'duration_minutes',\\\n",
    "#                                                  'dep_weeknum', 'lead_time']))]\n",
    "\n",
    "# pipeline = Pipeline(stages=indexers)\n",
    "# flight4 = pipeline.fit(flight3).transform(flight3)\n",
    "\n",
    "# flight4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cols = ['from_city_name', 'to_city_name', 'trip', 'company', 'flight_code', 'plane', \n",
    "                 'arr_time_group', 'dep_time_group', 'dep_weekday']\n",
    "\n",
    "for c in cols:\n",
    "    flight7.groupBy(col(c)).count().show(50000)\n",
    "    \n",
    "    \n",
    "flight7 = flight7.na.replace('', 'unknown', 'plane')\n",
    "\n",
    "# http://stackoverflow.com/questions/40711229/pyspark-ml-error-urequirement-failed-cannot-have-an-empty-string-for-name\n",
    "# replacements = {\n",
    "#   'plane': 'unknown', 'another_col': 'another_replacement',\n",
    "#   'numeric_column_wont_be_replaced': 1.0\n",
    "# }\n",
    "\n",
    "# for k, v in replacements.items():\n",
    "#     # We can replace string only if target is string\n",
    "#     # In Python 2 str -> basestring\n",
    "#     if isinstance(v, str):\n",
    "#         df = df.na.replace(\"\", v, [k])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cols = ['price_will_drop', 'price', 'few_tickets_left', 'from_city_name', 'to_city_name', 'trip', 'stay_days',\n",
    "                          'company', 'flight_code', 'plane', 'span_days', 'stop',\n",
    "                         'power', 'video', 'wifi', 'check_bag_not_inc', \n",
    "                         'dep_time_group', 'arr_time_group',  \n",
    "                         'lead_time', 'dep_weekday', 'dep_weeknum', \n",
    "                         'days_to_last_holiday', 'days_to_next_holiday',                         \n",
    "                         'duration_minutes', 'stop_minutes']\n",
    "\n",
    "for c in cols:\n",
    "    display(c + \" :\" + str(flight7.where(col(c).isNull()).count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flight7.show(2)\n",
    "flight7.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# flight7 = flight7.na.drop()\n",
    "output = pipeline.fit(flight7).transform(flight7)\n",
    "output = output.withColumnRenamed('price_will_drop_num', 'label')\n",
    "output.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# flight4.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# flight4.select('stay_days',\n",
    "#  'power', \n",
    "#  'span_days',\n",
    "#  'stop',\n",
    "#  'video',\n",
    "#  'wifi',\n",
    "#  'duration_minutes',\n",
    "#  'dep_weeknum',\n",
    "#  'lead_time',\n",
    "#  'trip_index',\n",
    "#  'arr_time_group_index',\n",
    "#  'to_city_name_index',\n",
    "#  'ticket_left_index',\n",
    "#  'company_index',\n",
    "#  'from_city_name_index',\n",
    "#  'airline_code_index',\n",
    "#  'plane_index',\n",
    "#  'check_bag_not_inc_index',\n",
    "#  'dep_weekday_index',\n",
    "#  'dep_time_group_index').dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from pyspark.ml.linalg import Vectors\n",
    "# from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# # from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# # ignore = ['id', 'label', 'binomial_label']\n",
    "# # assembler = VectorAssembler(\n",
    "# #     inputCols=[x for x in df.columns if x not in ignore],\n",
    "# #     outputCol='features')\n",
    "\n",
    "# # assembler.transform(df)\n",
    "# # arr_time_group\n",
    "\n",
    "# assembler = VectorAssembler(    \n",
    "#     inputCols=[\n",
    "#  'stay_days',\n",
    "#  'power', \n",
    "#  'span_days',\n",
    "#  'stop',\n",
    "#  'video',\n",
    "#  'wifi',\n",
    "#  'duration_minutes', \n",
    "#  'dep_weeknum',\n",
    "#  'lead_time',\n",
    "#  'trip_index',\n",
    "#  'arr_time_group_index',\n",
    "#  'to_city_name_index',\n",
    "#  'ticket_left_index',\n",
    "#  'company_index',\n",
    "#  'from_city_name_index',\n",
    "#  'airline_code_index',\n",
    "#  'plane_index',\n",
    "#  'check_bag_not_inc_index',\n",
    "#  'dep_weekday_index',\n",
    "#  'dep_time_group_index'],\n",
    "#     outputCol=\"features\")\n",
    "\n",
    "# output = assembler.transform(flight4)\n",
    "# # print(\"Assembled columns 'hour', 'mobile', 'userFeatures' to vector column 'features'\")\n",
    "# # output.select(\"features\", \"clicked\").show(truncate=False)\n",
    "# output = output.withColumnRenamed('price', 'label')\n",
    "# output.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display(output.count(), flight6.count(), flight7.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# output.coalesce(2).write.parquet(\"C:\\\\s3\\\\20170503_jsonl\\\\output.parquet\")\n",
    "output.coalesce(2).write.parquet(\"/home/ubuntu/parquet/output.parquet/output.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output = spark.read.parquet(\"/home/ubuntu/parquet/output.parquet/output.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output = output.sample(False, 0.1, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output.count()\n",
    "output.printSchema()\n",
    "output.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# interlude to model in h2o (not sparkling water)  \n",
    "When converting the whole dataset to pandas dataframe, there was not enough RAM, JAVA was forced to shutdown!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output.printSchema()\n",
    "output2 = output.select('label', 'features')\n",
    "# output2 = output.select('label', 'features').rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# output2.count()\n",
    "flight_pd = output.select('label', 'price', 'few_tickets_left', \n",
    "                          'from_city_name', 'to_city_name', 'trip', 'stay_days',\n",
    "                          'company', 'flight_code', 'plane', 'span_days', 'stop',\n",
    "                         'power', 'video', 'wifi', 'check_bag_not_inc', \n",
    "                         'dep_time_group', 'arr_time_group',  \n",
    "                         'lead_time', 'dep_weekday', 'dep_weeknum', \n",
    "                         'days_to_last_holiday', 'days_to_next_holiday',                         \n",
    "                         'duration_minutes', 'stop_minutes').toPandas()\n",
    "flight_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# flight_pd.to_csv('C:\\\\s3\\\\20170503_jsonl\\\\flight_pd.csv', sep='\\t')\n",
    "flight_pd.to_csv('/home/ubuntu/parquet/flight_pd.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h2o.init()\n",
    "# from h2o.estimators.gbm import H2OGradientBoostingEstimator\n",
    "\n",
    "# hf = h2o.H2OFrame(flight_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "flight_pd=pd.read_csv(\"/home/ubuntu/parquet/flight_pd.csv\", delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# flight_pd.head()\n",
    "# help(pd.read_csv)\n",
    "flight_hex = h2o.H2OFrame(flight_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, valid, test = flight_hex.split_frame([0.6, 0.2], seed=1234)\n",
    "\n",
    "flight_X = flight_hex.col_names[2:]\n",
    "flight_y = flight_hex.col_names[1]    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from h2o.estimators.gbm import H2OGradientBoostingEstimator\n",
    "from h2o.estimators.random_forest import H2ORandomForestEstimator\n",
    "\n",
    "rf_v1 = H2ORandomForestEstimator(\n",
    "    model_id=\"rf_v1\",\n",
    "    ntrees=200,\n",
    "    stopping_rounds=2,\n",
    "    score_each_iteration=True,\n",
    "    seed=1000000)\n",
    "\n",
    "rf_v1.train(flight_X, flight_y, training_frame=train, validation_frame=valid)\n",
    "rf_v1\n",
    "rf_v1.score_history()\n",
    "model_path = h2o.save_model(model=rf_v1, path=\"C:\\\\s3\\\\20170503_jsonl\", force=True)\n",
    "print(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rf_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# output2.take(2)\n",
    "(trainingData, testData) = output2.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stopped here! 20170513"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Load training data\n",
    "# training = spark.read.format(\"libsvm\").load(\"data/mllib/sample_libsvm_data.txt\")\n",
    "\n",
    "lr = LogisticRegression(maxIter=100, regParam=0.1, elasticNetParam=0.8)\n",
    "\n",
    "# Fit the model\n",
    "t0 = time()\n",
    "lrModel = lr.fit(trainingData)\n",
    "tt = time() - t0\n",
    "\n",
    "# Print the coefficients and intercept for logistic regression\n",
    "print(\"Classifier trained in \" + str(tt) + \" seconds.\")\n",
    "print(\"Coefficients: \" + str(lrModel.coefficients))\n",
    "print(\"Intercept: \" + str(lrModel.intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Extract the summary from the returned LogisticRegressionModel instance trained\n",
    "# in the earlier example\n",
    "trainingSummary = lrModel.summary\n",
    "\n",
    "# Obtain the objective per iteration\n",
    "objectiveHistory = trainingSummary.objectiveHistory\n",
    "print(\"objectiveHistory:\")\n",
    "for objective in objectiveHistory:\n",
    "    print(objective)\n",
    "\n",
    "# Obtain the receiver-operating characteristic as a dataframe and areaUnderROC.\n",
    "trainingSummary.roc.show()\n",
    "print(\"areaUnderROC: \" + str(trainingSummary.areaUnderROC))\n",
    "\n",
    "# Set the model threshold to maximize F-Measure\n",
    "fMeasure = trainingSummary.fMeasureByThreshold\n",
    "maxFMeasure = fMeasure.groupBy().max('F-Measure').select('max(F-Measure)').head()\n",
    "bestThreshold = fMeasure.where(fMeasure['F-Measure'] == maxFMeasure['max(F-Measure)']) \\\n",
    "    .select('threshold').head()['threshold']\n",
    "lr.setThreshold(bestThreshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make predictions.\n",
    "predictions = lrModel.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "predictions.select('label', 'prediction').coalesce(1).write.csv('D://Data Science//pySpark//check_pred7.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "# from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "# from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Load and parse the data file, converting it to a DataFrame.\n",
    "# data = spark.read.format(\"libsvm\").load(\"data/mllib/sample_libsvm_data.txt\")\n",
    "\n",
    "# Automatically identify categorical features, and index them.\n",
    "# Set maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "featureIndexer =\\\n",
    "    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=100).fit(output)\n",
    "    \n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = output.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train a RandomForest model.\n",
    "# rf = RandomForestRegressor(featuresCol=\"indexedFeatures\", numTrees=1000, featureSubsetStrategy=\"auto\",\n",
    "#                                     impurity='variance', maxDepth=4, maxBins=32)\n",
    "gbt = GBTClassifier(labelCol=\"label\", featuresCol=\"indexedFeatures\", maxIter=10)\n",
    "\n",
    "# Chain indexer and forest in a Pipeline\n",
    "pipeline = Pipeline(stages=[featureIndexer, gbt])\n",
    "\n",
    "# Train model.  This also runs the indexer.\n",
    "model = pipeline.fit(trainingData)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "# evaluator = RegressionEvaluator(\n",
    "#     labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\")\n",
    "auc = evaluator.evaluate(predictions)\n",
    "print(\"Area under the curve (AUC) on test data = %g\" % auc)\n",
    "\n",
    "gbtModel = model.stages[1]\n",
    "print(gbtModel)  # summary only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Load training data\n",
    "# training = spark.read.format(\"libsvm\").load(\"data/mllib/sample_libsvm_data.txt\")\n",
    "flight7.withColumnRenamed('price_will_drop_num', 'label')\n",
    "\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "# Fit the model\n",
    "lrModel = lr.fit(training)\n",
    "\n",
    "# Print the coefficients and intercept for logistic regression\n",
    "print(\"Coefficients: \" + str(lrModel.coefficients))\n",
    "print(\"Intercept: \" + str(lrModel.intercept))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "how to score model manually: http://stackoverflow.com/questions/35731140/apply-model-scores-to-spark-dataframe-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions.select('label', 'prediction').coalesce(1).write.csv('D://Data Science//pySpark//check_pred6.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Load and parse the data file, converting it to a DataFrame.\n",
    "data = flight3\n",
    "\n",
    "# Automatically identify categorical features, and index them.\n",
    "# Set maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "# featureIndexer =\\\n",
    "#     VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(data)\n",
    "\n",
    "\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = data.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train a GBT model.\n",
    "gbt = GBTRegressor(featuresCol=\"indexedFeatures\", maxIter=10)\n",
    "\n",
    "# Chain indexer and GBT in a Pipeline\n",
    "pipeline = Pipeline(stages=[featureIndexer, gbt])\n",
    "\n",
    "# Train model.  This also runs the indexer.\n",
    "model = pipeline.fit(trainingData)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
    "\n",
    "gbtModel = model.stages[1]\n",
    "print(gbtModel)  # summary only\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# # Load and parse the data file, converting it to a DataFrame.\n",
    "# data = spark.read.format(\"libsvm\").load(\"data/mllib/sample_libsvm_data.txt\")\n",
    "\n",
    "# # Automatically identify categorical features, and index them.\n",
    "# # Set maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "featureIndexer =\\\n",
    "    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(flight3)\n",
    "\n",
    "# # Split the data into training and test sets (30% held out for testing)\n",
    "# (trainingData, testData) = data.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train a GBT model.\n",
    "gbt = GBTRegressor(featuresCol=\"indexedFeatures\", maxIter=10)\n",
    "\n",
    "# Chain indexer and GBT in a Pipeline\n",
    "pipeline = Pipeline(stages=[featureIndexer, gbt])\n",
    "\n",
    "# Train model.  This also runs the indexer.\n",
    "model = pipeline.fit(trainingData)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
    "\n",
    "gbtModel = model.stages[1]\n",
    "print(gbtModel)  # summary only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# display(flight.select(\"duration\").show())\n",
    "# display(flight2.select(\"duration_h\", 'duration_m').show())\n",
    "display(flight2.show(5))\n",
    "# flight2.toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def getHours(x):\n",
    "  return re.match('([0-9]+(?=h))', x)\n",
    "temp = flight.select(\"duration\").rdd.map(lambda x:getHours(x[0])).toDF()\n",
    "temp.select(\"duration\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flight.select(\"arr_time\").show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# flight2 = (flight.withColumn('arr_time_local', flight.arr_time.substr(1, 23).cast('date'))                 \n",
    "#           )\n",
    "\n",
    "# flight3 = flight2.withColumn('arr_time', flight2.arr_time.cast('timestamp'))\n",
    "\n",
    "# .select('arr_time').show(2, truncate=False)\n",
    "\n",
    "\n",
    "# flight2 = (flight.withColumn('duration_minutes', flight.duration.substr(1, 23).cast('date'))                 \n",
    "#           )\n",
    "\n",
    "# flight.selectExpr(\"duration\", \"regexp_extract(duration,'([0-9]+(?=h))', 1) as duration_hour\", \"regexp_extract(duration,'([0-9]+(?=m))', 1)\", ).show()\n",
    "flight.selectExpr(\"regexp_extract(duration,'([0-9]+(?=h))', 1) as duration_hour\").duration_hour.cast(\"double\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flight2.select('arr_time', 'arr_time_local').dtypes\n",
    "flight2.select('arr_time', 'arr_time_local').show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql import crosstab\n",
    "# from pyspark.sql.functions import *\n",
    "\n",
    "flight.crosstab('start_date','from_city_name').show()\n",
    "# flight.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "3\n",
    "down vote\n",
    "In order to simply cast a string column to a timestamp, the string column must be properly formatted.\n",
    "\n",
    "To retrieve the \"createdAt\" column as a timestamp, you can write the UDF function that would convert the string\n",
    "\n",
    "\"2016-07-01T16:37:41-0400\"\n",
    "to\n",
    "\n",
    "\"2016-07-01 16:37:41\"\n",
    "and convert the \"createdAt\" column to a new format (don't forget to handle the timezone field).\n",
    "\n",
    "Once you have a column containing timestamps as strings like \"2016-07-01 16:37:41\", a simple cast to timestamp would do the job, as you have it in your code.\n",
    "\n",
    "You can read more about Date/Time/String Handling in Spark here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "display(flight.select(\"arr_time\").take(3))\n",
    "display(flight2.select(\"arr_time\").take(3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flight2.withColumn('local', F.from_utc_timestamp(flight2.arr_time, \"AEST\")).select(\"local\").show()\n",
    "flight2.withColumn('local', F.from_utc_timestamp(flight2.arr_time, \"CTZ\")).select(\"local\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import pytz # $ pip install pytz\n",
    "from tzlocal import get_localzone # $ pip install tzlocal\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get local timezone    \n",
    "local_tz = get_localzone() \n",
    "print(local_tz)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test it\n",
    "# utc_now, now = datetime.utcnow(), datetime.now()\n",
    "ts = time.time()\n",
    "utc_now, now = datetime.utcfromtimestamp(ts), datetime.fromtimestamp(ts)\n",
    "print(utc_now)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "local_now = utc_now.replace(tzinfo=pytz.utc).astimezone(local_tz) # utc -> local\n",
    "print(local_now)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert local_now.replace(tzinfo=None) == now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test it\n",
    "# utc_now, now = datetime.utcnow(), datetime.now()\n",
    "ts = time.time()\n",
    "utc_now, now = datetime.utcfromtimestamp(ts), datetime.fromtimestamp(ts)\n",
    "\n",
    "local_now = utc_now.replace(tzinfo=pytz.utc).astimezone(local_tz) # utc -> local\n",
    "assert local_now.replace(tzinfo=None) == now\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Spark-Sql doesn't support date-time, and nor timezones\n",
    "* Using timestamp is the only solution\n",
    "* from_unixtime(at) parses the epoch time correctly, just that the printing of it as a string changes it due to timezone. It is safe to assume that the  from_unixtime will convert it correctly ( although printing it might show different results)\n",
    "* from_utc_timestamp will shift ( not just convert) the timestamp to that timezone, in this case it will subtract 8 hours to the time since (-08:00)\n",
    "* printing sql results messes up the times with respect to timezone param\n",
    "  \t \t\n",
    "* from_unixtime(at) does what from_utc_timestamp does too, it will parse a Unix timestamp integer (seconds since midnight 1970-01-01), and convert the time instant parsed from UTC to the system's default timezone. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flight2.selectExpr(\"from_utc_timestamp(arr_time, 'AEST') as testthis\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flight2.dtypes\n",
    "display(flight2.select('arr_time').take(3))\n",
    "display(flight.select('arr_time').take(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, month, dayofmonth, hour, minute, weekofyear, crosstab\n",
    "\n",
    "flight2.select(year(\"arr_time\").alias('year'), \n",
    "               month(\"arr_time\").alias('month'),\n",
    "               dayofmonth(\"arr_time\").alias('day'),\n",
    "               hour(\"arr_time\").alias('hour'),\n",
    "               minute(\"arr_time\").alias('minute'),\n",
    "               weekofyear('arr_time').alias('week_no'),\n",
    "               \"arr_time_zone\"\n",
    "              ).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_extract, col\n",
    "\n",
    "#flight.select(regexp_extract('arr_time', r'[+-][0-9]{2}:[0-9]{2}\\b', 1)).alias('d').collect\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([('2017-04-09T07:15:00.000+08:00',)], ['str'])\n",
    "df.select(regexp_extract('str', '(\\d+)-(\\d+)', 1).alias('d')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flight2.describe('price').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "# flight2.dtypes\n",
    "# flight2.first()\n",
    "display(flight2.select(max(\"start_date\")).show())\n",
    "display(flight2.select(min(\"start_date\")).show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flight2 = flight2.withColumn('zero_price', flight2.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# flight.describe().show()\n",
    "flight.describe(\"from_city_name\").show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import colflight.filter(flight.price > 0).groupby(flight.search_date_x, flight.from_city_name, flight.to_city_name).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "# flight_to_brisbane = flight.where(col(\"price\") > 0 & col(\"to_city_name\") == \"brisbane\").groupby(flight.search_date_x).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.where(col(\"v\").isin({\"foo\", \"bar\"})).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numeric = sqlContext.createDataFrame([\n",
    "    ('3.5,', '5.0', 'null'), ('2.0', '14.0', 'null'),  ('null', '38.0', 'null'),\n",
    "    ('null', 'null', 'null'),  ('1.0', 'null', '4.0')],\n",
    "    ('low', 'high', 'normal'))\n",
    "\n",
    "numeric_filtered_1 = numeric.where(numeric['LOW'] != 'null')\n",
    "numeric_filtered_1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flight.select('from_city_name', 'price').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "h2o tutorial \n",
    "https://github.com/h2oai/h2o-tutorials/blob/master/tutorials/pysparkling/Chicago_Crime_Demo.md"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

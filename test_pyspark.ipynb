{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resources list**\n",
    "1. pyspark tutorial https://github.com/pydatasg/Pydata_meetup_Nov_16/blob/master/RF_modeling_2.3_business_weimin.ipynb\n",
    "2. follow this tutorial: https://www.analyticsvidhya.com/blog/2016/10/spark-dataframe-and-operations/\n",
    "3. and this one: https://www.dezyre.com/apache-spark-tutorial/pyspark-tutorial\n",
    "4. data engineer trick: http://nadbordrozd.github.io/blog/2016/05/22/one-weird-trick-that-will-fix-your-pyspark-schemas/\n",
    "5. git http://rogerdudler.github.io/git-guide/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get data\n",
    "## Downloand data from s3\n",
    "version 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! cd /home/ubuntu/s3/flight_1_5\n",
    "\n",
    "! aws s3 sync s3://flight.price/flight_1_5 . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "version 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! aws s3 ls s3://flight.price.11/flight_1_5 --recursive\n",
    "! aws s3 sync s3://flight.price.11/flight_1_5 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! du -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract\n",
    "Note: version 1.1 from 2017-05-11 onwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "# dir_in = 'C:\\\\s3\\\\20170503\\\\flight_1_6' # sydney to shanghai\n",
    "# dir_in = 'C:\\\\s3\\\\20170503\\\\flight_5_1' # beijing to sydney\n",
    "# dir_in = 'C:\\\\s3\\\\20170503'\n",
    "dir_in = '/home/ubuntu/s3/flight_1_5'\n",
    "\n",
    "# dir_out = 'C:\\\\s3\\\\20170503_extracted\\\\flight_1_6'\n",
    "dir_out = '/home/ubuntu/s3/flight_1_5/extracted'\n",
    "extension = \".zip\"\n",
    "\n",
    "os.chdir(dir_in) # change directory from working dir to dir with files\n",
    "\n",
    "for subdir, dirs, files in os.walk(dir_in):\n",
    "    for item in files:\n",
    "        if item.endswith(extension): # check for \".zip\" extension\n",
    "            file_name = os.path.join(subdir, item)\n",
    "#             file_name = os.path.abspath(item) # get full path of files\n",
    "            zip_ref = zipfile.ZipFile(file_name) # create zipfile object\n",
    "            zip_ref.extractall(dir_out) # extract file to dir\n",
    "            zip_ref.close() # close file  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move files in final_results to the main folder and delete the folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ! cd /home/ubuntu/s3/flight_1_5/extracted\n",
    "# ! mv /home/ubuntu/s3/flight_1_5/extracted/final_results/*.txt .\n",
    "! find /home/ubuntu/s3/flight_1_5/extracted/final_results -name '*.txt' -exec mv {} /home/ubuntu/s3/flight_1_5/extracted \\;\n",
    "# -exec runs any command,  {} inserts the filename found, \\; marks the end of the exec command.\n",
    "! rmdir /home/ubuntu/s3/flight_1_5/extracted/final_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://stackoverflow.com/questions/9157138/recursively-counting-files-in-a-linux-directory\n",
    "* -type f to include only files.  \n",
    "* | (and not Â¦) redirects find command's standard output to wc command's standard input.  \n",
    "* wc (short for word count) counts newlines, words and bytes on its input (docs).  \n",
    "* -l to count just newlines.\n",
    "* You can also remove the -type f to include directories (and symlinks) in the count.\n",
    "* It's possible this command will overcount if filenames can contain newline characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! find /home/ubuntu/s3/flight_1_5/extracted -type f | wc -l\n",
    "! du -sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Launch spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at <ipython-input-1-d9e30c3c1dcd>:25 ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-d9e30c3c1dcd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;31m# # # spark_home = os.environ.get('SPARK_HOME', None)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[0;32m    113\u001b[0m         \"\"\"\n\u001b[0;32m    114\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callsite\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfirst_spark_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mCallSite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 115\u001b[1;33m         \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    116\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[1;32m/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    273\u001b[0m                         \u001b[1;34m\" created by %s at %s:%s \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    274\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[1;32m--> 275\u001b[1;33m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[0;32m    276\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    277\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at <ipython-input-1-d9e30c3c1dcd>:25 "
     ]
    }
   ],
   "source": [
    "import h2o\n",
    "# import pysparkling\n",
    "import zipfile\n",
    "import os\n",
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "from IPython.display import display\n",
    "from pyspark.sql.functions import regexp_extract, col, split, udf, trim, when, from_unixtime, unix_timestamp, minute, hour, datediff\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import IntegerType, StringType, BooleanType\n",
    "# from pyspark.sql.types import *\n",
    "import datetime\n",
    "import argparse\n",
    "import json\n",
    "import glob, os, shutil\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize\n",
    "\n",
    "pd.options.display.max_columns = 99\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext()\n",
    "\n",
    "# # # spark_home = os.environ.get('SPARK_HOME', None)\n",
    "# spark_home = \"C:\\spark-2.1.0-bin-hadoop2.7\\spark-2.1.0-bin-hadoop2.7\"\n",
    "\n",
    "# if not spark_home:\n",
    "\n",
    "#     raise ValueError('SPARK_HOME environment variable is not set')\n",
    "\n",
    "# sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "\n",
    "# sys.path.insert(0, os.path.join(spark_home, 'C:\\spark-2.1.0-bin-hadoop2.7\\spark-2.1.0-bin-hadoop2.7\\python\\lib\\py4j-0.10.4-src.zip')) ## may need to adjust on your system depending on which Spark version you're using and where you installed it.\n",
    "\n",
    "# exec(open(os.path.join(spark_home, 'python/pyspark/shell.py')).read())\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"Python Spark SQL basic example\") \\\n",
    "        .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "        .getOrCreate()\n",
    "        \n",
    "display(spark.version)\n",
    "\n",
    "flight = spark.read.parquet(\"/home/ubuntu/s3/comb/flight_v1_0.pq\")\n",
    "# display(flight.count())\n",
    "# dsiplay(flight.take(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# hc= H2OContext(sc).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# h2o not working yet\n",
    "# # Start H2O Context\n",
    "# from pysparkling import *\n",
    "# sc\n",
    "# hc= H2OContext(sc).start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in json files and aggregate into a dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy version 1.1 files to a separate folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mkfldr(fldr):\n",
    "    try:\n",
    "      os.makedirs(fldr)\n",
    "    except:\n",
    "      print(\"Folder already exist or some error\")\n",
    "    \n",
    "def read_from_json_file(filename):\n",
    "    with open(filename,'r') as f:\n",
    "        d = json.load(f)   \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "src_fldr = \"/home/ubuntu/s3/flight_1_5/extracted\"\n",
    "dst_fldr = \"/home/ubuntu/s3/flight_1_5/extracted11\"\n",
    "\n",
    "mkfldr(dst_fldr)\n",
    "\n",
    "for txt_file in glob.glob(os.path.join(parent_dir, 'flight_1_5_price_2017-05-1[1-4]*.txt')):\n",
    "    shutil.move(txt_file, dst_fldr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Append multiple json files to a single jsonl file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! find -type f -name \"flight_1_5_price_2017-05-10_2017-06-10*.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "comb_fldr = '/home/ubuntu/s3/comb'\n",
    "mkfldr(comb_fldr)\n",
    "\n",
    "pq_fldr = '/home/ubuntu/s3/pq'\n",
    "mkfldr(pq_fldr)\n",
    "\n",
    "os.chdir('/home/ubuntu/s3/flight_1_5/extracted')\n",
    "\n",
    "# json_file_name = 'flight_1_5_price_2017-05-10_2017-11-06_2_7.txt'\n",
    "# pq_file_name = os.path.join(pq_fldr, json_file_name.replace('.jsonl','.parquet'))\n",
    "\n",
    "json_file_name = 'flight_1_5_price_2017-05-10_2017-06-10*.txt'\n",
    "jsonl_file_name = os.path.join(comb_fldr, json_file_name.replace('.txt','.jsonl'))\n",
    "\n",
    "# 'flight_1_5_price_2017-05-[8|9|10]*.txt'\n",
    "\n",
    "with open(jsonl_file_name, 'w') as outfile:\n",
    "    for file in glob.glob(json_file_name):                \n",
    "        d = read_from_json_file(file)                \n",
    "        json.dump(d, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in jsonl file, flatten it, and save as a new flat jsonl file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set parquet schema and create empty parquet file to append to\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def chunkIt(seq, num):\n",
    "  avg = len(seq) / float(num)\n",
    "  index_start = []\n",
    "  index_end = []\n",
    "  last = 0.0\n",
    "\n",
    "  while last < len(seq):\n",
    "#     out.append(seq[int(last):int(last + avg)])\n",
    "    index_start.append(int(last))\n",
    "    index_end.append(int(last + avg))\n",
    "    last += avg\n",
    "\n",
    "  return list(zip(index_start, index_end))\n",
    "\n",
    "# files = [os.path.join(ext_fldr, \"flight_1_5_price_2017-04-06_2017-04-07_1_0.txt\"), \n",
    "#         os.path.join(ext_fldr, \"flight_1_5_price_2017-04-06_2017-04-07_2_7.txt\")]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set chunk size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22665\n",
      "[(0, 226), (226, 453), (453, 679), (679, 906), (906, 1133), (1133, 1359), (1359, 1586), (1586, 1813), (1813, 2039), (2039, 2266), (2266, 2493), (2493, 2719), (2719, 2946), (2946, 3173), (3173, 3399), (3399, 3626), (3626, 3853), (3853, 4079), (4079, 4306), (4306, 4533), (4533, 4759), (4759, 4986), (4986, 5212), (5212, 5439), (5439, 5666), (5666, 5892), (5892, 6119), (6119, 6346), (6346, 6572), (6572, 6799), (6799, 7026), (7026, 7252), (7252, 7479), (7479, 7706), (7706, 7932), (7932, 8159), (8159, 8386), (8386, 8612), (8612, 8839), (8839, 9065), (9065, 9292), (9292, 9519), (9519, 9745), (9745, 9972), (9972, 10199), (10199, 10425), (10425, 10652), (10652, 10879), (10879, 11105), (11105, 11332), (11332, 11559), (11559, 11785), (11785, 12012), (12012, 12239), (12239, 12465), (12465, 12692), (12692, 12919), (12919, 13145), (13145, 13372), (13372, 13598), (13598, 13825), (13825, 14052), (14052, 14278), (14278, 14505), (14505, 14732), (14732, 14958), (14958, 15185), (15185, 15412), (15412, 15638), (15638, 15865), (15865, 16092), (16092, 16318), (16318, 16545), (16545, 16772), (16772, 16998), (16998, 17225), (17225, 17452), (17452, 17678), (17678, 17905), (17905, 18131), (18131, 18358), (18358, 18585), (18585, 18811), (18811, 19038), (19038, 19265), (19265, 19491), (19491, 19718), (19718, 19945), (19945, 20171), (20171, 20398), (20398, 20625), (20625, 20851), (20851, 21078), (21078, 21305), (21305, 21531), (21531, 21758), (21758, 21985), (21985, 22211), (22211, 22438), (22438, 22665)]\n",
      "226\n"
     ]
    }
   ],
   "source": [
    "ext_fldr = '/home/ubuntu/s3/flight_1_5/extracted'\n",
    "file_count = len(glob.glob(os.path.join(ext_fldr, \"*.txt\")))\n",
    "range_temp = chunkIt(range(file_count), 100)\n",
    "\n",
    "print(file_count)\n",
    "print(range_temp)\n",
    "print(range_temp[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create empty parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/s3/flight_1_5/extracted/flight_1_5_price_2017-04-23_2017-04-30_2_21.txt completed. Count: 1\n"
     ]
    }
   ],
   "source": [
    "flat_table_list = []  # a list\n",
    "\n",
    "ext_fldr = '/home/ubuntu/s3/flight_1_5/extracted'\n",
    "comb_fldr = '/home/ubuntu/s3/comb'\n",
    "# os.chdir(comb_fldr)\n",
    "# file_list = os.listdir(comb_fldr)\n",
    "\n",
    "aggDF = pd.DataFrame() #aggregate dataframe to hold all individual dataframes\n",
    "count = 0\n",
    "\n",
    "for file in glob.glob(os.path.join(ext_fldr, \"*.txt\"))[0:1]:\n",
    "    with open(file) as f:               \n",
    "        flat_table = []  # a list\n",
    "        d = json.load(f)\n",
    "        flight_list = json_normalize(d['flight_list'])\n",
    "        basic = json_normalize(d['basic'])\n",
    "        \n",
    "        # create dataframe\n",
    "        basic = basic.drop('search_date', 1)\n",
    "        basic['tmp'] = 1\n",
    "        flight_list = flight_list.drop('id', 1)\n",
    "        flight_list['tmp'] = 1\n",
    "        DF = pd.merge(basic, flight_list, on=['tmp'])\n",
    "        DF=DF.drop('tmp', 1)\n",
    "        \n",
    "        # append\n",
    "        aggDF = aggDF.append(DF)\n",
    "        \n",
    "        count = count + 1\n",
    "        \n",
    "        print(file+\" completed. Count: \" + str(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(114, 29)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggDF.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['from_city_name', 'start_date', 'stay_days', 'table_name', 'task_id',\n",
       "       'to_city_name', 'trip', 'version', 'airline_code', 'airline_codes',\n",
       "       'arr_time', 'check_bag_inc', 'company', 'dep_time', 'duration',\n",
       "       'flight_code', 'flight_number', 'index', 'plane', 'power', 'price',\n",
       "       'price_code', 'search_date', 'span_days', 'stop', 'stop_info',\n",
       "       'ticket_left', 'video', 'wifi'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggDF.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# force to string\n",
    "# aggDF['id'] = 'Unknown'\n",
    "# aggDF['video'] = False\n",
    "# aggDF['wifi'] = False\n",
    "# aggDF['power'] = False\n",
    "# aggDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+---------+----------------+-------+------------+----+-------+------------+-------------+-----------------------------+-------------+--------------+-----------------------------+--------+-----------+-------------+-----+-------------------------+-----+-------+----------+-----------+---------+----+--------------------+-----------+-----+-----+\n",
      "|from_city_name|start_date|stay_days|table_name      |task_id|to_city_name|trip|version|airline_code|airline_codes|arr_time                     |check_bag_inc|company       |dep_time                     |duration|flight_code|flight_number|index|plane                    |power|price  |price_code|search_date|span_days|stop|stop_info           |ticket_left|video|wifi |\n",
      "+--------------+----------+---------+----------------+-------+------------+----+-------+------------+-------------+-----------------------------+-------------+--------------+-----------------------------+--------+-----------+-------------+-----+-------------------------+-----+-------+----------+-----------+---------+----+--------------------+-----------+-----+-----+\n",
      "|sydney        |2017-04-30|21       |flight_1_5_price|34     |beijing     |2   |1.0    |CX          |[CX, KA]     |2017-04-30T22:30:00.000+08:00|false        |Cathay Pacific|2017-04-30T10:05:00.000+10:00|14h25m  |CX162      |162          |50   |AIRBUS INDUSTRIE A330-300|true |1001.79|AUD       |2017-04-23 |0        |1   |Hong Kong(HKG):1h10m|0          |true |false|\n",
      "|sydney        |2017-04-30|21       |flight_1_5_price|34     |beijing     |2   |1.0    |CX          |[CX, KA]     |2017-04-30T23:25:00.000+08:00|false        |Cathay Pacific|2017-04-30T10:05:00.000+10:00|15h20m  |CX162      |162          |77   |AIRBUS INDUSTRIE A330-300|false|1001.79|AUD       |2017-04-23 |0        |1   |Hong Kong(HKG):2h10m|0          |false|false|\n",
      "+--------------+----------+---------+----------------+-------+------------+----+-------+------------+-------------+-----------------------------+-------------+--------------+-----------------------------+--------+-----------+-------------+-----+-------------------------+-----+-------+----------+-----------+---------+----+--------------------+-----------+-----+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sp_df_test = spark.createDataFrame(aggDF)\n",
    "sp_df_test.show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create empty dataframe and save to empty parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# if necessary delete the parquet file\n",
    "! rm -rf ~/s3/comb/flight_v1_0.pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "struct_v1_0 = sp_df_test.schema\n",
    "emptyDF = spark.createDataFrame(sc.emptyRDD(), struct_v1_0)\n",
    "# emptyDF = spark.createDataFrame(aggDF, struct_v1_0)\n",
    "emptyDF.write.parquet(os.path.join(comb_fldr, \"flight_v1_0.pq\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 226\n",
      "226 453\n"
     ]
    }
   ],
   "source": [
    "len(range_temp)\n",
    "for chunk_num in range(0, 2):\n",
    "    print(str(range_temp[chunk_num][0]), str(range_temp[chunk_num][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopped here 20170521.  \n",
    "  \n",
    "**Need to**:   \n",
    "* test generating multip pandas df and read into spark and then parquet - DONE\n",
    "* build loop within flight_1_5 - DONE\n",
    "* build loop for different sub folders on s3\n",
    "\n",
    "**Problem**:   \n",
    "* need to figure out how to flatten nested spark dataframe - for now just do this in pandas\n",
    "\n",
    "Check out https://www.youtube.com/watch?v=noFkYVkixPA  \n",
    "and https://spark.apache.org/docs/2.1.1/sql-programming-guide.html#save-modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ext_fldr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-0bed59a2ddcc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mfile_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mglob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mext_fldr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"*.txt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mrange_temp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchunkIt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_count\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ext_fldr' is not defined"
     ]
    }
   ],
   "source": [
    "def append_pq(df):    \n",
    "    spDF = spark.createDataFrame(df, struct_v1_0)\n",
    "    spDF.repartition(1).write.mode('append').parquet(os.path.join(comb_fldr, \"flight_v1_0.pq\"))\n",
    "    print(os.path.join(comb_fldr, \"flight_v1_0.pq\") + \" saved!\")    \n",
    "    \n",
    "    \n",
    "file_count = len(glob.glob(os.path.join(ext_fldr, \"*.txt\")))\n",
    "range_temp = chunkIt(range(file_count), 100)\n",
    "\n",
    "count = 0\n",
    "\n",
    "for chunk_num in range(0, 100):    \n",
    "    aggDF = pd.DataFrame() #aggregate dataframe to hold all individual dataframes\n",
    "\n",
    "    for file in glob.glob(os.path.join(ext_fldr, \"*.txt\"))[range_temp[chunk_num][0]:range_temp[chunk_num][1]]:        \n",
    "        with open(file) as f:                           \n",
    "            d = json.load(f)\n",
    "            flight_list = json_normalize(d['flight_list'])\n",
    "            basic = json_normalize(d['basic'])\n",
    "\n",
    "            # create dataframe\n",
    "            basic = basic.drop('search_date', 1)\n",
    "            basic['tmp'] = 1\n",
    "            flight_list = flight_list.drop('id', 1)\n",
    "            flight_list['tmp'] = 1\n",
    "            DF = pd.merge(basic, flight_list, on=['tmp'])\n",
    "            DF=DF.drop('tmp', 1)\n",
    "            \n",
    "\n",
    "            # append\n",
    "            aggDF = aggDF.append(DF)\n",
    "    count = count + 1\n",
    "    print(\"Count: \" + str(count))        \n",
    "    append_pq(aggDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flight.repartition(1).write.parquet(os.path.join(comb_fldr, \"flight_v1_0_comb.pq\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get size of the s3 bucket\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "def get_folder_size(bucket, prefix):\n",
    "    total_size = 0\n",
    "    for obj in boto3.resource('s3').Bucket(bucket).objects.filter(Prefix=prefix):\n",
    "        total_size += obj.size\n",
    "    return total_size\n",
    "\n",
    "# get_folder_size('flight_1_14', 'flight.price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import boto3 \n",
    "s3_client = boto3.client(\"s3\")\n",
    "all_objects = s3_client.list_objects(Bucket = 'flight.price') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'ETag': '\"7ddc5ebf67b4f8b64deb75b23b0f1b32-7\"',\n",
       "  'Key': 'flight_15_10/flight_15_10_price_2017-04-25.zip',\n",
       "  'LastModified': datetime.datetime(2017, 4, 26, 4, 22, 58, tzinfo=tzlocal()),\n",
       "  'Owner': {'DisplayName': 'jingwangian',\n",
       "   'ID': '5e5078ad36b7435fdafdb000491d465746defe398a4f8f1c4b841323b53cc316'},\n",
       "  'Size': 56971822,\n",
       "  'StorageClass': 'STANDARD'},\n",
       " {'ETag': '\"6439938860b1f61d1f4ff87b190dedf1-8\"',\n",
       "  'Key': 'flight_15_10/flight_15_10_price_2017-04-26.zip',\n",
       "  'LastModified': datetime.datetime(2017, 4, 27, 13, 15, 33, tzinfo=tzlocal()),\n",
       "  'Owner': {'DisplayName': 'jingwangian',\n",
       "   'ID': '5e5078ad36b7435fdafdb000491d465746defe398a4f8f1c4b841323b53cc316'},\n",
       "  'Size': 58827784,\n",
       "  'StorageClass': 'STANDARD'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_objects['Contents'][0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_objects['Contents'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did a test using route 1_5  \n",
    "* 38 zip files  \n",
    "* 1.5 GB txt file\n",
    "* 20MB parquet\n",
    "* Optimal parquet say 120MB\n",
    "* 120/20 * 38 = 228 zip files\n",
    "* 120/20 * 1.5 = 9 GB\n",
    "so process 228 zip file at a time  \n",
    "1000 zip files, so do 5 chunks  \n",
    "within each chunk, process 5 zip files at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dir_in = '/home/ubuntu/s3/flight_1_5'\n",
    "# dir_out = '/home/ubuntu/s3/flight_1_5/extracted'\n",
    "# extension = \".zip\"\n",
    "\n",
    "from os.path import join\n",
    "from os import listdir, rmdir\n",
    "from shutil import move\n",
    "\n",
    "\n",
    "def unzip_files(dir_in, dir_out, extension):\n",
    "    os.chdir(dir_in) # change directory from working dir to dir with files\n",
    "    for subdir, dirs, files in os.walk(dir_in):\n",
    "        for item in files:\n",
    "            if item.endswith(extension): # check for \".zip\" extension\n",
    "                file_name = os.path.join(subdir, item)\n",
    "                zip_ref = zipfile.ZipFile(file_name) # create zipfile object\n",
    "                zip_ref.extractall(dir_out) # extract file to dir\n",
    "                zip_ref.close() # close file              \n",
    "                \n",
    "def chunker(seq, size):\n",
    "    return (seq[pos:pos + size] for pos in range(0, len(seq), size))\n",
    "\n",
    "\n",
    "def append_pq(df, pq_folder, pq_file_name):    \n",
    "    spDF = spark.createDataFrame(df, struct_v1_0)\n",
    "    \n",
    "    # if exists then append, otherwise create new\n",
    "    if os.path.isdir(os.path.join(pq_folder, pq_file_name)):     \n",
    "        spDF.repartition(1).write.mode('append').parquet(os.path.join(pq_folder, pq_file_name))\n",
    "        print(os.path.join(pq_folder, pq_file_name) + \" saved!\")\n",
    "    else:\n",
    "        spDF.repartition(1).write.parquet(os.path.join(pq_folder, pq_file_name))\n",
    "        \n",
    "\n",
    "def txt_to_parquet(ext_fldr, pq_folder, pq_file_name):\n",
    "    aggDF = pd.DataFrame() #aggregate dataframe to hold all individual dataframes        \n",
    "    count = 0\n",
    "    \n",
    "#     file_count = len(glob.glob(os.path.join(ext_fldr, \"*.txt\")))\n",
    "#     range_temp = chunkIt(range(file_count), 100)\n",
    "\n",
    "#     for chunk_num in range(0, 100):       \n",
    "#         for file in glob.glob(os.path.join(ext_fldr, \"*.txt\"))[range_temp[chunk_num][0]:range_temp[chunk_num][1]]:        \n",
    "    for file in glob.glob(os.path.join(ext_fldr, \"*.txt\")):        \n",
    "        with open(file) as f:                           \n",
    "            d = json.load(f)\n",
    "            try:\n",
    "                flight_list = json_normalize(d['flight_list'])\n",
    "                basic = json_normalize(d['basic'])\n",
    "            except Exception as e:                                \n",
    "                copyFile(file, file.replace('/txt/', '/txt_exception/'))\n",
    "#                 print(\"Not version 1.0. Copied to txt_exception folder: \" + file)\n",
    "                continue\n",
    "\n",
    "\n",
    "            # create dataframe\n",
    "            basic = basic.drop('search_date', 1)\n",
    "            basic['tmp'] = 1\n",
    "            flight_list = flight_list.drop('id', 1)\n",
    "            flight_list['tmp'] = 1\n",
    "            DF = pd.merge(basic, flight_list, on=['tmp'])\n",
    "            DF=DF.drop('tmp', 1)            \n",
    "\n",
    "            # append\n",
    "            aggDF = aggDF.append(DF)\n",
    "\n",
    "            # counter\n",
    "            count += 1\n",
    "            if count % 1000 == 0:\n",
    "                print(\"processed \" + str(count) + \" files\")\n",
    "    if not(aggDF.empty):\n",
    "        append_pq(aggDF, pq_folder, pq_file_name)    \n",
    "    #     count = count + 1\n",
    "        #     print(\"Count: \" + str(count))     \n",
    "\n",
    "    \n",
    "def clear_folder(folder):\n",
    "    for the_file in os.listdir(folder):\n",
    "        file_path = os.path.join(folder, the_file)\n",
    "        try:\n",
    "            if os.path.isfile(file_path):\n",
    "                os.unlink(file_path)\n",
    "            #elif os.path.isdir(file_path): shutil.rmtree(file_path)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "        # recreate the folder after deletion\n",
    "        if not os.path.exists(folder):\n",
    "            os.makedirs(folder)\n",
    "            \n",
    "def copyFile(src, dest):\n",
    "    try:\n",
    "        shutil.copy(src, dest)\n",
    "    except shutil.Error as e:\n",
    "        print('Error: %s' % e)\n",
    "    except IOError as e:\n",
    "        print('Error: %s' % e.strerror)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "big_loop_count = 0\n",
    "small_loop_count = 0\n",
    "\n",
    "zip_folder = '/home/ubuntu/s3/comb/zip/'\n",
    "txt_folder = '/home/ubuntu/s3/comb/txt/'\n",
    "txt_exception_folder = '/home/ubuntu/s3/comb/txt_exception/'\n",
    "pq_folder = \"/home/ubuntu/s3/comb/parquet_temp/\"\n",
    "pq_file_name = \"flight_v1_0.pq\"    \n",
    "\n",
    "flight = spark.read.parquet(\"/home/ubuntu/s3/comb/flight_v1_0.pq\")\n",
    "struct_v1_0 = flight.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be very careful - the following code clears the folders!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # clear folders before starting\n",
    "# clear_folder(zip_folder)\n",
    "# clear_folder(txt_folder)\n",
    "# clear_folder(pq_folder)\n",
    "# # clear_folder(txt_exception_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "big loop: 1\n",
      "/home/ubuntu/s3/comb/parquet_temp/flight_v1_0.pq saved!\n",
      "big loop: 2\n",
      "/home/ubuntu/s3/comb/parquet_temp/flight_v1_0.pq saved!\n",
      "big loop: 3\n",
      "/home/ubuntu/s3/comb/parquet_temp/flight_v1_0.pq saved!\n",
      "big loop: 4\n",
      "/home/ubuntu/s3/comb/parquet_temp/flight_v1_0.pq saved!\n",
      "big loop: 5\n",
      "/home/ubuntu/s3/comb/parquet_temp/flight_v1_0.pq saved!\n",
      "big loop: 6\n",
      "/home/ubuntu/s3/comb/parquet_temp/flight_v1_0.pq saved!\n",
      "big loop: 7\n",
      "/home/ubuntu/s3/comb/parquet_temp/flight_v1_0.pq saved!\n",
      "big loop: 8\n",
      "/home/ubuntu/s3/comb/parquet_temp/flight_v1_0.pq saved!\n",
      "big loop: 9\n",
      "big loop: 10\n",
      "big loop: 11\n",
      "/home/ubuntu/s3/comb/parquet_temp/flight_v1_0.pq saved!\n",
      "big loop: 12\n",
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 40292)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.5/socketserver.py\", line 313, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.5/socketserver.py\", line 341, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.5/socketserver.py\", line 354, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.5/socketserver.py\", line 681, in __init__\n",
      "    self.handle()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/accumulators.py\", line 235, in handle\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/serializers.py\", line 577, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling o9.sc",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-7fe53aa195d5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;31m# create or append to parquet\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0mtxt_to_parquet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtxt_folder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpq_folder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpq_file_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;31m# clean up folder to save space\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-bb560df2a41c>\u001b[0m in \u001b[0;36mtxt_to_parquet\u001b[1;34m(ext_fldr, pq_folder, pq_file_name)\u001b[0m\n\u001b[0;32m     70\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"processed \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" files\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maggDF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[0mappend_pq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maggDF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpq_folder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpq_file_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;31m#     count = count + 1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[1;31m#     print(\"Count: \" + str(count))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-bb560df2a41c>\u001b[0m in \u001b[0;36mappend_pq\u001b[1;34m(df, pq_folder, pq_file_name)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mappend_pq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpq_folder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpq_file_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m     \u001b[0mspDF\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstruct_v1_0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;31m# if exists then append, otherwise create new\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m    524\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    525\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 526\u001b[1;33m             \u001b[0mrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    527\u001b[0m         \u001b[0mjrdd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSerDeUtil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoJavaArray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_to_java_object_rdd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    528\u001b[0m         \u001b[0mjdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapplySchemaToPythonRDD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_createFromLocal\u001b[1;34m(self, data, schema)\u001b[0m\n\u001b[0;32m    402\u001b[0m         \u001b[1;31m# convert python objects to sql data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    403\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mschema\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoInternal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 404\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    405\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    406\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0msince\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/context.py\u001b[0m in \u001b[0;36mparallelize\u001b[1;34m(self, c, numSlices)\u001b[0m\n\u001b[0;32m    440\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    441\u001b[0m         \"\"\"\n\u001b[1;32m--> 442\u001b[1;33m         \u001b[0mnumSlices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumSlices\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mnumSlices\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefaultParallelism\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    443\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxrange\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    444\u001b[0m             \u001b[0msize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/context.py\u001b[0m in \u001b[0;36mdefaultParallelism\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    365\u001b[0m         reduce tasks)\n\u001b[0;32m    366\u001b[0m         \"\"\"\n\u001b[1;32m--> 367\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefaultParallelism\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    368\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    369\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu/anaconda3/lib/python3.5/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1133\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1135\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu/anaconda3/lib/python3.5/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    325\u001b[0m             raise Py4JError(\n\u001b[0;32m    326\u001b[0m                 \u001b[1;34m\"An error occurred while calling {0}{1}{2}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 327\u001b[1;33m                 format(target_id, \".\", name))\n\u001b[0m\u001b[0;32m    328\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    329\u001b[0m         \u001b[0mtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPy4JError\u001b[0m: An error occurred while calling o9.sc"
     ]
    }
   ],
   "source": [
    "# ends after 50+11\n",
    "for chunk in chunker(all_objects['Contents'][50:], 1):    \n",
    "    \n",
    "    # download zip files\n",
    "    for item in chunk:\n",
    "        s3_client.download_file('flight.price', item['Key'], zip_folder + item['Key'].replace('/', '__'))\n",
    "        \n",
    "        big_loop_count += 1\n",
    "        print(\"big loop: \" + str(big_loop_count))\n",
    "\n",
    "        # extract to txt\n",
    "        unzip_files('/home/ubuntu/s3/comb/zip/', '/home/ubuntu/s3/comb/txt/', '.zip')   \n",
    "\n",
    "        # if necessary move subfolder contents to parent folder        \n",
    "        try:\n",
    "            for filename in listdir(join(txt_folder, 'final_results')):\n",
    "                move(join(txt_folder, 'final_results', filename), join(txt_folder, filename))\n",
    "            rmdir(join(txt_folder, 'final_results'))\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "        # create or append to parquet\n",
    "        txt_to_parquet(txt_folder, pq_folder, pq_file_name)\n",
    "\n",
    "        # clean up folder to save space\n",
    "        clear_folder('/home/ubuntu/s3/comb/zip/')\n",
    "        clear_folder('/home/ubuntu/s3/comb/txt/')    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flight_15_10/flight_15_10_price_2017-04-25.zip\n",
      "flight_15_10/flight_15_10_price_2017-04-26.zip\n",
      "flight_15_10/flight_15_10_price_2017-04-27.zip\n",
      "flight_15_10/flight_15_10_price_2017-04-28.zip\n",
      "flight_15_10/flight_15_10_price_2017-04-29.zip\n",
      "flight_15_10/flight_15_10_price_2017-04-30.zip\n",
      "flight_15_10/flight_15_10_price_2017-05-01.zip\n",
      "flight_15_10/flight_15_10_price_2017-05-02.zip\n",
      "flight_15_10/flight_15_10_price_2017-05-03.zip\n",
      "flight_15_10/flight_15_10_price_2017-05-06.zip\n",
      "flight_15_10/flight_15_10_price_2017-05-07.zip\n",
      "flight_15_10/flight_15_10_price_2017-05-08.zip\n",
      "flight_15_10/flight_15_10_price_2017-05-09.zip\n",
      "flight_15_10/flight_15_10_price_2017-05-10.zip\n",
      "flight_15_10/flight_15_10_price_2017-05-11.zip\n",
      "flight_15_11/flight_15_11_price_2017-04-25.zip\n",
      "flight_15_11/flight_15_11_price_2017-04-26.zip\n",
      "flight_15_11/flight_15_11_price_2017-04-27.zip\n",
      "flight_15_11/flight_15_11_price_2017-04-28.zip\n",
      "flight_15_11/flight_15_11_price_2017-04-29.zip\n",
      "flight_15_11/flight_15_11_price_2017-04-30.zip\n",
      "flight_15_11/flight_15_11_price_2017-05-01.zip\n",
      "flight_15_11/flight_15_11_price_2017-05-02.zip\n",
      "flight_15_11/flight_15_11_price_2017-05-03.zip\n",
      "flight_15_11/flight_15_11_price_2017-05-06.zip\n",
      "flight_15_11/flight_15_11_price_2017-05-07.zip\n",
      "flight_15_11/flight_15_11_price_2017-05-08.zip\n",
      "flight_15_11/flight_15_11_price_2017-05-09.zip\n",
      "flight_15_11/flight_15_11_price_2017-05-10.zip\n",
      "flight_15_11/flight_15_11_price_2017-05-11.zip\n",
      "flight_15_12/flight_15_12_price_2017-04-25.zip\n",
      "flight_15_12/flight_15_12_price_2017-04-26.zip\n",
      "flight_15_12/flight_15_12_price_2017-04-27.zip\n",
      "flight_15_12/flight_15_12_price_2017-04-28.zip\n",
      "flight_15_12/flight_15_12_price_2017-04-29.zip\n",
      "flight_15_12/flight_15_12_price_2017-04-30.zip\n",
      "flight_15_12/flight_15_12_price_2017-05-01.zip\n",
      "flight_15_12/flight_15_12_price_2017-05-02.zip\n",
      "flight_15_12/flight_15_12_price_2017-05-03.zip\n",
      "flight_15_12/flight_15_12_price_2017-05-06.zip\n",
      "flight_15_12/flight_15_12_price_2017-05-07.zip\n",
      "flight_15_12/flight_15_12_price_2017-05-08.zip\n",
      "flight_15_12/flight_15_12_price_2017-05-09.zip\n",
      "flight_15_12/flight_15_12_price_2017-05-10.zip\n",
      "flight_15_12/flight_15_12_price_2017-05-11.zip\n",
      "flight_15_13/flight_15_13_price_2017-04-25.zip\n",
      "flight_15_13/flight_15_13_price_2017-04-26.zip\n",
      "flight_15_13/flight_15_13_price_2017-04-27.zip\n",
      "flight_15_13/flight_15_13_price_2017-04-28.zip\n",
      "flight_15_13/flight_15_13_price_2017-04-29.zip\n",
      "flight_15_13/flight_15_13_price_2017-04-30.zip\n",
      "flight_15_13/flight_15_13_price_2017-05-01.zip\n",
      "flight_15_13/flight_15_13_price_2017-05-02.zip\n",
      "flight_15_13/flight_15_13_price_2017-05-03.zip\n",
      "flight_15_13/flight_15_13_price_2017-05-06.zip\n",
      "flight_15_13/flight_15_13_price_2017-05-07.zip\n",
      "flight_15_13/flight_15_13_price_2017-05-08.zip\n",
      "flight_15_13/flight_15_13_price_2017-05-09.zip\n",
      "flight_15_13/flight_15_13_price_2017-05-10.zip\n",
      "flight_15_13/flight_15_13_price_2017-05-11.zip\n",
      "flight_15_14/flight_15_14_price_2017-04-25.zip\n",
      "flight_15_14/flight_15_14_price_2017-04-26.zip\n",
      "flight_15_14/flight_15_14_price_2017-04-27.zip\n",
      "flight_15_14/flight_15_14_price_2017-04-28.zip\n",
      "flight_15_14/flight_15_14_price_2017-04-29.zip\n",
      "flight_15_14/flight_15_14_price_2017-04-30.zip\n",
      "flight_15_14/flight_15_14_price_2017-05-01.zip\n",
      "flight_15_14/flight_15_14_price_2017-05-02.zip\n",
      "flight_15_14/flight_15_14_price_2017-05-03.zip\n",
      "flight_15_14/flight_15_14_price_2017-05-06.zip\n",
      "flight_15_14/flight_15_14_price_2017-05-07.zip\n",
      "flight_15_14/flight_15_14_price_2017-05-08.zip\n",
      "flight_15_14/flight_15_14_price_2017-05-09.zip\n",
      "flight_15_14/flight_15_14_price_2017-05-10.zip\n",
      "flight_15_14/flight_15_14_price_2017-05-11.zip\n",
      "flight_15_5/flight_15_5_price_2017-04-25.zip\n",
      "flight_15_5/flight_15_5_price_2017-04-26.zip\n",
      "flight_15_5/flight_15_5_price_2017-04-27.zip\n",
      "flight_15_5/flight_15_5_price_2017-04-28.zip\n",
      "flight_15_5/flight_15_5_price_2017-04-29.zip\n",
      "flight_15_5/flight_15_5_price_2017-04-30.zip\n",
      "flight_15_5/flight_15_5_price_2017-05-01.zip\n",
      "flight_15_5/flight_15_5_price_2017-05-02.zip\n",
      "flight_15_5/flight_15_5_price_2017-05-03.zip\n",
      "flight_15_5/flight_15_5_price_2017-05-06.zip\n",
      "flight_15_5/flight_15_5_price_2017-05-07.zip\n",
      "flight_15_5/flight_15_5_price_2017-05-08.zip\n",
      "flight_15_5/flight_15_5_price_2017-05-09.zip\n",
      "flight_15_5/flight_15_5_price_2017-05-10.zip\n",
      "flight_15_5/flight_15_5_price_2017-05-11.zip\n",
      "flight_15_6/flight_15_6_price_2017-04-25.zip\n",
      "flight_15_6/flight_15_6_price_2017-04-26.zip\n",
      "flight_15_6/flight_15_6_price_2017-04-27.zip\n",
      "flight_15_6/flight_15_6_price_2017-04-28.zip\n",
      "flight_15_6/flight_15_6_price_2017-04-29.zip\n",
      "flight_15_6/flight_15_6_price_2017-04-30.zip\n",
      "flight_15_6/flight_15_6_price_2017-05-01.zip\n",
      "flight_15_6/flight_15_6_price_2017-05-02.zip\n",
      "flight_15_6/flight_15_6_price_2017-05-03.zip\n",
      "flight_15_6/flight_15_6_price_2017-05-06.zip\n",
      "flight_15_6/flight_15_6_price_2017-05-07.zip\n",
      "flight_15_6/flight_15_6_price_2017-05-08.zip\n",
      "flight_15_6/flight_15_6_price_2017-05-09.zip\n",
      "flight_15_6/flight_15_6_price_2017-05-10.zip\n",
      "flight_15_6/flight_15_6_price_2017-05-11.zip\n",
      "flight_15_7/flight_15_7_price_2017-04-25.zip\n",
      "flight_15_7/flight_15_7_price_2017-04-26.zip\n",
      "flight_15_7/flight_15_7_price_2017-04-27.zip\n",
      "flight_15_7/flight_15_7_price_2017-04-28.zip\n",
      "flight_15_7/flight_15_7_price_2017-04-29.zip\n",
      "flight_15_7/flight_15_7_price_2017-04-30.zip\n",
      "flight_15_7/flight_15_7_price_2017-05-01.zip\n",
      "flight_15_7/flight_15_7_price_2017-05-02.zip\n",
      "flight_15_7/flight_15_7_price_2017-05-03.zip\n",
      "flight_15_7/flight_15_7_price_2017-05-06.zip\n",
      "flight_15_7/flight_15_7_price_2017-05-07.zip\n",
      "flight_15_7/flight_15_7_price_2017-05-08.zip\n",
      "flight_15_7/flight_15_7_price_2017-05-09.zip\n",
      "flight_15_7/flight_15_7_price_2017-05-10.zip\n",
      "flight_15_7/flight_15_7_price_2017-05-11.zip\n",
      "flight_15_8/flight_15_8_price_2017-04-25.zip\n",
      "flight_15_8/flight_15_8_price_2017-04-26.zip\n",
      "flight_15_8/flight_15_8_price_2017-04-27.zip\n",
      "flight_15_8/flight_15_8_price_2017-04-28.zip\n",
      "flight_15_8/flight_15_8_price_2017-04-29.zip\n",
      "flight_15_8/flight_15_8_price_2017-04-30.zip\n",
      "flight_15_8/flight_15_8_price_2017-05-01.zip\n",
      "flight_15_8/flight_15_8_price_2017-05-02.zip\n",
      "flight_15_8/flight_15_8_price_2017-05-03.zip\n",
      "flight_15_8/flight_15_8_price_2017-05-06.zip\n",
      "flight_15_8/flight_15_8_price_2017-05-07.zip\n",
      "flight_15_8/flight_15_8_price_2017-05-08.zip\n",
      "flight_15_8/flight_15_8_price_2017-05-09.zip\n",
      "flight_15_8/flight_15_8_price_2017-05-10.zip\n",
      "flight_15_8/flight_15_8_price_2017-05-11.zip\n",
      "flight_15_9/flight_15_9_price_2017-04-25.zip\n",
      "flight_15_9/flight_15_9_price_2017-04-26.zip\n",
      "flight_15_9/flight_15_9_price_2017-04-27.zip\n",
      "flight_15_9/flight_15_9_price_2017-04-28.zip\n",
      "flight_15_9/flight_15_9_price_2017-04-29.zip\n",
      "flight_15_9/flight_15_9_price_2017-04-30.zip\n",
      "flight_15_9/flight_15_9_price_2017-05-01.zip\n",
      "flight_15_9/flight_15_9_price_2017-05-02.zip\n",
      "flight_15_9/flight_15_9_price_2017-05-03.zip\n",
      "flight_15_9/flight_15_9_price_2017-05-06.zip\n",
      "flight_15_9/flight_15_9_price_2017-05-07.zip\n",
      "flight_15_9/flight_15_9_price_2017-05-08.zip\n",
      "flight_15_9/flight_15_9_price_2017-05-09.zip\n",
      "flight_15_9/flight_15_9_price_2017-05-10.zip\n",
      "flight_15_9/flight_15_9_price_2017-05-11.zip\n",
      "flight_16_10/flight_16_10_price_2017-04-25.zip\n",
      "flight_16_10/flight_16_10_price_2017-04-26.zip\n",
      "flight_16_10/flight_16_10_price_2017-04-27.zip\n",
      "flight_16_10/flight_16_10_price_2017-04-28.zip\n",
      "flight_16_10/flight_16_10_price_2017-04-29.zip\n",
      "flight_16_10/flight_16_10_price_2017-04-30.zip\n",
      "flight_16_10/flight_16_10_price_2017-05-01.zip\n",
      "flight_16_10/flight_16_10_price_2017-05-02.zip\n",
      "flight_16_10/flight_16_10_price_2017-05-06.zip\n",
      "flight_16_10/flight_16_10_price_2017-05-07.zip\n",
      "flight_16_10/flight_16_10_price_2017-05-08.zip\n",
      "flight_16_10/flight_16_10_price_2017-05-09.zip\n",
      "flight_16_10/flight_16_10_price_2017-05-10.zip\n",
      "flight_16_10/flight_16_10_price_2017-05-11.zip\n",
      "flight_16_11/flight_16_11_price_2017-04-25.zip\n",
      "flight_16_11/flight_16_11_price_2017-04-26.zip\n",
      "flight_16_11/flight_16_11_price_2017-04-27.zip\n",
      "flight_16_11/flight_16_11_price_2017-04-28.zip\n",
      "flight_16_11/flight_16_11_price_2017-04-29.zip\n",
      "flight_16_11/flight_16_11_price_2017-04-30.zip\n",
      "flight_16_11/flight_16_11_price_2017-05-01.zip\n",
      "flight_16_11/flight_16_11_price_2017-05-02.zip\n",
      "flight_16_11/flight_16_11_price_2017-05-06.zip\n",
      "flight_16_11/flight_16_11_price_2017-05-07.zip\n",
      "flight_16_11/flight_16_11_price_2017-05-08.zip\n",
      "flight_16_11/flight_16_11_price_2017-05-09.zip\n",
      "flight_16_11/flight_16_11_price_2017-05-10.zip\n",
      "flight_16_11/flight_16_11_price_2017-05-11.zip\n",
      "flight_16_12/flight_16_12_price_2017-04-25.zip\n",
      "flight_16_12/flight_16_12_price_2017-04-26.zip\n",
      "flight_16_12/flight_16_12_price_2017-04-27.zip\n",
      "flight_16_12/flight_16_12_price_2017-04-28.zip\n",
      "flight_16_12/flight_16_12_price_2017-04-29.zip\n",
      "flight_16_12/flight_16_12_price_2017-04-30.zip\n",
      "flight_16_12/flight_16_12_price_2017-05-01.zip\n",
      "flight_16_12/flight_16_12_price_2017-05-02.zip\n",
      "flight_16_12/flight_16_12_price_2017-05-06.zip\n",
      "flight_16_12/flight_16_12_price_2017-05-07.zip\n",
      "flight_16_12/flight_16_12_price_2017-05-08.zip\n",
      "flight_16_12/flight_16_12_price_2017-05-09.zip\n",
      "flight_16_12/flight_16_12_price_2017-05-10.zip\n",
      "flight_16_12/flight_16_12_price_2017-05-11.zip\n",
      "flight_16_13/flight_16_13_price_2017-04-25.zip\n",
      "flight_16_13/flight_16_13_price_2017-04-26.zip\n",
      "flight_16_13/flight_16_13_price_2017-04-27.zip\n",
      "flight_16_13/flight_16_13_price_2017-04-28.zip\n",
      "flight_16_13/flight_16_13_price_2017-04-29.zip\n",
      "flight_16_13/flight_16_13_price_2017-04-30.zip\n",
      "flight_16_13/flight_16_13_price_2017-05-01.zip\n",
      "flight_16_13/flight_16_13_price_2017-05-06.zip\n",
      "flight_16_13/flight_16_13_price_2017-05-07.zip\n",
      "flight_16_13/flight_16_13_price_2017-05-08.zip\n",
      "flight_16_13/flight_16_13_price_2017-05-09.zip\n",
      "flight_16_13/flight_16_13_price_2017-05-10.zip\n",
      "flight_16_13/flight_16_13_price_2017-05-11.zip\n",
      "flight_16_14/flight_16_14_price_2017-04-26.zip\n",
      "flight_16_14/flight_16_14_price_2017-04-27.zip\n",
      "flight_16_14/flight_16_14_price_2017-04-28.zip\n",
      "flight_16_14/flight_16_14_price_2017-04-29.zip\n",
      "flight_16_14/flight_16_14_price_2017-04-30.zip\n",
      "flight_16_14/flight_16_14_price_2017-05-01.zip\n",
      "flight_16_14/flight_16_14_price_2017-05-06.zip\n",
      "flight_16_14/flight_16_14_price_2017-05-07.zip\n",
      "flight_16_14/flight_16_14_price_2017-05-08.zip\n",
      "flight_16_14/flight_16_14_price_2017-05-09.zip\n",
      "flight_16_5/flight_16_5_price_2017-04-25.zip\n",
      "flight_16_5/flight_16_5_price_2017-04-26.zip\n",
      "flight_16_5/flight_16_5_price_2017-04-27.zip\n",
      "flight_16_5/flight_16_5_price_2017-04-28.zip\n",
      "flight_16_5/flight_16_5_price_2017-04-29.zip\n",
      "flight_16_5/flight_16_5_price_2017-04-30.zip\n",
      "flight_16_5/flight_16_5_price_2017-05-01.zip\n",
      "flight_16_5/flight_16_5_price_2017-05-02.zip\n",
      "flight_16_5/flight_16_5_price_2017-05-03.zip\n",
      "flight_16_5/flight_16_5_price_2017-05-06.zip\n",
      "flight_16_5/flight_16_5_price_2017-05-07.zip\n",
      "flight_16_5/flight_16_5_price_2017-05-08.zip\n",
      "flight_16_5/flight_16_5_price_2017-05-09.zip\n",
      "flight_16_5/flight_16_5_price_2017-05-10.zip\n",
      "flight_16_5/flight_16_5_price_2017-05-11.zip\n",
      "flight_16_6/flight_16_6_price_2017-04-25.zip\n",
      "flight_16_6/flight_16_6_price_2017-04-26.zip\n",
      "flight_16_6/flight_16_6_price_2017-04-27.zip\n",
      "flight_16_6/flight_16_6_price_2017-04-28.zip\n",
      "flight_16_6/flight_16_6_price_2017-04-29.zip\n",
      "flight_16_6/flight_16_6_price_2017-04-30.zip\n",
      "flight_16_6/flight_16_6_price_2017-05-01.zip\n",
      "flight_16_6/flight_16_6_price_2017-05-02.zip\n",
      "flight_16_6/flight_16_6_price_2017-05-03.zip\n",
      "flight_16_6/flight_16_6_price_2017-05-06.zip\n",
      "flight_16_6/flight_16_6_price_2017-05-07.zip\n",
      "flight_16_6/flight_16_6_price_2017-05-08.zip\n",
      "flight_16_6/flight_16_6_price_2017-05-09.zip\n",
      "flight_16_6/flight_16_6_price_2017-05-10.zip\n",
      "flight_16_6/flight_16_6_price_2017-05-11.zip\n",
      "flight_16_7/flight_16_7_price_2017-04-25.zip\n",
      "flight_16_7/flight_16_7_price_2017-04-26.zip\n",
      "flight_16_7/flight_16_7_price_2017-04-27.zip\n",
      "flight_16_7/flight_16_7_price_2017-04-28.zip\n",
      "flight_16_7/flight_16_7_price_2017-04-29.zip\n",
      "flight_16_7/flight_16_7_price_2017-04-30.zip\n",
      "flight_16_7/flight_16_7_price_2017-05-01.zip\n",
      "flight_16_7/flight_16_7_price_2017-05-02.zip\n",
      "flight_16_7/flight_16_7_price_2017-05-03.zip\n",
      "flight_16_7/flight_16_7_price_2017-05-06.zip\n",
      "flight_16_7/flight_16_7_price_2017-05-07.zip\n",
      "flight_16_7/flight_16_7_price_2017-05-08.zip\n",
      "flight_16_7/flight_16_7_price_2017-05-09.zip\n",
      "flight_16_7/flight_16_7_price_2017-05-10.zip\n",
      "flight_16_7/flight_16_7_price_2017-05-11.zip\n",
      "flight_16_8/flight_16_8_price_2017-04-25.zip\n",
      "flight_16_8/flight_16_8_price_2017-04-26.zip\n",
      "flight_16_8/flight_16_8_price_2017-04-27.zip\n",
      "flight_16_8/flight_16_8_price_2017-04-28.zip\n",
      "flight_16_8/flight_16_8_price_2017-04-29.zip\n",
      "flight_16_8/flight_16_8_price_2017-04-30.zip\n",
      "flight_16_8/flight_16_8_price_2017-05-01.zip\n",
      "flight_16_8/flight_16_8_price_2017-05-02.zip\n",
      "flight_16_8/flight_16_8_price_2017-05-06.zip\n",
      "flight_16_8/flight_16_8_price_2017-05-07.zip\n",
      "flight_16_8/flight_16_8_price_2017-05-08.zip\n",
      "flight_16_8/flight_16_8_price_2017-05-09.zip\n",
      "flight_16_8/flight_16_8_price_2017-05-10.zip\n",
      "flight_16_8/flight_16_8_price_2017-05-11.zip\n",
      "flight_16_9/flight_16_9_price_2017-04-25.zip\n",
      "flight_16_9/flight_16_9_price_2017-04-26.zip\n",
      "flight_16_9/flight_16_9_price_2017-04-27.zip\n",
      "flight_16_9/flight_16_9_price_2017-04-28.zip\n",
      "flight_16_9/flight_16_9_price_2017-04-29.zip\n",
      "flight_16_9/flight_16_9_price_2017-04-30.zip\n",
      "flight_16_9/flight_16_9_price_2017-05-01.zip\n",
      "flight_16_9/flight_16_9_price_2017-05-02.zip\n",
      "flight_16_9/flight_16_9_price_2017-05-06.zip\n",
      "flight_16_9/flight_16_9_price_2017-05-07.zip\n",
      "flight_16_9/flight_16_9_price_2017-05-08.zip\n",
      "flight_16_9/flight_16_9_price_2017-05-09.zip\n",
      "flight_16_9/flight_16_9_price_2017-05-10.zip\n",
      "flight_16_9/flight_16_9_price_2017-05-11.zip\n",
      "flight_1_10/flight_1_10_price_2017-04-23.zip\n",
      "flight_1_10/flight_1_10_price_2017-04-24.zip\n",
      "flight_1_10/flight_1_10_price_2017-04-25.zip\n",
      "flight_1_10/flight_1_10_price_2017-04-26.zip\n",
      "flight_1_10/flight_1_10_price_2017-04-27.zip\n",
      "flight_1_10/flight_1_10_price_2017-04-28.zip\n",
      "flight_1_10/flight_1_10_price_2017-04-29.zip\n",
      "flight_1_10/flight_1_10_price_2017-04-30.zip\n",
      "flight_1_10/flight_1_10_price_2017-05-01.zip\n",
      "flight_1_10/flight_1_10_price_2017-05-02.zip\n",
      "flight_1_10/flight_1_10_price_2017-05-03.zip\n",
      "flight_1_10/flight_1_10_price_2017-05-06.zip\n",
      "flight_1_10/flight_1_10_price_2017-05-07.zip\n",
      "flight_1_10/flight_1_10_price_2017-05-08.zip\n",
      "flight_1_10/flight_1_10_price_2017-05-09.zip\n",
      "flight_1_10/flight_1_10_price_2017-05-10.zip\n",
      "flight_1_10/flight_1_10_price_2017-05-11.zip\n",
      "flight_1_11/flight_1_11_price_2017-04-23.zip\n",
      "flight_1_11/flight_1_11_price_2017-04-24.zip\n",
      "flight_1_11/flight_1_11_price_2017-04-25.zip\n",
      "flight_1_11/flight_1_11_price_2017-04-26.zip\n",
      "flight_1_11/flight_1_11_price_2017-04-27.zip\n",
      "flight_1_11/flight_1_11_price_2017-04-28.zip\n",
      "flight_1_11/flight_1_11_price_2017-04-29.zip\n",
      "flight_1_11/flight_1_11_price_2017-04-30.zip\n",
      "flight_1_11/flight_1_11_price_2017-05-01.zip\n",
      "flight_1_11/flight_1_11_price_2017-05-02.zip\n",
      "flight_1_11/flight_1_11_price_2017-05-03.zip\n",
      "flight_1_11/flight_1_11_price_2017-05-06.zip\n",
      "flight_1_11/flight_1_11_price_2017-05-07.zip\n",
      "flight_1_11/flight_1_11_price_2017-05-08.zip\n",
      "flight_1_11/flight_1_11_price_2017-05-09.zip\n",
      "flight_1_11/flight_1_11_price_2017-05-10.zip\n",
      "flight_1_11/flight_1_11_price_2017-05-11.zip\n",
      "flight_1_12/flight_1_12_price_2017-04-23.zip\n",
      "flight_1_12/flight_1_12_price_2017-04-24.zip\n",
      "flight_1_12/flight_1_12_price_2017-04-25.zip\n",
      "flight_1_12/flight_1_12_price_2017-04-26.zip\n",
      "flight_1_12/flight_1_12_price_2017-04-27.zip\n",
      "flight_1_12/flight_1_12_price_2017-04-28.zip\n",
      "flight_1_12/flight_1_12_price_2017-04-29.zip\n",
      "flight_1_12/flight_1_12_price_2017-04-30.zip\n",
      "flight_1_12/flight_1_12_price_2017-05-01.zip\n",
      "flight_1_12/flight_1_12_price_2017-05-02.zip\n",
      "flight_1_12/flight_1_12_price_2017-05-03.zip\n",
      "flight_1_12/flight_1_12_price_2017-05-06.zip\n",
      "flight_1_12/flight_1_12_price_2017-05-07.zip\n",
      "flight_1_12/flight_1_12_price_2017-05-08.zip\n",
      "flight_1_12/flight_1_12_price_2017-05-09.zip\n",
      "flight_1_12/flight_1_12_price_2017-05-10.zip\n",
      "flight_1_12/flight_1_12_price_2017-05-11.zip\n",
      "flight_1_13/flight_1_13_price_2017-04-23.zip\n",
      "flight_1_13/flight_1_13_price_2017-04-24.zip\n",
      "flight_1_13/flight_1_13_price_2017-04-25.zip\n",
      "flight_1_13/flight_1_13_price_2017-04-26.zip\n",
      "flight_1_13/flight_1_13_price_2017-04-27.zip\n",
      "flight_1_13/flight_1_13_price_2017-04-28.zip\n",
      "flight_1_13/flight_1_13_price_2017-04-29.zip\n",
      "flight_1_13/flight_1_13_price_2017-04-30.zip\n",
      "flight_1_13/flight_1_13_price_2017-05-01.zip\n",
      "flight_1_13/flight_1_13_price_2017-05-02.zip\n",
      "flight_1_13/flight_1_13_price_2017-05-03.zip\n",
      "flight_1_13/flight_1_13_price_2017-05-06.zip\n",
      "flight_1_13/flight_1_13_price_2017-05-07.zip\n",
      "flight_1_13/flight_1_13_price_2017-05-08.zip\n",
      "flight_1_13/flight_1_13_price_2017-05-09.zip\n",
      "flight_1_13/flight_1_13_price_2017-05-10.zip\n",
      "flight_1_13/flight_1_13_price_2017-05-11.zip\n",
      "flight_1_14/flight_1_14_price_2017-04-23.zip\n",
      "flight_1_14/flight_1_14_price_2017-04-24.zip\n",
      "flight_1_14/flight_1_14_price_2017-04-25.zip\n",
      "flight_1_14/flight_1_14_price_2017-04-26.zip\n",
      "flight_1_14/flight_1_14_price_2017-04-27.zip\n",
      "flight_1_14/flight_1_14_price_2017-04-28.zip\n",
      "flight_1_14/flight_1_14_price_2017-04-29.zip\n",
      "flight_1_14/flight_1_14_price_2017-04-30.zip\n",
      "flight_1_14/flight_1_14_price_2017-05-01.zip\n",
      "flight_1_14/flight_1_14_price_2017-05-02.zip\n",
      "flight_1_14/flight_1_14_price_2017-05-03.zip\n",
      "flight_1_14/flight_1_14_price_2017-05-06.zip\n",
      "flight_1_14/flight_1_14_price_2017-05-07.zip\n",
      "flight_1_14/flight_1_14_price_2017-05-08.zip\n",
      "flight_1_14/flight_1_14_price_2017-05-09.zip\n",
      "flight_1_14/flight_1_14_price_2017-05-10.zip\n",
      "flight_1_14/flight_1_14_price_2017-05-11.zip\n",
      "flight_1_5/flight_1_5_price_2017-04-04.zip\n",
      "flight_1_5/flight_1_5_price_2017-04-06.zip\n",
      "flight_1_5/flight_1_5_price_2017-04-09.zip\n",
      "flight_1_5/flight_1_5_price_2017-04-12.zip\n",
      "flight_1_5/flight_1_5_price_2017-04-13.zip\n",
      "flight_1_5/flight_1_5_price_2017-04-14.zip\n",
      "flight_1_5/flight_1_5_price_2017-04-15.zip\n",
      "flight_1_5/flight_1_5_price_2017-04-16.zip\n",
      "flight_1_5/flight_1_5_price_2017-04-18.zip\n",
      "flight_1_5/flight_1_5_price_2017-04-19.zip\n",
      "flight_1_5/flight_1_5_price_2017-04-20.zip\n",
      "flight_1_5/flight_1_5_price_2017-04-21.zip\n",
      "flight_1_5/flight_1_5_price_2017-04-22.zip\n",
      "flight_1_5/flight_1_5_price_2017-04-23.zip\n",
      "flight_1_5/flight_1_5_price_2017-04-24.zip\n",
      "flight_1_5/flight_1_5_price_2017-04-25.zip\n",
      "flight_1_5/flight_1_5_price_2017-04-26.zip\n",
      "flight_1_5/flight_1_5_price_2017-04-27.zip\n",
      "flight_1_5/flight_1_5_price_2017-04-28.zip\n",
      "flight_1_5/flight_1_5_price_2017-04-29.zip\n",
      "flight_1_5/flight_1_5_price_2017-04-30.zip\n",
      "flight_1_5/flight_1_5_price_2017-05-01.zip\n",
      "flight_1_5/flight_1_5_price_2017-05-02.zip\n",
      "flight_1_5/flight_1_5_price_2017-05-03.zip\n",
      "flight_1_5/flight_1_5_price_2017-05-06.zip\n",
      "flight_1_5/flight_1_5_price_2017-05-07.zip\n",
      "flight_1_5/flight_1_5_price_2017-05-08.zip\n",
      "flight_1_5/flight_1_5_price_2017-05-09.zip\n",
      "flight_1_5/flight_1_5_price_2017-05-10.zip\n",
      "flight_1_5/flight_1_5_price_2017-05-11.zip\n",
      "flight_1_6/flight_1_6_price_2017-04-04.zip\n",
      "flight_1_6/flight_1_6_price_2017-04-06.zip\n",
      "flight_1_6/flight_1_6_price_2017-04-09.zip\n",
      "flight_1_6/flight_1_6_price_2017-04-12.zip\n",
      "flight_1_6/flight_1_6_price_2017-04-13.zip\n",
      "flight_1_6/flight_1_6_price_2017-04-15.zip\n",
      "flight_1_6/flight_1_6_price_2017-04-16.zip\n",
      "flight_1_6/flight_1_6_price_2017-04-18.zip\n",
      "flight_1_6/flight_1_6_price_2017-04-19.zip\n",
      "flight_1_6/flight_1_6_price_2017-04-20.zip\n",
      "flight_1_6/flight_1_6_price_2017-04-21.zip\n",
      "flight_1_6/flight_1_6_price_2017-04-22.zip\n",
      "flight_1_6/flight_1_6_price_2017-04-23.zip\n",
      "flight_1_6/flight_1_6_price_2017-04-24.zip\n",
      "flight_1_6/flight_1_6_price_2017-04-25.zip\n",
      "flight_1_6/flight_1_6_price_2017-04-26.zip\n",
      "flight_1_6/flight_1_6_price_2017-04-27.zip\n",
      "flight_1_6/flight_1_6_price_2017-04-28.zip\n",
      "flight_1_6/flight_1_6_price_2017-04-29.zip\n",
      "flight_1_6/flight_1_6_price_2017-04-30.zip\n",
      "flight_1_6/flight_1_6_price_2017-05-01.zip\n",
      "flight_1_6/flight_1_6_price_2017-05-02.zip\n",
      "flight_1_6/flight_1_6_price_2017-05-03.zip\n",
      "flight_1_6/flight_1_6_price_2017-05-06.zip\n",
      "flight_1_6/flight_1_6_price_2017-05-07.zip\n",
      "flight_1_6/flight_1_6_price_2017-05-08.zip\n",
      "flight_1_6/flight_1_6_price_2017-05-09.zip\n",
      "flight_1_6/flight_1_6_price_2017-05-10.zip\n",
      "flight_1_6/flight_1_6_price_2017-05-11.zip\n",
      "flight_1_7/flight_1_7_price_2017-04-04.zip\n",
      "flight_1_7/flight_1_7_price_2017-04-06.zip\n",
      "flight_1_7/flight_1_7_price_2017-04-09.zip\n",
      "flight_1_7/flight_1_7_price_2017-04-13.zip\n",
      "flight_1_7/flight_1_7_price_2017-04-15.zip\n",
      "flight_1_7/flight_1_7_price_2017-04-16.zip\n",
      "flight_1_7/flight_1_7_price_2017-04-19.zip\n",
      "flight_1_7/flight_1_7_price_2017-04-20.zip\n",
      "flight_1_7/flight_1_7_price_2017-04-21.zip\n",
      "flight_1_7/flight_1_7_price_2017-04-22.zip\n",
      "flight_1_7/flight_1_7_price_2017-04-23.zip\n",
      "flight_1_7/flight_1_7_price_2017-04-24.zip\n",
      "flight_1_7/flight_1_7_price_2017-04-25.zip\n",
      "flight_1_7/flight_1_7_price_2017-04-26.zip\n",
      "flight_1_7/flight_1_7_price_2017-04-27.zip\n",
      "flight_1_7/flight_1_7_price_2017-04-28.zip\n",
      "flight_1_7/flight_1_7_price_2017-04-29.zip\n",
      "flight_1_7/flight_1_7_price_2017-04-30.zip\n",
      "flight_1_7/flight_1_7_price_2017-05-01.zip\n",
      "flight_1_7/flight_1_7_price_2017-05-02.zip\n",
      "flight_1_7/flight_1_7_price_2017-05-03.zip\n",
      "flight_1_7/flight_1_7_price_2017-05-06.zip\n",
      "flight_1_7/flight_1_7_price_2017-05-07.zip\n",
      "flight_1_7/flight_1_7_price_2017-05-08.zip\n",
      "flight_1_7/flight_1_7_price_2017-05-09.zip\n",
      "flight_1_7/flight_1_7_price_2017-05-10.zip\n",
      "flight_1_7/flight_1_7_price_2017-05-11.zip\n",
      "flight_1_8/flight_1_8_price_2017-04-04.zip\n",
      "flight_1_8/flight_1_8_price_2017-04-06.zip\n",
      "flight_1_8/flight_1_8_price_2017-04-13.zip\n",
      "flight_1_8/flight_1_8_price_2017-04-15.zip\n",
      "flight_1_8/flight_1_8_price_2017-04-16.zip\n",
      "flight_1_8/flight_1_8_price_2017-04-19.zip\n",
      "flight_1_8/flight_1_8_price_2017-04-20.zip\n",
      "flight_1_8/flight_1_8_price_2017-04-21.zip\n",
      "flight_1_8/flight_1_8_price_2017-04-22.zip\n",
      "flight_1_8/flight_1_8_price_2017-04-23.zip\n",
      "flight_1_8/flight_1_8_price_2017-04-24.zip\n",
      "flight_1_8/flight_1_8_price_2017-04-25.zip\n",
      "flight_1_8/flight_1_8_price_2017-04-26.zip\n",
      "flight_1_8/flight_1_8_price_2017-04-27.zip\n",
      "flight_1_8/flight_1_8_price_2017-04-28.zip\n",
      "flight_1_8/flight_1_8_price_2017-04-29.zip\n",
      "flight_1_8/flight_1_8_price_2017-04-30.zip\n",
      "flight_1_8/flight_1_8_price_2017-05-01.zip\n",
      "flight_1_8/flight_1_8_price_2017-05-02.zip\n",
      "flight_1_8/flight_1_8_price_2017-05-03.zip\n",
      "flight_1_8/flight_1_8_price_2017-05-06.zip\n",
      "flight_1_8/flight_1_8_price_2017-05-07.zip\n",
      "flight_1_8/flight_1_8_price_2017-05-08.zip\n",
      "flight_1_8/flight_1_8_price_2017-05-09.zip\n",
      "flight_1_8/flight_1_8_price_2017-05-10.zip\n",
      "flight_1_8/flight_1_8_price_2017-05-11.zip\n",
      "flight_1_9/flight_1_9_price_2017-04-23.zip\n",
      "flight_1_9/flight_1_9_price_2017-04-24.zip\n",
      "flight_1_9/flight_1_9_price_2017-04-25.zip\n",
      "flight_1_9/flight_1_9_price_2017-04-26.zip\n",
      "flight_1_9/flight_1_9_price_2017-04-27.zip\n",
      "flight_1_9/flight_1_9_price_2017-04-28.zip\n",
      "flight_1_9/flight_1_9_price_2017-04-29.zip\n",
      "flight_1_9/flight_1_9_price_2017-04-30.zip\n",
      "flight_1_9/flight_1_9_price_2017-05-01.zip\n",
      "flight_1_9/flight_1_9_price_2017-05-02.zip\n",
      "flight_1_9/flight_1_9_price_2017-05-03.zip\n",
      "flight_1_9/flight_1_9_price_2017-05-06.zip\n",
      "flight_1_9/flight_1_9_price_2017-05-07.zip\n",
      "flight_1_9/flight_1_9_price_2017-05-08.zip\n",
      "flight_1_9/flight_1_9_price_2017-05-09.zip\n",
      "flight_1_9/flight_1_9_price_2017-05-10.zip\n",
      "flight_1_9/flight_1_9_price_2017-05-11.zip\n",
      "flight_2_10/flight_2_10_price_2017-04-23.zip\n",
      "flight_2_10/flight_2_10_price_2017-04-24.zip\n",
      "flight_2_10/flight_2_10_price_2017-04-25.zip\n",
      "flight_2_10/flight_2_10_price_2017-04-26.zip\n",
      "flight_2_10/flight_2_10_price_2017-04-27.zip\n",
      "flight_2_10/flight_2_10_price_2017-04-28.zip\n",
      "flight_2_10/flight_2_10_price_2017-04-29.zip\n",
      "flight_2_10/flight_2_10_price_2017-04-30.zip\n",
      "flight_2_10/flight_2_10_price_2017-05-01.zip\n",
      "flight_2_10/flight_2_10_price_2017-05-02.zip\n",
      "flight_2_10/flight_2_10_price_2017-05-03.zip\n",
      "flight_2_10/flight_2_10_price_2017-05-06.zip\n",
      "flight_2_10/flight_2_10_price_2017-05-07.zip\n",
      "flight_2_10/flight_2_10_price_2017-05-08.zip\n",
      "flight_2_10/flight_2_10_price_2017-05-09.zip\n",
      "flight_2_10/flight_2_10_price_2017-05-10.zip\n",
      "flight_2_10/flight_2_10_price_2017-05-11.zip\n",
      "flight_2_11/flight_2_11_price_2017-04-23.zip\n",
      "flight_2_11/flight_2_11_price_2017-04-24.zip\n",
      "flight_2_11/flight_2_11_price_2017-04-25.zip\n",
      "flight_2_11/flight_2_11_price_2017-04-26.zip\n",
      "flight_2_11/flight_2_11_price_2017-04-27.zip\n",
      "flight_2_11/flight_2_11_price_2017-04-28.zip\n",
      "flight_2_11/flight_2_11_price_2017-04-29.zip\n",
      "flight_2_11/flight_2_11_price_2017-04-30.zip\n",
      "flight_2_11/flight_2_11_price_2017-05-01.zip\n",
      "flight_2_11/flight_2_11_price_2017-05-02.zip\n",
      "flight_2_11/flight_2_11_price_2017-05-03.zip\n",
      "flight_2_11/flight_2_11_price_2017-05-06.zip\n",
      "flight_2_11/flight_2_11_price_2017-05-07.zip\n",
      "flight_2_11/flight_2_11_price_2017-05-08.zip\n",
      "flight_2_11/flight_2_11_price_2017-05-09.zip\n",
      "flight_2_11/flight_2_11_price_2017-05-10.zip\n",
      "flight_2_11/flight_2_11_price_2017-05-11.zip\n",
      "flight_2_12/flight_2_12_price_2017-04-23.zip\n",
      "flight_2_12/flight_2_12_price_2017-04-24.zip\n",
      "flight_2_12/flight_2_12_price_2017-04-25.zip\n",
      "flight_2_12/flight_2_12_price_2017-04-26.zip\n",
      "flight_2_12/flight_2_12_price_2017-04-27.zip\n",
      "flight_2_12/flight_2_12_price_2017-04-28.zip\n",
      "flight_2_12/flight_2_12_price_2017-04-29.zip\n",
      "flight_2_12/flight_2_12_price_2017-04-30.zip\n",
      "flight_2_12/flight_2_12_price_2017-05-01.zip\n",
      "flight_2_12/flight_2_12_price_2017-05-02.zip\n",
      "flight_2_12/flight_2_12_price_2017-05-03.zip\n",
      "flight_2_12/flight_2_12_price_2017-05-06.zip\n",
      "flight_2_12/flight_2_12_price_2017-05-07.zip\n",
      "flight_2_12/flight_2_12_price_2017-05-08.zip\n",
      "flight_2_12/flight_2_12_price_2017-05-09.zip\n",
      "flight_2_12/flight_2_12_price_2017-05-10.zip\n",
      "flight_2_12/flight_2_12_price_2017-05-11.zip\n",
      "flight_2_13/flight_2_13_price_2017-04-23.zip\n",
      "flight_2_13/flight_2_13_price_2017-04-24.zip\n",
      "flight_2_13/flight_2_13_price_2017-04-25.zip\n",
      "flight_2_13/flight_2_13_price_2017-04-26.zip\n",
      "flight_2_13/flight_2_13_price_2017-04-27.zip\n",
      "flight_2_13/flight_2_13_price_2017-04-28.zip\n",
      "flight_2_13/flight_2_13_price_2017-04-29.zip\n",
      "flight_2_13/flight_2_13_price_2017-04-30.zip\n",
      "flight_2_13/flight_2_13_price_2017-05-01.zip\n",
      "flight_2_13/flight_2_13_price_2017-05-02.zip\n",
      "flight_2_13/flight_2_13_price_2017-05-03.zip\n",
      "flight_2_13/flight_2_13_price_2017-05-06.zip\n",
      "flight_2_13/flight_2_13_price_2017-05-07.zip\n",
      "flight_2_13/flight_2_13_price_2017-05-08.zip\n",
      "flight_2_13/flight_2_13_price_2017-05-09.zip\n",
      "flight_2_13/flight_2_13_price_2017-05-10.zip\n",
      "flight_2_13/flight_2_13_price_2017-05-11.zip\n",
      "flight_2_14/flight_2_14_price_2017-04-23.zip\n",
      "flight_2_14/flight_2_14_price_2017-04-24.zip\n",
      "flight_2_14/flight_2_14_price_2017-04-25.zip\n",
      "flight_2_14/flight_2_14_price_2017-04-26.zip\n",
      "flight_2_14/flight_2_14_price_2017-04-27.zip\n",
      "flight_2_14/flight_2_14_price_2017-04-28.zip\n",
      "flight_2_14/flight_2_14_price_2017-04-29.zip\n",
      "flight_2_14/flight_2_14_price_2017-04-30.zip\n",
      "flight_2_14/flight_2_14_price_2017-05-01.zip\n",
      "flight_2_14/flight_2_14_price_2017-05-02.zip\n",
      "flight_2_14/flight_2_14_price_2017-05-06.zip\n",
      "flight_2_14/flight_2_14_price_2017-05-07.zip\n",
      "flight_2_14/flight_2_14_price_2017-05-08.zip\n",
      "flight_2_14/flight_2_14_price_2017-05-09.zip\n",
      "flight_2_14/flight_2_14_price_2017-05-10.zip\n",
      "flight_2_14/flight_2_14_price_2017-05-11.zip\n",
      "flight_2_5/flight_2_5_price_2017-04-13.zip\n",
      "flight_2_5/flight_2_5_price_2017-04-14.zip\n",
      "flight_2_5/flight_2_5_price_2017-04-15.zip\n",
      "flight_2_5/flight_2_5_price_2017-04-16.zip\n",
      "flight_2_5/flight_2_5_price_2017-04-19.zip\n",
      "flight_2_5/flight_2_5_price_2017-04-20.zip\n",
      "flight_2_5/flight_2_5_price_2017-04-21.zip\n",
      "flight_2_5/flight_2_5_price_2017-04-22.zip\n",
      "flight_2_5/flight_2_5_price_2017-04-23.zip\n",
      "flight_2_5/flight_2_5_price_2017-04-24.zip\n",
      "flight_2_5/flight_2_5_price_2017-04-25.zip\n",
      "flight_2_5/flight_2_5_price_2017-04-26.zip\n",
      "flight_2_5/flight_2_5_price_2017-04-27.zip\n",
      "flight_2_5/flight_2_5_price_2017-04-28.zip\n",
      "flight_2_5/flight_2_5_price_2017-04-29.zip\n",
      "flight_2_5/flight_2_5_price_2017-04-30.zip\n",
      "flight_2_5/flight_2_5_price_2017-05-01.zip\n",
      "flight_2_5/flight_2_5_price_2017-05-02.zip\n",
      "flight_2_5/flight_2_5_price_2017-05-03.zip\n",
      "flight_2_5/flight_2_5_price_2017-05-06.zip\n",
      "flight_2_5/flight_2_5_price_2017-05-07.zip\n",
      "flight_2_5/flight_2_5_price_2017-05-08.zip\n",
      "flight_2_5/flight_2_5_price_2017-05-09.zip\n",
      "flight_2_5/flight_2_5_price_2017-05-10.zip\n",
      "flight_2_5/flight_2_5_price_2017-05-11.zip\n",
      "flight_2_6/flight_2_6_price_2017-04-04.zip\n",
      "flight_2_6/flight_2_6_price_2017-04-06.zip\n",
      "flight_2_6/flight_2_6_price_2017-04-09.zip\n",
      "flight_2_6/flight_2_6_price_2017-04-13.zip\n",
      "flight_2_6/flight_2_6_price_2017-04-15.zip\n",
      "flight_2_6/flight_2_6_price_2017-04-16.zip\n",
      "flight_2_6/flight_2_6_price_2017-04-19.zip\n",
      "flight_2_6/flight_2_6_price_2017-04-20.zip\n",
      "flight_2_6/flight_2_6_price_2017-04-21.zip\n",
      "flight_2_6/flight_2_6_price_2017-04-22.zip\n",
      "flight_2_6/flight_2_6_price_2017-04-23.zip\n",
      "flight_2_6/flight_2_6_price_2017-04-24.zip\n",
      "flight_2_6/flight_2_6_price_2017-04-25.zip\n",
      "flight_2_6/flight_2_6_price_2017-04-26.zip\n",
      "flight_2_6/flight_2_6_price_2017-04-27.zip\n",
      "flight_2_6/flight_2_6_price_2017-04-28.zip\n",
      "flight_2_6/flight_2_6_price_2017-04-29.zip\n",
      "flight_2_6/flight_2_6_price_2017-04-30.zip\n",
      "flight_2_6/flight_2_6_price_2017-05-01.zip\n",
      "flight_2_6/flight_2_6_price_2017-05-02.zip\n",
      "flight_2_6/flight_2_6_price_2017-05-03.zip\n",
      "flight_2_6/flight_2_6_price_2017-05-06.zip\n",
      "flight_2_6/flight_2_6_price_2017-05-07.zip\n",
      "flight_2_6/flight_2_6_price_2017-05-08.zip\n",
      "flight_2_6/flight_2_6_price_2017-05-09.zip\n",
      "flight_2_6/flight_2_6_price_2017-05-10.zip\n",
      "flight_2_6/flight_2_6_price_2017-05-11.zip\n",
      "flight_2_7/flight_2_7_price_2017-04-04.zip\n",
      "flight_2_7/flight_2_7_price_2017-04-06.zip\n",
      "flight_2_7/flight_2_7_price_2017-04-13.zip\n",
      "flight_2_7/flight_2_7_price_2017-04-15.zip\n",
      "flight_2_7/flight_2_7_price_2017-04-16.zip\n",
      "flight_2_7/flight_2_7_price_2017-04-19.zip\n",
      "flight_2_7/flight_2_7_price_2017-04-20.zip\n",
      "flight_2_7/flight_2_7_price_2017-04-21.zip\n",
      "flight_2_7/flight_2_7_price_2017-04-22.zip\n",
      "flight_2_7/flight_2_7_price_2017-04-23.zip\n",
      "flight_2_7/flight_2_7_price_2017-04-24.zip\n",
      "flight_2_7/flight_2_7_price_2017-04-25.zip\n",
      "flight_2_7/flight_2_7_price_2017-04-26.zip\n",
      "flight_2_7/flight_2_7_price_2017-04-27.zip\n",
      "flight_2_7/flight_2_7_price_2017-04-28.zip\n",
      "flight_2_7/flight_2_7_price_2017-04-29.zip\n",
      "flight_2_7/flight_2_7_price_2017-04-30.zip\n",
      "flight_2_7/flight_2_7_price_2017-05-01.zip\n",
      "flight_2_7/flight_2_7_price_2017-05-02.zip\n",
      "flight_2_7/flight_2_7_price_2017-05-03.zip\n",
      "flight_2_7/flight_2_7_price_2017-05-06.zip\n",
      "flight_2_7/flight_2_7_price_2017-05-07.zip\n",
      "flight_2_7/flight_2_7_price_2017-05-08.zip\n",
      "flight_2_7/flight_2_7_price_2017-05-09.zip\n",
      "flight_2_7/flight_2_7_price_2017-05-10.zip\n",
      "flight_2_7/flight_2_7_price_2017-05-11.zip\n",
      "flight_2_8/flight_2_8_price_2017-04-04.zip\n",
      "flight_2_8/flight_2_8_price_2017-04-06.zip\n",
      "flight_2_8/flight_2_8_price_2017-04-09.zip\n",
      "flight_2_8/flight_2_8_price_2017-04-13.zip\n",
      "flight_2_8/flight_2_8_price_2017-04-14.zip\n",
      "flight_2_8/flight_2_8_price_2017-04-15.zip\n",
      "flight_2_8/flight_2_8_price_2017-04-16.zip\n",
      "flight_2_8/flight_2_8_price_2017-04-19.zip\n",
      "flight_2_8/flight_2_8_price_2017-04-20.zip\n",
      "flight_2_8/flight_2_8_price_2017-04-21.zip\n",
      "flight_2_8/flight_2_8_price_2017-04-22.zip\n",
      "flight_2_8/flight_2_8_price_2017-04-23.zip\n",
      "flight_2_8/flight_2_8_price_2017-04-24.zip\n",
      "flight_2_8/flight_2_8_price_2017-04-25.zip\n",
      "flight_2_8/flight_2_8_price_2017-04-26.zip\n",
      "flight_2_8/flight_2_8_price_2017-04-27.zip\n",
      "flight_2_8/flight_2_8_price_2017-04-28.zip\n",
      "flight_2_8/flight_2_8_price_2017-04-29.zip\n",
      "flight_2_8/flight_2_8_price_2017-04-30.zip\n",
      "flight_2_8/flight_2_8_price_2017-05-01.zip\n",
      "flight_2_8/flight_2_8_price_2017-05-02.zip\n",
      "flight_2_8/flight_2_8_price_2017-05-03.zip\n",
      "flight_2_8/flight_2_8_price_2017-05-06.zip\n",
      "flight_2_8/flight_2_8_price_2017-05-07.zip\n",
      "flight_2_8/flight_2_8_price_2017-05-08.zip\n",
      "flight_2_8/flight_2_8_price_2017-05-09.zip\n",
      "flight_2_8/flight_2_8_price_2017-05-10.zip\n",
      "flight_2_8/flight_2_8_price_2017-05-11.zip\n",
      "flight_2_9/flight_2_9_price_2017-04-23.zip\n",
      "flight_2_9/flight_2_9_price_2017-04-24.zip\n",
      "flight_2_9/flight_2_9_price_2017-04-25.zip\n",
      "flight_2_9/flight_2_9_price_2017-04-26.zip\n",
      "flight_2_9/flight_2_9_price_2017-04-27.zip\n",
      "flight_2_9/flight_2_9_price_2017-04-28.zip\n",
      "flight_2_9/flight_2_9_price_2017-04-29.zip\n",
      "flight_2_9/flight_2_9_price_2017-04-30.zip\n",
      "flight_2_9/flight_2_9_price_2017-05-01.zip\n",
      "flight_2_9/flight_2_9_price_2017-05-02.zip\n",
      "flight_2_9/flight_2_9_price_2017-05-03.zip\n",
      "flight_2_9/flight_2_9_price_2017-05-06.zip\n",
      "flight_2_9/flight_2_9_price_2017-05-07.zip\n",
      "flight_2_9/flight_2_9_price_2017-05-08.zip\n",
      "flight_2_9/flight_2_9_price_2017-05-09.zip\n",
      "flight_2_9/flight_2_9_price_2017-05-10.zip\n",
      "flight_2_9/flight_2_9_price_2017-05-11.zip\n",
      "flight_3_10/flight_3_10_price_2017-04-23.zip\n",
      "flight_3_10/flight_3_10_price_2017-04-24.zip\n",
      "flight_3_10/flight_3_10_price_2017-04-25.zip\n",
      "flight_3_10/flight_3_10_price_2017-04-26.zip\n",
      "flight_3_10/flight_3_10_price_2017-04-27.zip\n",
      "flight_3_10/flight_3_10_price_2017-04-28.zip\n",
      "flight_3_10/flight_3_10_price_2017-04-29.zip\n",
      "flight_3_10/flight_3_10_price_2017-04-30.zip\n",
      "flight_3_10/flight_3_10_price_2017-05-01.zip\n",
      "flight_3_10/flight_3_10_price_2017-05-02.zip\n",
      "flight_3_10/flight_3_10_price_2017-05-06.zip\n",
      "flight_3_10/flight_3_10_price_2017-05-07.zip\n",
      "flight_3_10/flight_3_10_price_2017-05-08.zip\n",
      "flight_3_10/flight_3_10_price_2017-05-09.zip\n",
      "flight_3_10/flight_3_10_price_2017-05-10.zip\n",
      "flight_3_10/flight_3_10_price_2017-05-11.zip\n",
      "flight_3_11/flight_3_11_price_2017-04-23.zip\n",
      "flight_3_11/flight_3_11_price_2017-04-24.zip\n",
      "flight_3_11/flight_3_11_price_2017-04-25.zip\n",
      "flight_3_11/flight_3_11_price_2017-04-26.zip\n",
      "flight_3_11/flight_3_11_price_2017-04-27.zip\n",
      "flight_3_11/flight_3_11_price_2017-04-28.zip\n",
      "flight_3_11/flight_3_11_price_2017-04-29.zip\n",
      "flight_3_11/flight_3_11_price_2017-04-30.zip\n",
      "flight_3_11/flight_3_11_price_2017-05-01.zip\n",
      "flight_3_11/flight_3_11_price_2017-05-02.zip\n",
      "flight_3_11/flight_3_11_price_2017-05-06.zip\n",
      "flight_3_11/flight_3_11_price_2017-05-07.zip\n",
      "flight_3_11/flight_3_11_price_2017-05-08.zip\n",
      "flight_3_11/flight_3_11_price_2017-05-09.zip\n",
      "flight_3_11/flight_3_11_price_2017-05-10.zip\n",
      "flight_3_11/flight_3_11_price_2017-05-11.zip\n",
      "flight_3_12/flight_3_12_price_2017-04-23.zip\n",
      "flight_3_12/flight_3_12_price_2017-04-24.zip\n",
      "flight_3_12/flight_3_12_price_2017-04-25.zip\n",
      "flight_3_12/flight_3_12_price_2017-04-26.zip\n",
      "flight_3_12/flight_3_12_price_2017-04-27.zip\n",
      "flight_3_12/flight_3_12_price_2017-04-28.zip\n",
      "flight_3_12/flight_3_12_price_2017-04-29.zip\n",
      "flight_3_12/flight_3_12_price_2017-04-30.zip\n",
      "flight_3_12/flight_3_12_price_2017-05-01.zip\n",
      "flight_3_12/flight_3_12_price_2017-05-02.zip\n",
      "flight_3_12/flight_3_12_price_2017-05-06.zip\n",
      "flight_3_12/flight_3_12_price_2017-05-07.zip\n",
      "flight_3_12/flight_3_12_price_2017-05-08.zip\n",
      "flight_3_12/flight_3_12_price_2017-05-09.zip\n",
      "flight_3_12/flight_3_12_price_2017-05-10.zip\n",
      "flight_3_12/flight_3_12_price_2017-05-11.zip\n",
      "flight_3_13/flight_3_13_price_2017-04-23.zip\n",
      "flight_3_13/flight_3_13_price_2017-04-24.zip\n",
      "flight_3_13/flight_3_13_price_2017-04-25.zip\n",
      "flight_3_13/flight_3_13_price_2017-04-26.zip\n",
      "flight_3_13/flight_3_13_price_2017-04-27.zip\n",
      "flight_3_13/flight_3_13_price_2017-04-28.zip\n",
      "flight_3_13/flight_3_13_price_2017-04-29.zip\n",
      "flight_3_13/flight_3_13_price_2017-04-30.zip\n",
      "flight_3_13/flight_3_13_price_2017-05-01.zip\n",
      "flight_3_13/flight_3_13_price_2017-05-02.zip\n",
      "flight_3_13/flight_3_13_price_2017-05-06.zip\n",
      "flight_3_13/flight_3_13_price_2017-05-07.zip\n",
      "flight_3_13/flight_3_13_price_2017-05-08.zip\n",
      "flight_3_13/flight_3_13_price_2017-05-09.zip\n",
      "flight_3_13/flight_3_13_price_2017-05-10.zip\n",
      "flight_3_13/flight_3_13_price_2017-05-11.zip\n",
      "flight_3_14/flight_3_14_price_2017-04-23.zip\n",
      "flight_3_14/flight_3_14_price_2017-04-24.zip\n",
      "flight_3_14/flight_3_14_price_2017-04-26.zip\n",
      "flight_3_14/flight_3_14_price_2017-04-27.zip\n",
      "flight_3_14/flight_3_14_price_2017-04-28.zip\n",
      "flight_3_14/flight_3_14_price_2017-04-29.zip\n",
      "flight_3_14/flight_3_14_price_2017-04-30.zip\n",
      "flight_3_14/flight_3_14_price_2017-05-01.zip\n",
      "flight_3_14/flight_3_14_price_2017-05-02.zip\n",
      "flight_3_14/flight_3_14_price_2017-05-06.zip\n",
      "flight_3_14/flight_3_14_price_2017-05-07.zip\n",
      "flight_3_14/flight_3_14_price_2017-05-08.zip\n",
      "flight_3_14/flight_3_14_price_2017-05-09.zip\n",
      "flight_3_14/flight_3_14_price_2017-05-10.zip\n",
      "flight_3_14/flight_3_14_price_2017-05-11.zip\n",
      "flight_3_5/flight_3_5_price_2017-04-04.zip\n",
      "flight_3_5/flight_3_5_price_2017-04-06.zip\n",
      "flight_3_5/flight_3_5_price_2017-04-13.zip\n",
      "flight_3_5/flight_3_5_price_2017-04-14.zip\n",
      "flight_3_5/flight_3_5_price_2017-04-15.zip\n",
      "flight_3_5/flight_3_5_price_2017-04-16.zip\n",
      "flight_3_5/flight_3_5_price_2017-04-19.zip\n",
      "flight_3_5/flight_3_5_price_2017-04-20.zip\n",
      "flight_3_5/flight_3_5_price_2017-04-21.zip\n",
      "flight_3_5/flight_3_5_price_2017-04-22.zip\n",
      "flight_3_5/flight_3_5_price_2017-04-23.zip\n",
      "flight_3_5/flight_3_5_price_2017-04-24.zip\n",
      "flight_3_5/flight_3_5_price_2017-04-25.zip\n",
      "flight_3_5/flight_3_5_price_2017-04-26.zip\n",
      "flight_3_5/flight_3_5_price_2017-04-27.zip\n",
      "flight_3_5/flight_3_5_price_2017-04-28.zip\n",
      "flight_3_5/flight_3_5_price_2017-04-29.zip\n",
      "flight_3_5/flight_3_5_price_2017-04-30.zip\n",
      "flight_3_5/flight_3_5_price_2017-05-01.zip\n",
      "flight_3_5/flight_3_5_price_2017-05-02.zip\n",
      "flight_3_5/flight_3_5_price_2017-05-06.zip\n",
      "flight_3_5/flight_3_5_price_2017-05-07.zip\n",
      "flight_3_5/flight_3_5_price_2017-05-08.zip\n",
      "flight_3_5/flight_3_5_price_2017-05-09.zip\n",
      "flight_3_5/flight_3_5_price_2017-05-10.zip\n",
      "flight_3_5/flight_3_5_price_2017-05-11.zip\n",
      "flight_3_6/flight_3_6_price_2017-04-04.zip\n",
      "flight_3_6/flight_3_6_price_2017-04-06.zip\n",
      "flight_3_6/flight_3_6_price_2017-04-13.zip\n",
      "flight_3_6/flight_3_6_price_2017-04-14.zip\n",
      "flight_3_6/flight_3_6_price_2017-04-15.zip\n",
      "flight_3_6/flight_3_6_price_2017-04-16.zip\n",
      "flight_3_6/flight_3_6_price_2017-04-19.zip\n",
      "flight_3_6/flight_3_6_price_2017-04-20.zip\n",
      "flight_3_6/flight_3_6_price_2017-04-21.zip\n",
      "flight_3_6/flight_3_6_price_2017-04-22.zip\n",
      "flight_3_6/flight_3_6_price_2017-04-23.zip\n",
      "flight_3_6/flight_3_6_price_2017-04-24.zip\n",
      "flight_3_6/flight_3_6_price_2017-04-25.zip\n",
      "flight_3_6/flight_3_6_price_2017-04-26.zip\n",
      "flight_3_6/flight_3_6_price_2017-04-27.zip\n",
      "flight_3_6/flight_3_6_price_2017-04-28.zip\n",
      "flight_3_6/flight_3_6_price_2017-04-29.zip\n",
      "flight_3_6/flight_3_6_price_2017-04-30.zip\n",
      "flight_3_6/flight_3_6_price_2017-05-01.zip\n",
      "flight_3_6/flight_3_6_price_2017-05-02.zip\n",
      "flight_3_6/flight_3_6_price_2017-05-06.zip\n",
      "flight_3_6/flight_3_6_price_2017-05-07.zip\n",
      "flight_3_6/flight_3_6_price_2017-05-08.zip\n",
      "flight_3_6/flight_3_6_price_2017-05-09.zip\n",
      "flight_3_6/flight_3_6_price_2017-05-10.zip\n",
      "flight_3_6/flight_3_6_price_2017-05-11.zip\n",
      "flight_3_7/flight_3_7_price_2017-04-04.zip\n",
      "flight_3_7/flight_3_7_price_2017-04-06.zip\n",
      "flight_3_7/flight_3_7_price_2017-04-13.zip\n",
      "flight_3_7/flight_3_7_price_2017-04-14.zip\n",
      "flight_3_7/flight_3_7_price_2017-04-15.zip\n",
      "flight_3_7/flight_3_7_price_2017-04-16.zip\n",
      "flight_3_7/flight_3_7_price_2017-04-18.zip\n",
      "flight_3_7/flight_3_7_price_2017-04-19.zip\n",
      "flight_3_7/flight_3_7_price_2017-04-20.zip\n",
      "flight_3_7/flight_3_7_price_2017-04-21.zip\n",
      "flight_3_7/flight_3_7_price_2017-04-22.zip\n",
      "flight_3_7/flight_3_7_price_2017-04-23.zip\n",
      "flight_3_7/flight_3_7_price_2017-04-24.zip\n",
      "flight_3_7/flight_3_7_price_2017-04-25.zip\n",
      "flight_3_7/flight_3_7_price_2017-04-26.zip\n",
      "flight_3_7/flight_3_7_price_2017-04-27.zip\n",
      "flight_3_7/flight_3_7_price_2017-04-28.zip\n",
      "flight_3_7/flight_3_7_price_2017-04-29.zip\n",
      "flight_3_7/flight_3_7_price_2017-04-30.zip\n",
      "flight_3_7/flight_3_7_price_2017-05-01.zip\n",
      "flight_3_7/flight_3_7_price_2017-05-02.zip\n",
      "flight_3_7/flight_3_7_price_2017-05-06.zip\n",
      "flight_3_7/flight_3_7_price_2017-05-07.zip\n",
      "flight_3_7/flight_3_7_price_2017-05-08.zip\n",
      "flight_3_7/flight_3_7_price_2017-05-09.zip\n",
      "flight_3_7/flight_3_7_price_2017-05-10.zip\n",
      "flight_3_7/flight_3_7_price_2017-05-11.zip\n",
      "flight_3_8/flight_3_8_price_2017-04-04.zip\n",
      "flight_3_8/flight_3_8_price_2017-04-06.zip\n",
      "flight_3_8/flight_3_8_price_2017-04-13.zip\n",
      "flight_3_8/flight_3_8_price_2017-04-14.zip\n",
      "flight_3_8/flight_3_8_price_2017-04-15.zip\n",
      "flight_3_8/flight_3_8_price_2017-04-16.zip\n",
      "flight_3_8/flight_3_8_price_2017-04-19.zip\n",
      "flight_3_8/flight_3_8_price_2017-04-20.zip\n",
      "flight_3_8/flight_3_8_price_2017-04-21.zip\n",
      "flight_3_8/flight_3_8_price_2017-04-22.zip\n",
      "flight_3_8/flight_3_8_price_2017-04-23.zip\n",
      "flight_3_8/flight_3_8_price_2017-04-24.zip\n",
      "flight_3_8/flight_3_8_price_2017-04-25.zip\n",
      "flight_3_8/flight_3_8_price_2017-04-26.zip\n",
      "flight_3_8/flight_3_8_price_2017-04-27.zip\n",
      "flight_3_8/flight_3_8_price_2017-04-28.zip\n",
      "flight_3_8/flight_3_8_price_2017-04-29.zip\n",
      "flight_3_8/flight_3_8_price_2017-04-30.zip\n",
      "flight_3_8/flight_3_8_price_2017-05-01.zip\n",
      "flight_3_8/flight_3_8_price_2017-05-02.zip\n",
      "flight_3_8/flight_3_8_price_2017-05-06.zip\n",
      "flight_3_8/flight_3_8_price_2017-05-07.zip\n",
      "flight_3_8/flight_3_8_price_2017-05-08.zip\n",
      "flight_3_8/flight_3_8_price_2017-05-09.zip\n",
      "flight_3_8/flight_3_8_price_2017-05-10.zip\n",
      "flight_3_8/flight_3_8_price_2017-05-11.zip\n",
      "flight_3_9/flight_3_9_price_2017-04-23.zip\n",
      "flight_3_9/flight_3_9_price_2017-04-24.zip\n",
      "flight_3_9/flight_3_9_price_2017-04-25.zip\n",
      "flight_3_9/flight_3_9_price_2017-04-26.zip\n",
      "flight_3_9/flight_3_9_price_2017-04-27.zip\n",
      "flight_3_9/flight_3_9_price_2017-04-28.zip\n",
      "flight_3_9/flight_3_9_price_2017-04-29.zip\n",
      "flight_3_9/flight_3_9_price_2017-04-30.zip\n",
      "flight_3_9/flight_3_9_price_2017-05-01.zip\n",
      "flight_3_9/flight_3_9_price_2017-05-02.zip\n",
      "flight_3_9/flight_3_9_price_2017-05-06.zip\n",
      "flight_3_9/flight_3_9_price_2017-05-07.zip\n",
      "flight_3_9/flight_3_9_price_2017-05-08.zip\n",
      "flight_3_9/flight_3_9_price_2017-05-09.zip\n",
      "flight_3_9/flight_3_9_price_2017-05-10.zip\n",
      "flight_3_9/flight_3_9_price_2017-05-11.zip\n",
      "flight_4_10/flight_4_10_price_2017-04-25.zip\n",
      "flight_4_10/flight_4_10_price_2017-04-26.zip\n",
      "flight_4_10/flight_4_10_price_2017-04-27.zip\n",
      "flight_4_10/flight_4_10_price_2017-04-28.zip\n",
      "flight_4_10/flight_4_10_price_2017-04-29.zip\n",
      "flight_4_10/flight_4_10_price_2017-04-30.zip\n",
      "flight_4_10/flight_4_10_price_2017-05-01.zip\n",
      "flight_4_10/flight_4_10_price_2017-05-02.zip\n",
      "flight_4_10/flight_4_10_price_2017-05-03.zip\n",
      "flight_4_10/flight_4_10_price_2017-05-06.zip\n",
      "flight_4_10/flight_4_10_price_2017-05-07.zip\n",
      "flight_4_10/flight_4_10_price_2017-05-08.zip\n",
      "flight_4_10/flight_4_10_price_2017-05-09.zip\n",
      "flight_4_10/flight_4_10_price_2017-05-10.zip\n",
      "flight_4_10/flight_4_10_price_2017-05-11.zip\n",
      "flight_4_11/flight_4_11_price_2017-04-25.zip\n",
      "flight_4_11/flight_4_11_price_2017-04-26.zip\n",
      "flight_4_11/flight_4_11_price_2017-04-27.zip\n",
      "flight_4_11/flight_4_11_price_2017-04-28.zip\n",
      "flight_4_11/flight_4_11_price_2017-04-29.zip\n",
      "flight_4_11/flight_4_11_price_2017-04-30.zip\n",
      "flight_4_11/flight_4_11_price_2017-05-01.zip\n",
      "flight_4_11/flight_4_11_price_2017-05-02.zip\n",
      "flight_4_11/flight_4_11_price_2017-05-03.zip\n",
      "flight_4_11/flight_4_11_price_2017-05-06.zip\n",
      "flight_4_11/flight_4_11_price_2017-05-07.zip\n",
      "flight_4_11/flight_4_11_price_2017-05-08.zip\n",
      "flight_4_11/flight_4_11_price_2017-05-09.zip\n",
      "flight_4_11/flight_4_11_price_2017-05-10.zip\n",
      "flight_4_11/flight_4_11_price_2017-05-11.zip\n",
      "flight_4_12/flight_4_12_price_2017-04-25.zip\n",
      "flight_4_12/flight_4_12_price_2017-04-26.zip\n",
      "flight_4_12/flight_4_12_price_2017-04-27.zip\n",
      "flight_4_12/flight_4_12_price_2017-04-28.zip\n",
      "flight_4_12/flight_4_12_price_2017-04-29.zip\n",
      "flight_4_12/flight_4_12_price_2017-04-30.zip\n",
      "flight_4_12/flight_4_12_price_2017-05-01.zip\n",
      "flight_4_12/flight_4_12_price_2017-05-02.zip\n",
      "flight_4_12/flight_4_12_price_2017-05-03.zip\n",
      "flight_4_12/flight_4_12_price_2017-05-06.zip\n",
      "flight_4_12/flight_4_12_price_2017-05-07.zip\n",
      "flight_4_12/flight_4_12_price_2017-05-08.zip\n",
      "flight_4_12/flight_4_12_price_2017-05-09.zip\n",
      "flight_4_12/flight_4_12_price_2017-05-10.zip\n",
      "flight_4_12/flight_4_12_price_2017-05-11.zip\n",
      "flight_4_13/flight_4_13_price_2017-04-25.zip\n",
      "flight_4_13/flight_4_13_price_2017-04-26.zip\n",
      "flight_4_13/flight_4_13_price_2017-04-27.zip\n",
      "flight_4_13/flight_4_13_price_2017-04-28.zip\n",
      "flight_4_13/flight_4_13_price_2017-04-29.zip\n",
      "flight_4_13/flight_4_13_price_2017-04-30.zip\n",
      "flight_4_13/flight_4_13_price_2017-05-01.zip\n",
      "flight_4_13/flight_4_13_price_2017-05-02.zip\n",
      "flight_4_13/flight_4_13_price_2017-05-03.zip\n",
      "flight_4_13/flight_4_13_price_2017-05-06.zip\n",
      "flight_4_13/flight_4_13_price_2017-05-07.zip\n",
      "flight_4_13/flight_4_13_price_2017-05-08.zip\n",
      "flight_4_13/flight_4_13_price_2017-05-09.zip\n",
      "flight_4_13/flight_4_13_price_2017-05-10.zip\n",
      "flight_4_13/flight_4_13_price_2017-05-11.zip\n",
      "flight_4_14/flight_4_14_price_2017-04-25.zip\n",
      "flight_4_14/flight_4_14_price_2017-04-26.zip\n",
      "flight_4_14/flight_4_14_price_2017-04-27.zip\n",
      "flight_4_14/flight_4_14_price_2017-04-28.zip\n",
      "flight_4_14/flight_4_14_price_2017-04-29.zip\n",
      "flight_4_14/flight_4_14_price_2017-04-30.zip\n",
      "flight_4_14/flight_4_14_price_2017-05-01.zip\n",
      "flight_4_14/flight_4_14_price_2017-05-02.zip\n",
      "flight_4_14/flight_4_14_price_2017-05-03.zip\n",
      "flight_4_14/flight_4_14_price_2017-05-06.zip\n",
      "flight_4_14/flight_4_14_price_2017-05-07.zip\n",
      "flight_4_14/flight_4_14_price_2017-05-08.zip\n",
      "flight_4_14/flight_4_14_price_2017-05-09.zip\n",
      "flight_4_14/flight_4_14_price_2017-05-10.zip\n",
      "flight_4_14/flight_4_14_price_2017-05-11.zip\n",
      "flight_4_5/flight_4_5_price_2017-04-04.zip\n",
      "flight_4_5/flight_4_5_price_2017-04-06.zip\n",
      "flight_4_5/flight_4_5_price_2017-04-13.zip\n",
      "flight_4_5/flight_4_5_price_2017-04-14.zip\n",
      "flight_4_5/flight_4_5_price_2017-04-15.zip\n",
      "flight_4_5/flight_4_5_price_2017-04-16.zip\n",
      "flight_4_5/flight_4_5_price_2017-04-19.zip\n",
      "flight_4_5/flight_4_5_price_2017-04-20.zip\n",
      "flight_4_5/flight_4_5_price_2017-04-21.zip\n",
      "flight_4_5/flight_4_5_price_2017-04-22.zip\n",
      "flight_4_5/flight_4_5_price_2017-04-25.zip\n",
      "flight_4_5/flight_4_5_price_2017-04-26.zip\n",
      "flight_4_5/flight_4_5_price_2017-04-27.zip\n",
      "flight_4_5/flight_4_5_price_2017-04-28.zip\n",
      "flight_4_5/flight_4_5_price_2017-04-29.zip\n",
      "flight_4_5/flight_4_5_price_2017-04-30.zip\n"
     ]
    }
   ],
   "source": [
    "for item in all_objects['Contents']:\n",
    "    print(item['Key'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look at v1.1 jsonl files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/s3/comb/txt_exception/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "74603"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_exception_folder = '/home/ubuntu/s3/comb/txt_exception/'\n",
    "print(txt_exception_folder)\n",
    "\n",
    "\n",
    "# for file in glob.glob(os.path.join(txt_exception_folder, \"flight_15_13_price_2017-05-11_2017-11-0*.txt\")):\n",
    "# #     flightv1_1 = spark.read.json(file)\n",
    "#     print(flightv1_1.count())\n",
    "\n",
    "# for file in glob.glob(os.path.join(ext_fldr, \"*.txt\")):        \n",
    "#         with open(file) as f:                           \n",
    "#             d = json.load(f)\n",
    "\n",
    "# flightv1_1.count()\n",
    "\n",
    "flightv1_1 = spark.read.json(os.path.join(txt_exception_folder, \"flight_15_13_price_2017-05-11*.txt\"))\n",
    "# flightv1_1 = spark.read.json('s3n://flight.price/flight_15_13/*.txt') # not working yet - to be investigated\n",
    "flightv1_1.count()\n",
    "# flightv1_1.first()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|trip|\n",
      "+----+\n",
      "|   1|\n",
      "|   2|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightv1_1.select('trip').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+--------------------+-----------+--------+------------------+----------+--------+------------------+-------+--------------------+-------------+--------+----+--------------------+-------+\n",
      "|currencyCode|   depDate|         flight_leg1|flight_leg2|fromCity|             price|searchDate|stayDays|         tableName|task_id|       timeline_leg1|timeline_leg2|  toCity|trip|                 url|version|\n",
      "+------------+----------+--------------------+-----------+--------+------------------+----------+--------+------------------+-------+--------------------+-------------+--------+----+--------------------+-------+\n",
      "|         AUD|2017-10-22|[[Hangzhou,HGH],2...|       null| Bangkok|            192.91|2017-05-11|       0|flight_15_13_price|  17016|[[[Hangzhou, Chin...|         null|Hangzhou|   1|https://www.exped...|    1.1|\n",
      "|         AUD|2017-10-22|[[Hangzhou,HGH],2...|       null| Bangkok|214.04999999999998|2017-05-11|       0|flight_15_13_price|  17016|[[[Kuala Lumpur, ...|         null|Hangzhou|   1|https://www.exped...|    1.1|\n",
      "+------------+----------+--------------------+-----------+--------+------------------+----------+--------+------------------+-------+--------------------+-------------+--------+----+--------------------+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightv1_1.where(flightv1_1.trip == 1).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- airline_code: string (nullable = true)\n",
      " |-- airline_codes: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- arr_time: string (nullable = true)\n",
      " |-- check_bag_inc: boolean (nullable = true)\n",
      " |-- company: string (nullable = true)\n",
      " |-- dep_time: string (nullable = true)\n",
      " |-- duration: string (nullable = true)\n",
      " |-- flight_code: string (nullable = true)\n",
      " |-- flight_number: string (nullable = true)\n",
      " |-- from_city_name: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- index: long (nullable = true)\n",
      " |-- plane: string (nullable = true)\n",
      " |-- power: boolean (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- price_code: string (nullable = true)\n",
      " |-- search_date: string (nullable = true)\n",
      " |-- span_days: long (nullable = true)\n",
      " |-- start_date: string (nullable = true)\n",
      " |-- stay_days: string (nullable = true)\n",
      " |-- stop: long (nullable = true)\n",
      " |-- stop_info: string (nullable = true)\n",
      " |-- table_name: string (nullable = true)\n",
      " |-- task_id: string (nullable = true)\n",
      " |-- ticket_left: long (nullable = true)\n",
      " |-- to_city_name: string (nullable = true)\n",
      " |-- trip: string (nullable = true)\n",
      " |-- version: string (nullable = true)\n",
      " |-- video: boolean (nullable = true)\n",
      " |-- wifi: boolean (nullable = true)\n",
      "\n",
      "root\n",
      " |-- currencyCode: string (nullable = true)\n",
      " |-- depDate: string (nullable = true)\n",
      " |-- flight_leg1: struct (nullable = true)\n",
      " |    |-- arrivalLocation: struct (nullable = true)\n",
      " |    |    |-- airportCity: string (nullable = true)\n",
      " |    |    |-- airportCode: string (nullable = true)\n",
      " |    |-- arrivalTime: string (nullable = true)\n",
      " |    |-- carrierSummary: struct (nullable = true)\n",
      " |    |    |-- airProviderId: long (nullable = true)\n",
      " |    |    |-- airlineCodes: array (nullable = true)\n",
      " |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |-- airlineImageFileName: string (nullable = true)\n",
      " |    |    |-- airlineName: string (nullable = true)\n",
      " |    |    |-- displayUrgencyMessage: boolean (nullable = true)\n",
      " |    |    |-- mixedCabinClass: boolean (nullable = true)\n",
      " |    |    |-- multiStop: boolean (nullable = true)\n",
      " |    |    |-- nextDayArrival: boolean (nullable = true)\n",
      " |    |    |-- noOfTicketsLeft: long (nullable = true)\n",
      " |    |    |-- oneTicketLeft: boolean (nullable = true)\n",
      " |    |    |-- seatMapAvailable: boolean (nullable = true)\n",
      " |    |-- departureLocation: struct (nullable = true)\n",
      " |    |    |-- airportCity: string (nullable = true)\n",
      " |    |    |-- airportCode: string (nullable = true)\n",
      " |    |-- departureTime: string (nullable = true)\n",
      " |    |-- stop_list: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- airport: string (nullable = true)\n",
      " |    |    |    |-- duration: string (nullable = true)\n",
      " |    |-- stops: long (nullable = true)\n",
      " |-- flight_leg2: struct (nullable = true)\n",
      " |    |-- arrivalLocation: struct (nullable = true)\n",
      " |    |    |-- airportCity: string (nullable = true)\n",
      " |    |    |-- airportCode: string (nullable = true)\n",
      " |    |-- arrivalTime: string (nullable = true)\n",
      " |    |-- carrierSummary: struct (nullable = true)\n",
      " |    |    |-- airProviderId: long (nullable = true)\n",
      " |    |    |-- airlineCodes: array (nullable = true)\n",
      " |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |-- airlineImageFileName: string (nullable = true)\n",
      " |    |    |-- airlineName: string (nullable = true)\n",
      " |    |    |-- displayUrgencyMessage: boolean (nullable = true)\n",
      " |    |    |-- mixedCabinClass: boolean (nullable = true)\n",
      " |    |    |-- multiStop: boolean (nullable = true)\n",
      " |    |    |-- nextDayArrival: boolean (nullable = true)\n",
      " |    |    |-- noOfTicketsLeft: long (nullable = true)\n",
      " |    |    |-- oneTicketLeft: boolean (nullable = true)\n",
      " |    |    |-- seatMapAvailable: boolean (nullable = true)\n",
      " |    |-- departureLocation: struct (nullable = true)\n",
      " |    |    |-- airportCity: string (nullable = true)\n",
      " |    |    |-- airportCode: string (nullable = true)\n",
      " |    |-- departureTime: string (nullable = true)\n",
      " |    |-- stop_list: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- airport: string (nullable = true)\n",
      " |    |    |    |-- duration: string (nullable = true)\n",
      " |    |-- stops: long (nullable = true)\n",
      " |-- fromCity: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- searchDate: string (nullable = true)\n",
      " |-- stayDays: long (nullable = true)\n",
      " |-- tableName: string (nullable = true)\n",
      " |-- task_id: long (nullable = true)\n",
      " |-- timeline_leg1: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- arrivalAirport: struct (nullable = true)\n",
      " |    |    |    |-- airportCityState: string (nullable = true)\n",
      " |    |    |    |-- city: string (nullable = true)\n",
      " |    |    |    |-- code: string (nullable = true)\n",
      " |    |    |    |-- localName: string (nullable = true)\n",
      " |    |    |    |-- longName: string (nullable = true)\n",
      " |    |    |    |-- name: string (nullable = true)\n",
      " |    |    |-- arrivalTime: struct (nullable = true)\n",
      " |    |    |    |-- date: string (nullable = true)\n",
      " |    |    |    |-- dateLongStr: string (nullable = true)\n",
      " |    |    |    |-- dateTime: long (nullable = true)\n",
      " |    |    |    |-- hour: string (nullable = true)\n",
      " |    |    |    |-- isoStr: string (nullable = true)\n",
      " |    |    |    |-- time: string (nullable = true)\n",
      " |    |    |    |-- travelDate: string (nullable = true)\n",
      " |    |    |-- brandedFareName: string (nullable = true)\n",
      " |    |    |-- carrier: struct (nullable = true)\n",
      " |    |    |    |-- airlineCode: string (nullable = true)\n",
      " |    |    |    |-- airlineImageFileNameWithoutExtension: string (nullable = true)\n",
      " |    |    |    |-- airlineName: string (nullable = true)\n",
      " |    |    |    |-- bookingCode: string (nullable = true)\n",
      " |    |    |    |-- cabinClass: string (nullable = true)\n",
      " |    |    |    |-- flightNumber: string (nullable = true)\n",
      " |    |    |    |-- plane: string (nullable = true)\n",
      " |    |    |    |-- planeCode: string (nullable = true)\n",
      " |    |    |-- departureAirport: struct (nullable = true)\n",
      " |    |    |    |-- airportCityState: string (nullable = true)\n",
      " |    |    |    |-- city: string (nullable = true)\n",
      " |    |    |    |-- code: string (nullable = true)\n",
      " |    |    |    |-- localName: string (nullable = true)\n",
      " |    |    |    |-- longName: string (nullable = true)\n",
      " |    |    |    |-- name: string (nullable = true)\n",
      " |    |    |-- departureTime: struct (nullable = true)\n",
      " |    |    |    |-- date: string (nullable = true)\n",
      " |    |    |    |-- dateLongStr: string (nullable = true)\n",
      " |    |    |    |-- dateTime: long (nullable = true)\n",
      " |    |    |    |-- hour: string (nullable = true)\n",
      " |    |    |    |-- isoStr: string (nullable = true)\n",
      " |    |    |    |-- time: string (nullable = true)\n",
      " |    |    |    |-- travelDate: string (nullable = true)\n",
      " |    |    |-- distance: struct (nullable = true)\n",
      " |    |    |    |-- formattedTotal: string (nullable = true)\n",
      " |    |    |    |-- total: long (nullable = true)\n",
      " |    |    |    |-- unit: string (nullable = true)\n",
      " |    |    |-- duration: struct (nullable = true)\n",
      " |    |    |    |-- hours: long (nullable = true)\n",
      " |    |    |    |-- minutes: long (nullable = true)\n",
      " |    |    |-- layover: boolean (nullable = true)\n",
      " |    |    |-- segment: boolean (nullable = true)\n",
      " |    |    |-- type: string (nullable = true)\n",
      " |-- timeline_leg2: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- arrivalAirport: struct (nullable = true)\n",
      " |    |    |    |-- airportCityState: string (nullable = true)\n",
      " |    |    |    |-- city: string (nullable = true)\n",
      " |    |    |    |-- code: string (nullable = true)\n",
      " |    |    |    |-- localName: string (nullable = true)\n",
      " |    |    |    |-- longName: string (nullable = true)\n",
      " |    |    |    |-- name: string (nullable = true)\n",
      " |    |    |-- arrivalTime: struct (nullable = true)\n",
      " |    |    |    |-- date: string (nullable = true)\n",
      " |    |    |    |-- dateLongStr: string (nullable = true)\n",
      " |    |    |    |-- dateTime: long (nullable = true)\n",
      " |    |    |    |-- hour: string (nullable = true)\n",
      " |    |    |    |-- isoStr: string (nullable = true)\n",
      " |    |    |    |-- time: string (nullable = true)\n",
      " |    |    |    |-- travelDate: string (nullable = true)\n",
      " |    |    |-- brandedFareName: string (nullable = true)\n",
      " |    |    |-- carrier: struct (nullable = true)\n",
      " |    |    |    |-- airlineCode: string (nullable = true)\n",
      " |    |    |    |-- airlineImageFileNameWithoutExtension: string (nullable = true)\n",
      " |    |    |    |-- airlineName: string (nullable = true)\n",
      " |    |    |    |-- bookingCode: string (nullable = true)\n",
      " |    |    |    |-- cabinClass: string (nullable = true)\n",
      " |    |    |    |-- flightNumber: string (nullable = true)\n",
      " |    |    |    |-- plane: string (nullable = true)\n",
      " |    |    |    |-- planeCode: string (nullable = true)\n",
      " |    |    |-- departureAirport: struct (nullable = true)\n",
      " |    |    |    |-- airportCityState: string (nullable = true)\n",
      " |    |    |    |-- city: string (nullable = true)\n",
      " |    |    |    |-- code: string (nullable = true)\n",
      " |    |    |    |-- localName: string (nullable = true)\n",
      " |    |    |    |-- longName: string (nullable = true)\n",
      " |    |    |    |-- name: string (nullable = true)\n",
      " |    |    |-- departureTime: struct (nullable = true)\n",
      " |    |    |    |-- date: string (nullable = true)\n",
      " |    |    |    |-- dateLongStr: string (nullable = true)\n",
      " |    |    |    |-- dateTime: long (nullable = true)\n",
      " |    |    |    |-- hour: string (nullable = true)\n",
      " |    |    |    |-- isoStr: string (nullable = true)\n",
      " |    |    |    |-- time: string (nullable = true)\n",
      " |    |    |    |-- travelDate: string (nullable = true)\n",
      " |    |    |-- distance: struct (nullable = true)\n",
      " |    |    |    |-- formattedTotal: string (nullable = true)\n",
      " |    |    |    |-- total: long (nullable = true)\n",
      " |    |    |    |-- unit: string (nullable = true)\n",
      " |    |    |-- duration: struct (nullable = true)\n",
      " |    |    |    |-- hours: long (nullable = true)\n",
      " |    |    |    |-- minutes: long (nullable = true)\n",
      " |    |    |-- layover: boolean (nullable = true)\n",
      " |    |    |-- segment: boolean (nullable = true)\n",
      " |    |    |-- type: string (nullable = true)\n",
      " |-- toCity: string (nullable = true)\n",
      " |-- trip: long (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      " |-- version: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flight.printSchema()\n",
    "flightv1_1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+--------------------+-------------+----------------+--------------------+--------+-----------+-------------+--------------+----+-----+--------------------+-----+-------+----------+-----------+---------+----------+---------+----+-----------------+----------------+-------+-----------+------------+----+-------+-----+-----+\n",
      "|airline_code|airline_codes|            arr_time|check_bag_inc|         company|            dep_time|duration|flight_code|flight_number|from_city_name|  id|index|               plane|power|  price|price_code|search_date|span_days|start_date|stay_days|stop|        stop_info|      table_name|task_id|ticket_left|to_city_name|trip|version|video| wifi|\n",
      "+------------+-------------+--------------------+-------------+----------------+--------------------+--------+-----------+-------------+--------------+----+-----+--------------------+-----+-------+----------+-----------+---------+----------+---------+----+-----------------+----------------+-------+-----------+------------+----+-------+-----+-----+\n",
      "|          QF|         [QF]|2017-04-05T19:20:...|        false|  Qantas Airways|2017-04-05T11:00:...|  10h20m|      QF301|          301|        sydney|null|   12|AIRBUS INDUSTRIE ...|false|1178.42|       AUD| 2017-04-04|        0|2017-04-05|        0|   0|                 |flight_1_6_price|    901|          9|    shanghai|   1|    1.0| true|false|\n",
      "|          VN|     [VN, VN]|2017-04-06T14:25:...|        false|Vietnam Airlines|2017-04-05T14:15:...|  26h10m|      VN786|          786|        sydney|null|   69|          Boeing 787|false|1095.22|       AUD| 2017-04-04|        0|2017-04-05|        0|   1|Hanoi(HAN):13h35m|flight_1_6_price|    901|          4|    shanghai|   1|    1.0|false|false|\n",
      "+------------+-------------+--------------------+-------------+----------------+--------------------+--------+-----------+-------------+--------------+----+-----+--------------------+-----+-------+----------+-----------+---------+----------+---------+----+-----------------+----------------+-------+-----------+------------+----+-------+-----+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flight.where(flight.trip == 1).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify some v1.0 columns before appending v1.1 onto v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+----------+----------------+-------+------------+--------+-------+----+----------+-------+--------+--------------------+--------------------+--------------+----------+-----------+----------------+-----+---------------+------------+-------------+--------------------+---------+-----+-----+-----+\n",
      "| price|version|searchDate|       tableName|task_id|currencyCode|fromCity| toCity|trip|   depDate|retDate|stayDays|       departureTime|         arrivalTime|   airlineName|duration_m|flight_code|           plane|stops|noOfTicketsLeft|airline_code|airline_codes|           stop_info|span_days|power|video| wifi|\n",
      "+------+-------+----------+----------------+-------+------------+--------+-------+----+----------+-------+--------+--------------------+--------------------+--------------+----------+-----------+----------------+-----+---------------+------------+-------------+--------------------+---------+-----+-----+-----+\n",
      "|605.72|    1.0|2017-05-01|flight_1_5_price|    676|         AUD|  sydney|beijing|   1|2017-09-14|   null|    null|2017-09-14T21:55:...|2017-09-15T18:20:...|Cathay Pacific|    1345.0|      CX138|BOEING 777-300ER|    1|              9|          CX|     [CX, CX]|[Hong Kong(HKG):9...|        0| true| true|false|\n",
      "+------+-------+----------+----------------+-------+------------+--------+-------+----+----------+-------+--------+--------------------+--------------------+--------------+----------+-----------+----------------+-----+---------------+------------+-------------+--------------------+---------+-----+-----+-----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+----------+----------------+-------+------------+--------+-------+----+----------+----------+--------+-----------------------------+-----------------------------+--------------+----------+-----------+-----------------------------------+-----+---------------+------------+-------------+---------------------+---------+-----+-----+----+\n",
      "|price|version|searchDate|tableName       |task_id|currencyCode|fromCity|toCity |trip|depDate   |retDate   |stayDays|departureTime                |arrivalTime                  |airlineName   |duration_m|flight_code|plane                              |stops|noOfTicketsLeft|airline_code|airline_codes|stop_info            |span_days|power|video|wifi|\n",
      "+-----+-------+----------+----------------+-------+------------+--------+-------+----+----------+----------+--------+-----------------------------+-----------------------------+--------------+----------+-----------+-----------------------------------+-----+---------------+------------+-------------+---------------------+---------+-----+-----+----+\n",
      "|0.0  |1.0    |2017-05-08|flight_1_5_price|620    |            |sydney  |beijing|2   |2017-09-09|2017-10-07|28      |2017-09-09T11:15:00.000+10:00|2017-09-10T04:10:00.000+08:00|Qantas Airways|1135.0    |QF145      |BOEING 737-800 (WINGLETS) PASSENGER|1    |999            |QF          |[QF, CA]     |[Auckland(AKL):2h35m]|0        |null |null |null|\n",
      "+-----+-------+----------+----------------+-------+------------+--------+-------+----+----------+----------+--------+-----------------------------+-----------------------------+--------------+----------+-----------+-----------------------------------+-----+---------------+------------+-------------+---------------------+---------+-----+-----+----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- price: double (nullable = true)\n",
      " |-- version: string (nullable = true)\n",
      " |-- searchDate: string (nullable = true)\n",
      " |-- tableName: string (nullable = true)\n",
      " |-- task_id: string (nullable = true)\n",
      " |-- currencyCode: string (nullable = true)\n",
      " |-- fromCity: string (nullable = true)\n",
      " |-- toCity: string (nullable = true)\n",
      " |-- trip: string (nullable = true)\n",
      " |-- depDate: string (nullable = true)\n",
      " |-- retDate: date (nullable = true)\n",
      " |-- stayDays: integer (nullable = true)\n",
      " |-- departureTime: string (nullable = true)\n",
      " |-- arrivalTime: string (nullable = true)\n",
      " |-- airlineName: string (nullable = true)\n",
      " |-- duration_m: double (nullable = true)\n",
      " |-- flight_code: string (nullable = true)\n",
      " |-- plane: string (nullable = true)\n",
      " |-- stops: long (nullable = true)\n",
      " |-- noOfTicketsLeft: integer (nullable = true)\n",
      " |-- airline_code: string (nullable = true)\n",
      " |-- airline_codes: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- stop_info: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- span_days: long (nullable = true)\n",
      " |-- power: boolean (nullable = true)\n",
      " |-- video: boolean (nullable = true)\n",
      " |-- wifi: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# for one way trips, display None in stay_days\n",
    "def correct_stay_days(trip, stay_days):\n",
    "    if trip == '1':\n",
    "        return None\n",
    "    else:\n",
    "        return int(stay_days)\n",
    "\n",
    "correct_stay_days_UDF = udf(correct_stay_days, IntegerType())\n",
    "\n",
    "def correct_tickets_left(noOfTicketsLeft):\n",
    "    if noOfTicketsLeft == 0:\n",
    "        return 999\n",
    "    else:\n",
    "        return noOfTicketsLeft\n",
    "    \n",
    "correct_tickets_left_UDF = udf(correct_tickets_left, IntegerType())\n",
    "\n",
    "\n",
    "flight2 = (flight.withColumn('stayDays', correct_stay_days_UDF(col('trip'), col('stay_days')))\n",
    "                 .drop('stay_days')\n",
    "                 .withColumnRenamed('start_date', 'depDate')                 \n",
    "                 .selectExpr('*', 'date_add(depDate, stayDays) as retDate')# this is when the return trip starts, might arrive a day later\n",
    "                 .withColumnRenamed('from_city_name', 'fromCity')\n",
    "                 .withColumnRenamed('to_city_name', 'toCity')                 \n",
    "                 .withColumnRenamed('search_date', 'searchDate')                 \n",
    "                 .withColumnRenamed('company', 'airlineName')                 \n",
    "                 .withColumnRenamed('dep_time', 'departureTime')                                  \n",
    "                 .withColumnRenamed('arr_time', 'arrivalTime')                                                   \n",
    "                 .withColumn('duration_h', split(flight.duration,'h').getItem(0))\n",
    "                 .withColumn('duration_m', F.substring_index(split(flight.duration,'h').getItem(1), 'm', 1))\n",
    "#                  .withColumn('duration', F.struct(col('duration_h'), col('duration_m')))\n",
    "                 .withColumn('duration_m', (col('duration_h')*60 + col('duration_m')))\n",
    "                 .drop('duration', 'duration_h', 'flight_number')\n",
    "                 .withColumnRenamed('price_code', 'currencyCode')                                  \n",
    "                 .withColumnRenamed('stop', 'stops')                                  \n",
    "                 .withColumn('stop_info', split(col('stop_info'), ';'))\n",
    "                 .withColumn('noOfTicketsLeft', correct_tickets_left_UDF('ticket_left'))\n",
    "                .drop('ticket_left')\n",
    "               .withColumnRenamed('table_name', 'tableName')\n",
    "                .select('price', 'version', 'searchDate', 'tableName', 'task_id', 'currencyCode', \n",
    "                        'fromCity', 'toCity', 'trip', 'depDate', 'retDate',\n",
    "                        'stayDays', \n",
    "                       'departureTime', 'arrivalTime', \n",
    "                        'airlineName',  'duration_m', \n",
    "                        'flight_code', 'plane', 'stops', 'noOfTicketsLeft',\n",
    "                       'airline_code', 'airline_codes',\n",
    "                       'stop_info', 'span_days', 'power', 'video', 'wifi')                \n",
    "          )\n",
    "# varaibles added in v1.1: 'departureTime_leg2', 'arrivalTime_leg2', 'airlineName_leg2','duration_m_leg2','stops_leg2'\n",
    "#  'noOfTicketsLeft_leg2','airline_codes_leg2', \n",
    "# 'stop_list', 'url'\n",
    "\n",
    "# variables dropped in v1.1:\n",
    "# 'span_days', 'power', 'video', 'wifi', 'stop_info'\n",
    "\n",
    "display(flight2.where(col('trip') == 1).show(1))\n",
    "display(flight2.where(col('trip') == 2).show(1, truncate=False))\n",
    "flight2.printSchema()\n",
    "\n",
    "# flight2.select('flight_code', 'flight_number').distinct().show(1000)\n",
    "# flight2.select('stop_info').distinct().show()\n",
    "# flight2.select('stop_list').distinct().show(100, truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V1.1 dataframe\n",
    "Need to generate  \n",
    "* airline_code how to generate? is this for the outbound trip?\n",
    "* airline_codes array\n",
    "* arr_time is this for the outbound trip?\n",
    "* check_bag_inc is this for the outbound trip? - **Can't find it in v1.1!!!**\n",
    "* company is this for the outbound trip?\n",
    "* dep_time is this for the outbound trip?\n",
    "* duration is this for the outbound trip? ** timeline leg 1 first element??**\n",
    "* flight_code: string (nullable = true)\n",
    "* flight_number: string (nullable = true)\n",
    "* index: long (nullable = true)\n",
    "* plane: string (nullable = true)\n",
    "* power: boolean (nullable = true) - **Can't find it in v1.1!!!**\n",
    "* price: double (nullable = true) OK no change.\n",
    "* price_code: string (nullable = true) - ** currencyCode? **\n",
    "* search_date: string (nullable = true)\n",
    "* span_days: long (nullable = true) - **Can't find it in v1.1!!!**\n",
    "* stop: long (nullable = true)\n",
    "* stop_info: string (nullable = true) - Need to work out how to make this compatible with stop_list\n",
    "* ticket_left: long (nullable = true)\n",
    "* video: boolean (nullable = true) - **Can't find it in v1.1!!!**\n",
    "* wifi: boolean (nullable = true) - **Can't find it in v1.1!!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+----------+------------------+-------+------------+--------+--------+----+----------+-------+--------+--------------------+--------------------+------------------+----------------+------------+----------------+----------+---------------+-----------+-----+-----+----------+---------+--------------+---------------+--------------------+------------+-------------+------------------+--------------------+-------------------+-----------------+------------------------+----------------------+--------------------+---------------------------+----------------------+----------------+---------------------+-------------------------+--------------------------------+---------------------------+---------------------+--------------------------+\n",
      "| price|version|searchDate|         tableName|task_id|currencyCode|fromCity|  toCity|trip|   depDate|retDate|stayDays|       departureTime|         arrivalTime|departureTime_leg2|arrivalTime_leg2| airlineName|airlineName_leg2|duration_m|duration_m_leg2|flight_code|plane|stops|stops_leg2|stop_list|stop_list_leg2|noOfTicketsLeft|noOfTicketsLeft_leg2|airline_code|airline_codes|airline_codes_leg2|                 url|fromCityAirportCode|toCityAirportCode|fromCityAirportCode_leg2|toCityAirportCode_leg2|carrierAirProviderId|carrierAirlineImageFileName|carrierMixedCabinClass|carrierMultiStop|carrierNextDayArrival|carrierAirProviderId_leg2|carrierAirlineImageFileName_leg2|carrierMixedCabinClass_leg2|carrierMultiStop_leg2|carrierNextDayArrival_leg2|\n",
      "+------+-------+----------+------------------+-------+------------+--------+--------+----+----------+-------+--------+--------------------+--------------------+------------------+----------------+------------+----------------+----------+---------------+-----------+-----+-----+----------+---------+--------------+---------------+--------------------+------------+-------------+------------------+--------------------+-------------------+-----------------+------------------------+----------------------+--------------------+---------------------------+----------------------+----------------+---------------------+-------------------------+--------------------------------+---------------------------+---------------------+--------------------------+\n",
      "|192.91|    1.1|2017-05-11|flight_15_13_price|  17016|         AUD| Bangkok|Hangzhou|   1|2017-10-22|   null|    null|2017-10-22T07:10:...|2017-10-22T11:35:...|              null|            null|Thai AirAsia|            null|     265.0|           null|      FD566|     |    0|      null|       []|            []|            999|                null|          FD|         [FD]|              null|https://www.exped...|                DMK|              HGH|                    null|                  null|                  75|                     FD.gif|                 false|           false|                false|                     null|                            null|                       null|                 null|                      null|\n",
      "+------+-------+----------+------------------+-------+------------+--------+--------+----+----------+-------+--------+--------------------+--------------------+------------------+----------------+------------+----------------+----------+---------------+-----------+-----+-----+----------+---------+--------------+---------------+--------------------+------------+-------------+------------------+--------------------+-------------------+-----------------+------------------------+----------------------+--------------------+---------------------------+----------------------+----------------+---------------------+-------------------------+--------------------------------+---------------------------+---------------------+--------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+----------+------------------+-------+------------+--------+--------+----+----------+----------+--------+--------------------+--------------------+--------------------+--------------------+-----------------+-----------------+----------+---------------+-----------+-----------+-----+----------+--------------------+--------------------+---------------+--------------------+------------+-------------+------------------+--------------------+-------------------+-----------------+------------------------+----------------------+--------------------+---------------------------+----------------------+----------------+---------------------+-------------------------+--------------------------------+---------------------------+---------------------+--------------------------+\n",
      "|price|version|searchDate|         tableName|task_id|currencyCode|fromCity|  toCity|trip|   depDate|   retDate|stayDays|       departureTime|         arrivalTime|  departureTime_leg2|    arrivalTime_leg2|      airlineName| airlineName_leg2|duration_m|duration_m_leg2|flight_code|      plane|stops|stops_leg2|           stop_list|      stop_list_leg2|noOfTicketsLeft|noOfTicketsLeft_leg2|airline_code|airline_codes|airline_codes_leg2|                 url|fromCityAirportCode|toCityAirportCode|fromCityAirportCode_leg2|toCityAirportCode_leg2|carrierAirProviderId|carrierAirlineImageFileName|carrierMixedCabinClass|carrierMultiStop|carrierNextDayArrival|carrierAirProviderId_leg2|carrierAirlineImageFileName_leg2|carrierMixedCabinClass_leg2|carrierMultiStop_leg2|carrierNextDayArrival_leg2|\n",
      "+-----+-------+----------+------------------+-------+------------+--------+--------+----+----------+----------+--------+--------------------+--------------------+--------------------+--------------------+-----------------+-----------------+----------+---------------+-----------+-----------+-----+----------+--------------------+--------------------+---------------+--------------------+------------+-------------+------------------+--------------------+-------------------+-----------------+------------------------+----------------------+--------------------+---------------------------+----------------------+----------------+---------------------+-------------------------+--------------------------------+---------------------------+---------------------+--------------------------+\n",
      "|401.3|    1.1|2017-05-11|flight_15_13_price|  16232|         AUD| Bangkok|Hangzhou|   2|2017-05-18|2017-05-25|       7|2017-05-18T12:15:...|2017-05-18T22:50:...|2017-05-25T09:00:...|2017-05-25T21:00:...|Air Macau Company|Air Macau Company|     635.0|          720.0|      NX885|Airbus A320|    1|         1|[[Macau, Macau (M...|[[Macau, Macau (M...|              4|                   4|          NX|     [NX, NX]|          [NX, NX]|https://www.exped...|                BKK|              HGH|                     HGH|                   BKK|                   7|                     NX.gif|                 false|            true|                false|                        7|                          NX.gif|                      false|                 true|                     false|\n",
      "+-----+-------+----------+------------------+-------+------------+--------+--------+----+----------+----------+--------+--------------------+--------------------+--------------------+--------------------+-----------------+-----------------+----------+---------------+-----------+-----------+-----+----------+--------------------+--------------------+---------------+--------------------+------------+-------------+------------------+--------------------+-------------------+-----------------+------------------------+----------------------+--------------------+---------------------------+----------------------+----------------+---------------------+-------------------------+--------------------------------+---------------------------+---------------------+--------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- price: double (nullable = true)\n",
      " |-- version: string (nullable = true)\n",
      " |-- searchDate: string (nullable = true)\n",
      " |-- tableName: string (nullable = true)\n",
      " |-- task_id: long (nullable = true)\n",
      " |-- currencyCode: string (nullable = true)\n",
      " |-- fromCity: string (nullable = true)\n",
      " |-- toCity: string (nullable = true)\n",
      " |-- trip: string (nullable = true)\n",
      " |-- depDate: string (nullable = true)\n",
      " |-- retDate: date (nullable = true)\n",
      " |-- stayDays: integer (nullable = true)\n",
      " |-- departureTime: string (nullable = true)\n",
      " |-- arrivalTime: string (nullable = true)\n",
      " |-- departureTime_leg2: string (nullable = true)\n",
      " |-- arrivalTime_leg2: string (nullable = true)\n",
      " |-- airlineName: string (nullable = true)\n",
      " |-- airlineName_leg2: string (nullable = true)\n",
      " |-- duration_m: double (nullable = true)\n",
      " |-- duration_m_leg2: double (nullable = true)\n",
      " |-- flight_code: string (nullable = true)\n",
      " |-- plane: string (nullable = true)\n",
      " |-- stops: long (nullable = true)\n",
      " |-- stops_leg2: long (nullable = true)\n",
      " |-- stop_list: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- airport: string (nullable = true)\n",
      " |    |    |-- duration: string (nullable = true)\n",
      " |-- stop_list_leg2: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- airport: string (nullable = true)\n",
      " |    |    |-- duration: string (nullable = true)\n",
      " |-- noOfTicketsLeft: integer (nullable = true)\n",
      " |-- noOfTicketsLeft_leg2: integer (nullable = true)\n",
      " |-- airline_code: string (nullable = true)\n",
      " |-- airline_codes: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- airline_codes_leg2: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- url: string (nullable = true)\n",
      " |-- fromCityAirportCode: string (nullable = true)\n",
      " |-- toCityAirportCode: string (nullable = true)\n",
      " |-- fromCityAirportCode_leg2: string (nullable = true)\n",
      " |-- toCityAirportCode_leg2: string (nullable = true)\n",
      " |-- carrierAirProviderId: long (nullable = true)\n",
      " |-- carrierAirlineImageFileName: string (nullable = true)\n",
      " |-- carrierMixedCabinClass: boolean (nullable = true)\n",
      " |-- carrierMultiStop: boolean (nullable = true)\n",
      " |-- carrierNextDayArrival: boolean (nullable = true)\n",
      " |-- carrierAirProviderId_leg2: long (nullable = true)\n",
      " |-- carrierAirlineImageFileName_leg2: string (nullable = true)\n",
      " |-- carrierMixedCabinClass_leg2: boolean (nullable = true)\n",
      " |-- carrierMultiStop_leg2: boolean (nullable = true)\n",
      " |-- carrierNextDayArrival_leg2: boolean (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------+\n",
      "|airport                                                              |\n",
      "+---------------------------------------------------------------------+\n",
      "|[Macau, Macau (MFM-Macau Intl.)]                                     |\n",
      "|[Macau, Macau (MFM-Macau Intl.)]                                     |\n",
      "|[]                                                                   |\n",
      "|[Wuhan, China (WUH-Tianhe Intl.)]                                    |\n",
      "|[Shenzhen, China (SZX-Shenzhen Intl.)]                               |\n",
      "|[Shenzhen, China (SZX-Shenzhen Intl.)]                               |\n",
      "|[]                                                                   |\n",
      "|[Shenzhen, China (SZX-Shenzhen Intl.)]                               |\n",
      "|[Guangzhou, China (CAN-Baiyun Intl.)]                                |\n",
      "|[Shenzhen, China (SZX-Shenzhen Intl.)]                               |\n",
      "|[Guangzhou, China (CAN-Baiyun Intl.)]                                |\n",
      "|[Guangzhou, China (CAN-Baiyun Intl.)]                                |\n",
      "|[Guangzhou, China (CAN-Baiyun Intl.)]                                |\n",
      "|[Wuhan, China (WUH-Tianhe Intl.)]                                    |\n",
      "|[Guangzhou, China (CAN-Baiyun Intl.)]                                |\n",
      "|[Guangzhou, China (CAN-Baiyun Intl.)]                                |\n",
      "|[]                                                                   |\n",
      "|[Wuhan, China (WUH-Tianhe Intl.)]                                    |\n",
      "|[Guangzhou, China (CAN-Baiyun Intl.)]                                |\n",
      "|[Kuala Lumpur, Malaysia (KUL-Kuala Lumpur Intl.)]                    |\n",
      "|[Kuala Lumpur, Malaysia (KUL-Kuala Lumpur Intl.)]                    |\n",
      "|[Shenzhen, China (SZX-Shenzhen Intl.)]                               |\n",
      "|[Shenzhen, China (SZX-Shenzhen Intl.)]                               |\n",
      "|[Shenzhen, China (SZX-Shenzhen Intl.)]                               |\n",
      "|[Kuala Lumpur, Malaysia (KUL-Kuala Lumpur Intl.)]                    |\n",
      "|[Kuala Lumpur, Malaysia (KUL-Kuala Lumpur Intl.)]                    |\n",
      "|[Kuala Lumpur, Malaysia (KUL-Kuala Lumpur Intl.)]                    |\n",
      "|[Kuala Lumpur, Malaysia (KUL-Kuala Lumpur Intl.)]                    |\n",
      "|[Kuala Lumpur, Malaysia (KUL-Kuala Lumpur Intl.)]                    |\n",
      "|[Shenzhen, China (SZX-Shenzhen Intl.)]                               |\n",
      "|[Guangzhou, China (CAN-Baiyun Intl.)]                                |\n",
      "|[Kuala Lumpur, Malaysia (KUL-Kuala Lumpur Intl.)]                    |\n",
      "|[Shenzhen, China (SZX-Shenzhen Intl.)]                               |\n",
      "|[Beijing, China (PEK-Capital Intl.)]                                 |\n",
      "|[Guangzhou, China (CAN-Baiyun Intl.)]                                |\n",
      "|[Guangzhou, China (CAN-Baiyun Intl.)]                                |\n",
      "|[Guangzhou, China (CAN-Baiyun Intl.)]                                |\n",
      "|[Xiamen, China (XMN-Xiamen Intl.)]                                   |\n",
      "|[Shanghai, China (PVG-Pudong Intl.)]                                 |\n",
      "|[Guangzhou, China (CAN-Baiyun Intl.)]                                |\n",
      "|[Guangzhou, China (CAN-Baiyun Intl.)]                                |\n",
      "|[Shenzhen, China (SZX-Shenzhen Intl.)]                               |\n",
      "|[Guangzhou, China (CAN-Baiyun Intl.)]                                |\n",
      "|[Guangzhou, China (CAN-Baiyun Intl.)]                                |\n",
      "|[Guangzhou, China (CAN-Baiyun Intl.)]                                |\n",
      "|[Guangzhou, China (CAN-Baiyun Intl.)]                                |\n",
      "|[Guangzhou, China (CAN-Baiyun Intl.)]                                |\n",
      "|[Shenzhen, China (SZX-Shenzhen Intl.)]                               |\n",
      "|[Shenzhen, China (SZX-Shenzhen Intl.)]                               |\n",
      "|[Shenzhen, China (SZX-Shenzhen Intl.)]                               |\n",
      "|[Beijing, China (PEK-Capital Intl.)]                                 |\n",
      "|[Beijing, China (PEK-Capital Intl.)]                                 |\n",
      "|[Shenzhen, China (SZX-Shenzhen Intl.)]                               |\n",
      "|[Shenzhen, China (SZX-Shenzhen Intl.)]                               |\n",
      "|[Shenzhen, China (SZX-Shenzhen Intl.)]                               |\n",
      "|[Guangzhou, China (CAN-Baiyun Intl.)]                                |\n",
      "|[Guangzhou, China (CAN-Baiyun Intl.)]                                |\n",
      "|[Shenzhen, China (SZX-Shenzhen Intl.)]                               |\n",
      "|[Guangzhou, China (CAN-Baiyun Intl.)]                                |\n",
      "|[Shenzhen, China (SZX-Shenzhen Intl.)]                               |\n",
      "|[Shenzhen, China (SZX-Shenzhen Intl.)]                               |\n",
      "|[Guangzhou, China (CAN-Baiyun Intl.)]                                |\n",
      "|[Guangzhou, China (CAN-Baiyun Intl.)]                                |\n",
      "|[Guangzhou, China (CAN-Baiyun Intl.)]                                |\n",
      "|[Guangzhou, China (CAN-Baiyun Intl.)]                                |\n",
      "|[Guangzhou, China (CAN-Baiyun Intl.)]                                |\n",
      "|[Guangzhou, China (CAN-Baiyun Intl.)]                                |\n",
      "|[Guangzhou, China (CAN-Baiyun Intl.)]                                |\n",
      "|[Guangzhou, China (CAN-Baiyun Intl.)]                                |\n",
      "|[Guangzhou, China (CAN-Baiyun Intl.)]                                |\n",
      "|[Chengdu, China (CTU-Shuangliu Intl.)]                               |\n",
      "|[Chengdu, China (CTU-Shuangliu Intl.)]                               |\n",
      "|[Xiamen, China (XMN-Xiamen Intl.)]                                   |\n",
      "|[Shanghai, China (PVG-Pudong Intl.)]                                 |\n",
      "|[Kunming, China (KMG-Changshui Intl.)]                               |\n",
      "|[Taiyuan, China (TYN-Wusu)]                                          |\n",
      "|[Changsha, China (CSX-Huanghua Intl.)]                               |\n",
      "|[Kunming, China (KMG-Changshui Intl.)]                               |\n",
      "|[Xiamen, China (XMN-Xiamen Intl.)]                                   |\n",
      "|[Xiamen, China (XMN-Xiamen Intl.)]                                   |\n",
      "|[Fuzhou, China (FOC-Changle Intl.), Dalian, China (DLC-Dalian Intl.)]|\n",
      "|[Seoul, South Korea (ICN-Incheon Intl.)]                             |\n",
      "|[Changsha, China (CSX-Huanghua Intl.)]                               |\n",
      "|[Shanghai, China (PVG-Pudong Intl.)]                                 |\n",
      "|[Shanghai, China (PVG-Pudong Intl.)]                                 |\n",
      "|[Shanghai, China (PVG-Pudong Intl.)]                                 |\n",
      "|[Haikou, China (HAK-Meilan Intl.)]                                   |\n",
      "|[Nanning, China (NNG-Wuxu)]                                          |\n",
      "|[Seoul, South Korea (ICN-Incheon Intl.)]                             |\n",
      "|[Da Nang, Vietnam (DAD-Da Nang Intl.)]                               |\n",
      "|[Hong Kong, Hong Kong (HKG-Hong Kong Intl.)]                         |\n",
      "|[Hong Kong, Hong Kong (HKG-Hong Kong Intl.)]                         |\n",
      "|[Hong Kong, Hong Kong (HKG-Hong Kong Intl.)]                         |\n",
      "|[Hong Kong, Hong Kong (HKG-Hong Kong Intl.)]                         |\n",
      "|[Hong Kong, Hong Kong (HKG-Hong Kong Intl.)]                         |\n",
      "|[Hong Kong, Hong Kong (HKG-Hong Kong Intl.)]                         |\n",
      "|[Hong Kong, Hong Kong (HKG-Hong Kong Intl.)]                         |\n",
      "|[Hong Kong, Hong Kong (HKG-Hong Kong Intl.)]                         |\n",
      "|[Hong Kong, Hong Kong (HKG-Hong Kong Intl.)]                         |\n",
      "|[Kunming, China (KMG-Changshui Intl.)]                               |\n",
      "+---------------------------------------------------------------------+\n",
      "only showing top 100 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# flightv1_1.select(flightv1_1.depDate, F.explode(flightv1_1.timeline_leg1)).show(truncate=False)\n",
    "# flightv1_1.select(\"flight_leg1.carrierSummary.*\").show(truncate=False)\n",
    "# flightv1_1.select(\"*\").show()\n",
    "# flightv1_1.select(flightv1_1.url).show(truncate=False)\n",
    "# flightv1_1.select(col(\"flight_leg1.stop_list\").getItem(0)).show(100, truncate=False)\n",
    "# flightv1_1.select(col('flight_leg1.carrierSummary.airProviderId')).show()\n",
    "\n",
    "def get_elements_UDF(timeline_element, column_name):\n",
    "    for \n",
    "\n",
    "\n",
    "\n",
    "          \n",
    "timeFmt = \"yyyy-MM-dd'T'HH:mm:ss.SSS\"\n",
    "\n",
    "flightv1_1_2 = (flightv1_1.withColumn('trip', col('trip').cast('string'))\n",
    "                    .withColumn('stayDays', correct_stay_days_UDF(col('trip'), col('stayDays')))                    \n",
    "                    .selectExpr('*', 'date_add(depDate, stayDays) as retDate')# this is when the return trip starts, might arrive a day later\n",
    "                    .withColumn('airline_code', flightv1_1.flight_leg1.carrierSummary.airlineCodes.getItem(0))                   \n",
    "                    .withColumn('airline_codes', flightv1_1.flight_leg1.carrierSummary.airlineCodes)                    \n",
    "                    .withColumn('airline_codes_leg2', flightv1_1.flight_leg2.carrierSummary.airlineCodes)                    \n",
    "                    .withColumn('departureTime', flightv1_1.flight_leg1.departureTime)\n",
    "                    .withColumn('departureTime_leg2', flightv1_1.flight_leg2.departureTime)\n",
    "                    .withColumn('arrivalTime', flightv1_1.flight_leg1.arrivalTime)\n",
    "                    .withColumn('arrivalTime_leg2', flightv1_1.flight_leg2.arrivalTime)\n",
    "#                 .withColumn('check_bag_inc', flightv1_1.flight_leg1.arrivalTime)\n",
    "                    .withColumn('airlineName', flightv1_1.flight_leg1.carrierSummary.airlineName)\n",
    "                    .withColumn('airlineName_leg2', flightv1_1.flight_leg2.carrierSummary.airlineName)\n",
    "                    .withColumn('duration_m', (F.unix_timestamp('arrivalTime', format=timeFmt) - \n",
    "                                               F.unix_timestamp('departureTime', format=timeFmt))/60)                    \n",
    "                .withColumn('duration_m_leg2', (F.unix_timestamp('arrivalTime_leg2', format=timeFmt) - \n",
    "                                               F.unix_timestamp('departureTime_leg2', format=timeFmt))/60)                    \n",
    "#                     .withColumn('duration', flightv1_1.timeline_leg1.getItem(1).duration)\n",
    "                .withColumn('airlineCode', flightv1_1.timeline_leg1.getItem(0).carrier.airlineCode)\n",
    "                .withColumn('flightNumber', flightv1_1.timeline_leg1.getItem(0).carrier.flightNumber.cast('string'))                \n",
    "                .select('*', F.concat(col('airlineCode'), col('flightNumber')).alias('flight_code'))\n",
    "                .drop('airlineCode', 'flightNumber')\n",
    "                .withColumn('plane', flightv1_1.timeline_leg1.getItem(0).carrier.plane)                \n",
    "                .withColumn('stops', flightv1_1.flight_leg1.stops)                \n",
    "                .withColumn('stops_leg2', flightv1_1.flight_leg2.stops)                \n",
    "                .withColumn('stop_list', flightv1_1.flight_leg1.stop_list)# need to do more work                \n",
    "                .withColumn('stop_list_leg2', flightv1_1.flight_leg1.stop_list)\n",
    "                .withColumn('noOfTicketsLeft', correct_tickets_left_UDF(flightv1_1.flight_leg1.carrierSummary.noOfTicketsLeft))\n",
    "                .withColumn('noOfTicketsLeft_leg2', correct_tickets_left_UDF(flightv1_1.flight_leg2.carrierSummary.noOfTicketsLeft))\n",
    "                \n",
    "                .withColumn('fromCityAirportCode', flightv1_1.flight_leg1.departureLocation.airportCode)                \n",
    "                .withColumn('toCityAirportCode', flightv1_1.flight_leg1.arrivalLocation.airportCode)\n",
    "                .withColumn('fromCityAirportCode_leg2', flightv1_1.flight_leg2.departureLocation.airportCode)\n",
    "                .withColumn('toCityAirportCode_leg2', flightv1_1.flight_leg2.arrivalLocation.airportCode)\n",
    "                \n",
    "                .withColumn('carrierAirProviderId', flightv1_1.flight_leg1.carrierSummary.airProviderId)\n",
    "                .withColumn('carrierAirlineImageFileName', flightv1_1.flight_leg1.carrierSummary.airlineImageFileName)\n",
    "                .withColumn('carrierMixedCabinClass', flightv1_1.flight_leg1.carrierSummary.mixedCabinClass)\n",
    "                .withColumn('carrierMultiStop', flightv1_1.flight_leg1.carrierSummary.multiStop)\n",
    "                .withColumn('carrierNextDayArrival', flightv1_1.flight_leg1.carrierSummary.nextDayArrival)\n",
    "                \n",
    "                .withColumn('carrierAirProviderId_leg2', flightv1_1.flight_leg2.carrierSummary.airProviderId)\n",
    "                .withColumn('carrierAirlineImageFileName_leg2', flightv1_1.flight_leg2.carrierSummary.airlineImageFileName)\n",
    "                .withColumn('carrierMixedCabinClass_leg2', flightv1_1.flight_leg2.carrierSummary.mixedCabinClass)\n",
    "                .withColumn('carrierMultiStop_leg2', flightv1_1.flight_leg2.carrierSummary.multiStop)\n",
    "                .withColumn('carrierNextDayArrival_leg2', flightv1_1.flight_leg2.carrierSummary.nextDayArrival)\n",
    "                \n",
    "                .withColumn('timeline_arrivalAirport', get_elements_UDF(flightv1_1.timeline_leg1, 'arrivalAirport'))\n",
    "                \n",
    "                \n",
    "                .select('price', 'version', 'searchDate', 'tableName', 'task_id', 'currencyCode', \n",
    "                        'fromCity', 'toCity', 'trip', 'depDate', 'retDate',\n",
    "                        'stayDays', \n",
    "                       'departureTime', 'arrivalTime', 'departureTime_leg2', 'arrivalTime_leg2',\n",
    "                        'airlineName', 'airlineName_leg2', 'duration_m', 'duration_m_leg2',                \n",
    "                        'flight_code', 'plane', 'stops', 'stops_leg2', 'stop_list', 'stop_list_leg2',\n",
    "                        'noOfTicketsLeft', 'noOfTicketsLeft_leg2',\n",
    "                       'airline_code', 'airline_codes', 'airline_codes_leg2', \n",
    "                        'url', 'fromCityAirportCode', 'toCityAirportCode', 'fromCityAirportCode_leg2', 'toCityAirportCode_leg2',\n",
    "                       'carrierAirProviderId', 'carrierAirlineImageFileName', 'carrierMixedCabinClass', 'carrierMultiStop', 'carrierNextDayArrival',\n",
    "                        'carrierAirProviderId_leg2', 'carrierAirlineImageFileName_leg2', 'carrierMixedCabinClass_leg2', 'carrierMultiStop_leg2', 'carrierNextDayArrival_leg2'\n",
    "                       )                \n",
    "               )\n",
    "\n",
    "display(flightv1_1_2.where(col('trip')=='1').show(1))\n",
    "display(flightv1_1_2.where(col('trip')=='2').show(1))\n",
    "display(flightv1_1_2.printSchema())\n",
    "\n",
    "display(flightv1_1.select(\"flight_leg1.stop_list.airport\").show(100, truncate=False))\n",
    "        \n",
    "#      |         flight_leg1|flight_leg2|| |||   airline_code      ||       timeline_leg1|timeline_leg2|  |                 |||||       ||\n",
    "               \n",
    "\n",
    "# flightv1_1_2.show(3)\n",
    "\n",
    "# temp = flightv1_1.select(\"flight_leg1.stop_list\").show(100, truncate=False)\n",
    "# flightv1_1_2.printSchema()         \n",
    "# flightv1_1_2.crosstab('noOfTicketsLeft', 'noOfTicketsLeft_leg2')\n",
    "# flightv1_1_2.cube('trip', flightv1_1_2.noOfTicketsLeft, flightv1_1_2.noOfTicketsLeft_leg2).count().orderBy('trip', \"noOfTicketsLeft\", \"noOfTicketsLeft_leg2\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'flightv1_1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-048008658229>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# display(flightv1_1.select(\"flight_leg1.carrierSummary\").show(100, truncate=False))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mflightv1_1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'timeline_leg1.getItem(0)'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'flightv1_1' is not defined"
     ]
    }
   ],
   "source": [
    "# display(flightv1_1.select(\"flight_leg1.carrierSummary\").show(100, truncate=False))\n",
    "display(flightv1_1.select('timeline_leg1.getItem(0)').show(100, truncate=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------------+--------------------+-----+\n",
      "|trip|noOfTicketsLeft|noOfTicketsLeft_leg2|count|\n",
      "+----+---------------+--------------------+-----+\n",
      "|2   |6              |8                   |6    |\n",
      "|2   |7              |6                   |31   |\n",
      "|2   |4              |8                   |11   |\n",
      "|2   |2              |5                   |33   |\n",
      "|2   |4              |3                   |116  |\n",
      "|2   |6              |2                   |5    |\n",
      "|1   |3              |null                |241  |\n",
      "|2   |2              |9                   |73   |\n",
      "|2   |3              |0                   |9    |\n",
      "|2   |2              |2                   |665  |\n",
      "+----+---------------+--------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightv1_1_2.groupBy('trip', 'noOfTicketsLeft', 'noOfTicketsLeft_leg2').count().show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+--------------------+-------------+----------------+--------------------+--------+-----------+-------------+--------------+----+-----+--------------------+-----+-------+----------+-----------+---------+----------+---------+----+-----------------+----------------+-------+-----------+------------+----+-------+-----+-----+\n",
      "|airline_code|airline_codes|            arr_time|check_bag_inc|         company|            dep_time|duration|flight_code|flight_number|from_city_name|  id|index|               plane|power|  price|price_code|search_date|span_days|start_date|stay_days|stop|        stop_info|      table_name|task_id|ticket_left|to_city_name|trip|version|video| wifi|\n",
      "+------------+-------------+--------------------+-------------+----------------+--------------------+--------+-----------+-------------+--------------+----+-----+--------------------+-----+-------+----------+-----------+---------+----------+---------+----+-----------------+----------------+-------+-----------+------------+----+-------+-----+-----+\n",
      "|          QF|         [QF]|2017-04-05T19:20:...|        false|  Qantas Airways|2017-04-05T11:00:...|  10h20m|      QF301|          301|        sydney|null|   12|AIRBUS INDUSTRIE ...|false|1178.42|       AUD| 2017-04-04|        0|2017-04-05|        0|   0|                 |flight_1_6_price|    901|          9|    shanghai|   1|    1.0| true|false|\n",
      "|          VN|     [VN, VN]|2017-04-06T14:25:...|        false|Vietnam Airlines|2017-04-05T14:15:...|  26h10m|      VN786|          786|        sydney|null|   69|          Boeing 787|false|1095.22|       AUD| 2017-04-04|        0|2017-04-05|        0|   1|Hanoi(HAN):13h35m|flight_1_6_price|    901|          4|    shanghai|   1|    1.0|false|false|\n",
      "+------------+-------------+--------------------+-------------+----------------+--------------------+--------+-----------+-------------+--------------+----+-----+--------------------+-----+-------+----------+-----------+---------+----------+---------+----+-----------------+----------------+-------+-----------+------------+----+-------+-----+-----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+------------+----------+--------------------+--------------------+--------+-----+----------+--------+------------------+-------+--------------------+--------------------+--------+----+--------------------+-------+\n",
      "|currencyCode|   depDate|         flight_leg1|         flight_leg2|fromCity|price|searchDate|stayDays|         tableName|task_id|       timeline_leg1|       timeline_leg2|  toCity|trip|                 url|version|\n",
      "+------------+----------+--------------------+--------------------+--------+-----+----------+--------+------------------+-------+--------------------+--------------------+--------+----+--------------------+-------+\n",
      "|         AUD|2017-05-18|[[Hangzhou,HGH],2...|[[Bangkok,BKK],20...| Bangkok|401.3|2017-05-11|       7|flight_15_13_price|  16232|[[[Macau, Macau,M...|[[[Macau, Macau,M...|Hangzhou|   2|https://www.exped...|    1.1|\n",
      "|         AUD|2017-05-18|[[Hangzhou,HGH],2...|[[Bangkok,BKK],20...| Bangkok|401.3|2017-05-11|       7|flight_15_13_price|  16232|[[[Macau, Macau,M...|[[[Macau, Macau,M...|Hangzhou|   2|https://www.exped...|    1.1|\n",
      "+------------+----------+--------------------+--------------------+--------+-----+----------+--------+------------------+-------+--------------------+--------------------+--------+----+--------------------+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flight.show(2)\n",
    "flightv1_1.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "emptyDF.write.mode('append').parquet(os.path.join(comb_fldr, \"flight_v1_0.pq\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! rm -rf /home/ubuntu/s3/flight_1_5/extracted/flight_1_5_price_2017-05-10_.pq\n",
    "# -f = to ignore non-existent files, never prompt\n",
    "# -r = to remove directories and their contents recursively\n",
    "# -v = to explain what is being done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spark.createDataFrame(testDF2, struct_v1_1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "\n",
    "df1 = spark.read.json(os.path.join(comb_fldr, \"flight_1_5_price_2017-05-10_2017-06-10*.jsonl\"))\n",
    "df2 = spark.read.json(os.path.join(comb_fldr, \"flight_1_5_price_2017-05-10_2017-11-06_2_*.jsonl\"))\n",
    "df = df1.unionAll(df2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# df.printSchema()\n",
    "# df.select(\"basic.*\").show()\n",
    "df.select(F.explode(\"flight_list\")).show(200, truncate=False)\n",
    "# df.select[\"basic\"]\n",
    "# flat_table = []  # a list\n",
    "# d = json.load(f)\n",
    "# flight_list = json_normalize(d['flight_list'])\n",
    "# basic = json_normalize(d['basic'])\n",
    "\n",
    "# # create dataframe\n",
    "# basic2 = basic.drop('search_date', 1)\n",
    "# basic2['tmp'] = 1\n",
    "# flight_list['tmp'] = 1\n",
    "# DF = pd.merge(basic2, flight_list, on=['tmp'])\n",
    "# DF=DF.drop('tmp', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "basic2 = basic.drop('search_date', 1)\n",
    "basic2['tmp'] = 1\n",
    "flight_list['tmp'] = 1\n",
    "DF = pd.merge(basic2, flight_list, on=['tmp'])\n",
    "DF=DF.drop('tmp', 1)\n",
    "# DF.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = open(\"temp.jsonl\", \"w\")\n",
    "\n",
    "for row in DF.iterrows():\n",
    "    row[1].to_json(f)\n",
    "    f.write(\"\\n\")\n",
    "    \n",
    "f.close()\n",
    "\n",
    "open(\"temp.jsonl\").read()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flight = spark.read.json(\"/home/ubuntu/s3/comb/temp.jsonl\")\n",
    "flight.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'flight' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-efd007dfc180>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mflight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'flight' is not defined"
     ]
    }
   ],
   "source": [
    "flight.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "data = []\n",
    "with open('/home/ubuntu/s3/comb/flight_1_5_price_2017-05-10_2017-11-06_2_7.jsonl') as f:\n",
    "    for line in f:\n",
    "        d = json.load(f)\n",
    "        flight_list = json_normalize(d['flight_list'])\n",
    "        basic = json_normalize(d['flight_list'])\n",
    "        flat_table = dict()\n",
    "        flat_table['basic'] = basic\n",
    "        flat_table['flight'] = flight_list\n",
    "        flat_table_list.append(flat_table)    \n",
    "df = pd.DataFrame.from_records(data)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! cd /home/ubuntu/s3/comb\n",
    "! head -n10 *.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(flat_table_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_records(flat_table_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(jsonl_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having errors. Probably due to 1 Gb RAM being too small to hold the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flight = spark.read.json(jsonl_file_name)\n",
    "flight.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flight.show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flight.write.parquet(pq_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flight.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "extraction_path = \"/home/ubuntu/s3/flight_1_5/extracted\"\n",
    "\n",
    "\n",
    "\n",
    "d = read_from_json_file(json_file_name)\n",
    "\n",
    "jsonl_file_name = json_file_name.replace('.json','.jsonl')\n",
    "\n",
    "with open(jsonl_file_name,'w') as f:\n",
    "    json.dump(d, f)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# def spark_write(spark,jsonl_file_name,parquet_file_name):\n",
    "#     df = spark.read.json(jsonl_file_name)\n",
    "#     df.write.parquet(parquet_file_name)\n",
    "    \n",
    "# def spark_print_json(spark,json_file_name):\n",
    "#     d = read_from_json_file(json_file_name)\n",
    "    \n",
    "#     jsonl_file_name = json_file_name.replace('.json','.jsonl')\n",
    "    \n",
    "#     with open(jsonl_file_name,'w') as f:\n",
    "#         json.dump(d, f)\n",
    "    \n",
    "#     df = spark.read.json(jsonl_file_name)\n",
    "    \n",
    "#     df.createGlobalTempView(\"flight\")\n",
    "    \n",
    "#     spark.sql(\"SELECT fromCity,toCity,flight_leg1.departureTime, flight_leg1.departureLocation.airportLongName FROM global_temp.flight\").show()\n",
    "    \n",
    "def read_from_json_file(filename):\n",
    "    with open(filename,'r') as f:\n",
    "        d = json.load(f)\n",
    "        \n",
    "    return d\n",
    "\n",
    "# def pq_to_json_file(spark, pq_file_name, json_file_name):\n",
    "#     df = spark.read.load(pq_file_name)\n",
    "    \n",
    "#     df.write.json(json_file_name)\n",
    "    \n",
    "# def read_data_from_pq_file(spark,filename):\n",
    "#     df = spark.read.load(filename)\n",
    "    \n",
    "#     df.select('from_city_name','to_city_name').show()\n",
    "#     #spark.sql(\"SELECT from_city_name,to_city_name FROM global_temp.flight\").show()\n",
    "    \n",
    "# def main():\n",
    "#     full_path_pq_file_name = \"test/t.parquet\"\n",
    "\n",
    "#     spark = SparkSession \\\n",
    "#         .builder \\\n",
    "#         .appName(\"Python Spark SQL basic example\") \\\n",
    "#         .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "#         .getOrCreate()\n",
    "            \n",
    "#     parser = argparse.ArgumentParser()\n",
    "    \n",
    "#     parser.add_argument('cmd',help='[json,pq,json2pq,jsonprint]')\n",
    "#     parser.add_argument('--json',help='json file namejson,pq]')\n",
    "#     parser.add_argument('--pq',help='parquet file name')\n",
    "    \n",
    "#     args = parser.parse_args()\n",
    "    \n",
    "#     cmd_ind = args.cmd\n",
    "#     json_file_name = args.json\n",
    "#     pq_file_name = args.pq\n",
    "    \n",
    "#     if cmd_ind == 'json2pq':\n",
    "#         if json_file_name is None:\n",
    "#             print(\"Please set the json file name by --json\")\n",
    "#             quit(0)\n",
    "            \n",
    "#         if pq_file_name is None:\n",
    "#             print(\"Please set the pqrquet file name by --pq\")\n",
    "#             quit(0)\n",
    "#         d = read_from_json_file(json_file_name)\n",
    "        \n",
    "#         jsonl_file_name = json_file_name.replace('.json','.jsonl')\n",
    "        \n",
    "#         with open(jsonl_file_name,'w') as f:\n",
    "#             json.dump(d, f)\n",
    "            \n",
    "#         spark_write(spark, jsonl_file_name, pq_file_name)\n",
    "#     elif cmd_ind == 'pq':\n",
    "#         if pq_file_name is None:\n",
    "#             print(\"Please set the pqrquet file name by --pq\")\n",
    "#             quit(0)\n",
    "#         read_data_from_pq_file(spark,full_path_pq_file_name)\n",
    "#     elif cmd_ind == 'pq2json':\n",
    "#         if json_file_name is None:\n",
    "#             print(\"Please set the json file name by --json\")\n",
    "#             quit(0)\n",
    "            \n",
    "#         if pq_file_name is None:\n",
    "#             print(\"Please set the pqrquet file name by --pq\")\n",
    "#             quit(0)\n",
    "            \n",
    "#         pq_to_json_file(spark,pq_file_name,json_file_name)\n",
    "        \n",
    "#     elif cmd_ind== 'jsonprint':\n",
    "#         if json_file_name is None:\n",
    "#             print(\"Please set the json file name by --json\")\n",
    "#             quit(0)\n",
    "            \n",
    "#         spark_print_json(spark,json_file_name)\n",
    "#     else:\n",
    "#         print('invalid cmd command')\n",
    "        \n",
    "#     spark.stop()\n",
    "    \n",
    "    \n",
    "# if __name__== '__main__':\n",
    "#     main()\n",
    "    \n",
    "    \n",
    "# spark-submit sp.py jsonprint --json final_flight_result_format_v1.1.json\n",
    "\n",
    "\n",
    "# json_file_name = \"D:\\\\Data Science\\\\pySpark\\\\test_schema\\\\final_flight_result_format_v1.1.json\"\n",
    "# jsonl_file_name = \"C:\\\\s3\\\\20170503_jsonl\\\\flight_1_6.jsonl\"\n",
    "jsonl_file_name = \"C:\\\\s3\\\\a3.json\"\n",
    "\n",
    "# d = read_from_json_file(json_file_name)\n",
    "\n",
    "# jsonl_file_name = json_file_name.replace('.json','.jsonl')\n",
    "\n",
    "# with open(jsonl_file_name,'w') as f:\n",
    "#     json.dump(d, f)\n",
    "\n",
    "# flight = spark.read.json(jsonl_file_name)\n",
    "\n",
    "# df.createGlobalTempView(\"flight\")\n",
    "\n",
    "# spark.sql(\"SELECT fromCity,toCity,flight_leg1.departureTime, flight_leg1.departureLocation.airportLongName FROM global_temp.flight\").show()\n",
    "  \n",
    "# flight.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# flight.printSchema()\n",
    "# flight.drop('airline_codes').repartition(1).write.format('com.databricks.spark.csv').save('C:\\\\s3\\\\20170503_jsonl\\\\flight_1_6.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# flight.write.parquet(\"C:\\\\s3\\\\20170503_jsonl\\\\flight.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# flight = spark.read.parquet(\"C:\\\\s3\\\\20170503_jsonl\\\\flight.parquet\")\n",
    "flight = spark.read.parquet(\"/home/ubuntu/parquet/flight.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1862553"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flight.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flight.printSchema()\n",
    "flight.show(2,truncate=False)\n",
    "display(flight.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# path_to_input = \"D:\\Data Science\\pySpark\\original_json\"\n",
    "# original_data = spark.read.json(sc.wholeTextFiles(path_to_input).values())\n",
    "\n",
    "# original_data = sqlContext.read.json(\"D:\\\\Data Science\\\\pySpark\\\\original_json\\\\original_flight_info.txt\")\n",
    "# original_data.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import json\n",
    "# from pprint import pprint\n",
    "\n",
    "# # with open(\"D:\\\\Data Science\\\\pySpark\\\\original_json\\\\original_flight_info.txt\") as data_file:    \n",
    "# #     data = json.load(data_file)\n",
    "\n",
    "# # pprint(data)\n",
    "# # # data.head()\n",
    "\n",
    "# df = spark.read.json(\"D:\\\\Data Science\\\\Flight v2\\\\python\\\\final_flight_result_format_v1.1.json\")\n",
    "# # Displays the content of the DataFrame to stdout\n",
    "# df.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# flight = sc.textFile(\"C:\\\\s3_temp\\\\summary\\\\flat_table.csv\").map(lambda line: (line.split(',')[5], line.split(',')[10])).collect()\n",
    "# flight = sc.textFile(\"C:\\\\s3_temp\\\\summary\\\\flat_table.csv\")\n",
    "# flight = spark.read.csv(\"C:\\\\s3_temp\\\\summary\\\\flat_table.csv\", header=True, mode=\"DROPMALFORMED\")\n",
    "\n",
    "# flight.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# flight.repartition(1).write.format('com.databricks.spark.csv').save('D://Data Science//pySpark//test2.csv', header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flight.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# flight.rdd.takeSample(False, 3, 1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flight.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'udf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-940461c5ce6a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mudfToBool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mudf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoBool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBooleanType\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'udf' is not defined"
     ]
    }
   ],
   "source": [
    "# from pyspark.sql.functions import year, month, dayofmonth, hour, minute, weekofyear, crosstab\n",
    "# flight2.select(year(\"arr_time\").alias('year'), \n",
    "# from pyspark.sql.functions import regexp_extract, col, split\n",
    "# # import re\n",
    "\n",
    "# from pyspark.sql.functions import udf\n",
    "# from pyspark.sql.types import IntegerType, StringType, BooleanType\n",
    "\n",
    "def toBool(aString):\n",
    "    if aString.lower() == 'true':\n",
    "        return True\n",
    "    else: \n",
    "        return False\n",
    " \n",
    "udfToBool = udf(toBool, BooleanType())\n",
    "\n",
    "\n",
    "def groupTicketLeft(aString):\n",
    "    if aString == '0':\n",
    "        return 'plenty'\n",
    "    else: \n",
    "        return aString\n",
    " \n",
    "udfGroupTicketLeft = udf(groupTicketLeft, StringType())\n",
    "\n",
    "# def getMinutes(aString):\n",
    "#     return minute(unix_timestamp(aString, \"HH'h'mm'm'\").cast(\"timestamp\"))\n",
    "\n",
    "# udfGetMinutes = udf(getMinutes, IntegerType())\n",
    "\n",
    "# def getHours(x):\n",
    "#   return re.match('([0-9]+(?=h))', x)\n",
    "# getHours(\"14h\")\n",
    "# len(flight.columns)\n",
    "# flight.columns\n",
    "# flight.dtypes\n",
    "flight2 = (flight.withColumn('search_date', flight.search_date.cast('timestamp'))\n",
    "#           .withColumn('search_date_y', flight.search_date_y.cast('timestamp'))\n",
    "#           .withColumnRenamed('search_date_y', 'search_date')\n",
    "          .withColumn('stay_days', flight.stay_days.cast('int'))                                \n",
    "           #create local time first\n",
    "           .withColumn('dep_time_local', flight.dep_time.substr(1, 23).cast('timestamp'))                      \n",
    "          .withColumn('dep_time', flight.dep_time.cast('timestamp'))\n",
    "           #create local time first\n",
    "          .withColumn('arr_time_local', flight.arr_time.substr(1, 23).cast('timestamp'))                      \n",
    "          .withColumn('arr_time', flight.arr_time.cast('timestamp'))\n",
    "           #duration           \n",
    "           .withColumn('duration_h',split(flight.duration,'h').getItem(0))\n",
    "           .withColumn('duration_m',split(flight.duration,'h').getItem(1))\n",
    "           #stop info\n",
    "           .withColumn('stop_info1',split(flight.stop_info,';').getItem(0))\n",
    "           .withColumn('stop_info2',split(flight.stop_info,';').getItem(1))\n",
    "#            .withColumn('druation_hours', \n",
    "#                        flight.selectExpr(\"duration\", \"regexp_extract(duration,'([0-9]+(?=h))', 1) as duration_hours\")\n",
    "#                        .duration_hours\n",
    "#                        .cast('int'))\n",
    "          .withColumn('start_date', flight.start_date.cast('date'))           \n",
    "          .withColumn('price', flight.price.cast('double'))          \n",
    "           # to correct the name\n",
    "          .withColumnRenamed('check_bag_inc', 'check_bag_not_inc')\n",
    "#           .withColumn('check_bag_not_inc', udfToBool(flight.check_bag_inc))\n",
    "          .withColumn('power', flight.power.cast('boolean'))           \n",
    "          .withColumn('video', flight.video.cast('boolean'))\n",
    "          .withColumn('wifi', flight.wifi.cast('boolean'))           \n",
    "          .withColumn('stop', flight.stop.cast('int')) \n",
    "          .withColumn('span_days', flight.span_days.cast('int'))           \n",
    "           .withColumn('ticket_left', udfGroupTicketLeft(flight.ticket_left))\n",
    "           .drop('search_date_x', 'check_bag_inc')\n",
    "          )\n",
    "\n",
    "flight2 = (flight2.withColumn('stop_loc1',split(flight2.stop_info1,':').getItem(0))\n",
    "           .withColumn('stop_duration1',split(flight2.stop_info1, ':').getItem(1))\n",
    "           .withColumn('stop_loc2',split(flight2.stop_info2,':').getItem(0))\n",
    "           .withColumn('stop_duration2',split(flight2.stop_info2, ':').getItem(1)))\n",
    "\n",
    "flight2 = (flight2.withColumn('stop_duration_h1',split(flight2.stop_duration1,'h').getItem(0))\n",
    "           .withColumn('stop_duration_m1',split(flight2.stop_duration1,'h').getItem(1))\n",
    "           .withColumn('stop_duration_h2',split(flight2.stop_duration2,'h').getItem(0))\n",
    "           .withColumn('stop_duration_m2',split(flight2.stop_duration2,'h').getItem(1)))\n",
    "# .withColumn('arr_time_zone', flight.arr_time.substr(24, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define udf\n",
    "\n",
    "def getMinutes(hString, minString):\n",
    "    if (hString is not None) & (minString is not None): return int(hString) * 60 + int(minString[:-1])\n",
    "    else: return None\n",
    "#     if score >= 80: return 'A'\n",
    "#     elif score >= 60: return 'B'\n",
    "#     elif score >= 35: return 'C'\n",
    "#     else: return 'D'\n",
    " \n",
    "udfGetMinutes = udf(getMinutes, IntegerType())\n",
    "flight2 = (flight2.withColumn(\"duration_minutes\", udfGetMinutes(\"duration_h\", \"duration_m\"))\n",
    "                    .withColumn(\"stop1_minutes\", udfGetMinutes(\"stop_duration_h1\", \"stop_duration_m1\"))\n",
    "                   .withColumn(\"stop2_minutes\", udfGetMinutes(\"stop_duration_h2\", \"stop_duration_m2\")))\n",
    "                 \n",
    "flight2.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# flight2.groupby('search_date', 'from_city_name', 'to_city_name', 'trip').count().show()\n",
    "# flight2.groupby('table_name').count().show()\n",
    "flight2.groupby('stop_info', \n",
    "                'stop_info1', 'stop_loc1', \n",
    "                'stop_duration1', 'stop_duration_h1', 'stop_duration_m1',\n",
    "                'stop_info2', 'stop_loc2', \n",
    "                'stop_duration2',  'stop_duration_h2', 'stop_duration_m2').count().show(500, truncate=False)\n",
    "# flight2.groupby('stop_duration1').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flight2.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# flight2 = flight2.drop('stop_duration_h1', 'stop_duration_m1', 'stop_duration_h2', 'stop_duration_m2',\n",
    "#                       'stop_duration1', 'stop_duration2',\n",
    "#                       'duration_h', 'duration_m', \n",
    "#                        'stop_info', 'stop_info1', 'stop_info2')\n",
    "# display(flight2.show(2))\n",
    "# display(flight2.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flight2.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def groupTime(dt):\n",
    "    h = dt.hour\n",
    "    if h < 5 : return 'early morning'    \n",
    "    elif h < 12 : return 'morning'\n",
    "    elif h < 18 : return 'afternoon'    \n",
    "    else: return 'evening'\n",
    "#     if h >= 18 or h < 7 : return 'night'    \n",
    "#     elif 7 <= h < 12 : return 'morning'\n",
    "#     elif 12 <= h < 18 : return 'afternoon'    \n",
    "#     else: return 'error'\n",
    "    \n",
    "    \n",
    "udfGroupTime = udf(groupTime, StringType())\n",
    "flight2 = flight2.withColumn(\"arr_time_group\", udfGroupTime(\"arr_time_local\")) \\\n",
    "                .withColumn(\"dep_time_group\", udfGroupTime(\"dep_time_local\"))              \n",
    "flight2.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "# flight2.select(\"dep_time_local\", \"dep_time_group\", \"arr_time_local\", \"arr_time_group\").show(10)\n",
    "#flight2.describe(['price']).show()# groupBy(\"dep_time_group\", \"arr_time_group\")\n",
    "# flight2.freqItems(flight2.trip)\n",
    "# flight2.groupby(flight2.trip, flight2.stay_days, flight.dep_time_group).agg(func.mean('price')).show()\n",
    "# flight2.groupby(flight2.trip, flight2.dep_time_group).agg(func.mean('price')).orderBy(['trip', 'avg(price)'], ascending=[1, 0]).show()\n",
    "# flight2.groupby(flight2.trip, flight2.company).agg(F.mean('price'), F.stddev('price'), F.max('price'), F.min('price')).orderBy(['trip', 'stddev_samp(price)'], ascending=[1, 0]).show(10000)\n",
    "flight2.filter(flight2.price <= 0).count() / flight2.count()\n",
    "\n",
    "# drop rows with zero price\n",
    "flight2 = flight2.filter(flight2.price > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# duration_test.select(trim(col(\"duration\"))).show()\n",
    "# df\n",
    "# .withColumn('Created-formatted',when((df.Created.isNull() | (df.Created == '')) ,'0')\n",
    "# .otherwise(unix_timestamp(df.Created,'yyyy-MM-dd')))\n",
    "\n",
    "duration_test = flight2.select(\"stop_duration1\")\n",
    "duration_test.show()\n",
    "\n",
    "duration_test.withColumn('duration_h', when(duration_test.stop_duration1.isNull(), None)\n",
    "                          .otherwise(hour(unix_timestamp(duration_test.stop_duration1,\"HH'h'mm'm'\").cast(\"timestamp\")))).show(20)          \n",
    "    \n",
    "#     .withColumn('duration_m', minute(unix_timestamp(duration_test.duration,\"HH'h'mm'm'\").cast(\"timestamp\")))).show(2)\n",
    "\n",
    "\n",
    "# duration_test.withColumn('duration2',when((duration_test.duration.isNull() | (duration_test.duration == '')) , 0)\\\n",
    "#   .otherwise(unix_timestamp(duration_test.duration,\"HH'h'mm'm'\").cast(\"timestamp\"))).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from datetime import date\n",
    "\n",
    "# import holidays\n",
    "\n",
    "# us_holidays = holidays.UnitedStates()  # or holidays.US()\n",
    "\n",
    "# date(2015, 1, 1) in us_holidays  # True\n",
    "# date(2020, 12, 25) in us_holidays  # False\n",
    "\n",
    "# # print(us_holidays)\n",
    "\n",
    "\n",
    "# for date, name in sorted(holidays.AU(state='NSW', years=2017).items()):\n",
    "#      print(date, name)\n",
    "        \n",
    "# for date, name in sorted(holidays.NZ(state='AUK', years=2017).items()):\n",
    "#      print(date, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # The Holiday class will also recognize strings of any format\n",
    "# # and int/float representing a Unix timestamp\n",
    "# '2014-01-01' in us_holidays  # True\n",
    "# '1/1/2014' in us_holidays    # True\n",
    "# 1388597445 in us_holidays    # True\n",
    "\n",
    "# us_holidays.get('2014-01-01')  # \"New Year's Day\"\n",
    "\n",
    "# # Easily create custom Holiday objects with your own dates instead\n",
    "# # of using the pre-defined countries/states/provinces available\n",
    "# custom_holidays = holidays.HolidayBase()\n",
    "# # Append custom holiday dates by passing:\n",
    "# # 1) a dict with date/name key/value pairs,\n",
    "# custom_holidays.append({\"2015-01-01\": \"New Year's Day\"})\n",
    "# # 2) a list of dates (in any format: date, datetime, string, integer),\n",
    "# custom_holidays.append(['2015-07-01', '07/04/2015'])\n",
    "# # 3) a single date item\n",
    "# custom_holidays.append(date(2015, 12, 25))\n",
    "\n",
    "# date(2015, 1, 1) in custom_holidays  # True\n",
    "# date(2015, 1, 2) in custom_holidays  # False\n",
    "# '12/25/2015' in custom_holidays      # True\n",
    "\n",
    "# # For more complex logic like 4th Monday of January, you can inherit the\n",
    "# # HolidayBase class and define your own _populate(year) method. See below\n",
    "# # documentation for examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from datetime import date\n",
    "# from workalendar.europe import France\n",
    "# cal = France()\n",
    "# cal.holidays(2017)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# flight2.show(2)\n",
    "flight3 = flight2\n",
    "\n",
    "\n",
    "def getWeekday(dt):\n",
    "    return dt.weekday()\n",
    "    \n",
    "def getWeeknumber(dt):\n",
    "    return dt.isocalendar()[1]\n",
    "\n",
    "    \n",
    "udfGetWeekday = udf(getWeekday, StringType())\n",
    "udfGetWeeknumber = udf(getWeeknumber, IntegerType())\n",
    "\n",
    "flight3 = flight3.withColumn(\"dep_weekday\", udfGetWeekday(\"start_date\")) \\\n",
    "                .withColumn(\"dep_weeknum\", udfGetWeeknumber(\"start_date\")) \\\n",
    "                .withColumn(\"lead_time\", datediff(flight3.start_date, flight3.search_date)) \n",
    "#                 .drop('start_date', 'search_date')\n",
    "\n",
    "flight3.show(2)\n",
    "display(flight2.count(), flight3.count())\n",
    "flight3.describe('lead_time').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flight3.show(5)\n",
    "flight3.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get distance to nearest holiday: http://stackoverflow.com/questions/40752378/spark-sql-distance-to-nearest-holiday\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# datetime.datetime('2017-09-01')\n",
    "# date1 = datetime.datetime.strptime(\"2015-01-30\", \"%Y-%m-%d\").strftime(\"%d-%m-%Y\")\n",
    "# print(date1)\n",
    "# start_date_test = flight3.select(\"start_date\").distinct()\n",
    "flight3 = flight3.withColumn(\"start_date_str\", flight3.start_date.cast('string'))\n",
    "\n",
    "holidays = ['2017-01-01', '2017-01-02', '2017-01-27', '2017-01-28',\n",
    "            '2017-01-29', '2017-01-30', '2017-01-31', '2017-02-01', '2017-02-02',\n",
    "            '2017-04-02', '2017-04-03', '2017-04-04',\n",
    "            '2017-05-01', '2017-05-28', '2017-05-29', '2017-05-30',\n",
    "            '2017-10-01', '2017-10-02', '2017-10-03', '2017-10-04', '217-10-05', '2017-10-06']\n",
    " \n",
    "\n",
    "index = spark.sparkContext.broadcast(sorted(holidays))\n",
    "\n",
    "def last_holiday(date):\n",
    "    last_holiday = index.value[0]\n",
    "    for next_holiday in index.value:\n",
    "        if next_holiday >= date:\n",
    "            break\n",
    "        last_holiday = next_holiday\n",
    "    if last_holiday > date:\n",
    "        last_holiday = None\n",
    "    if next_holiday < date:\n",
    "        next_holiday = None        \n",
    "    return last_holiday\n",
    "\n",
    "def next_holiday(date):\n",
    "    last_holiday = index.value[0]\n",
    "    for next_holiday in index.value:\n",
    "        if next_holiday >= date:\n",
    "            break\n",
    "        last_holiday = next_holiday\n",
    "    if last_holiday > date:\n",
    "        last_holiday = None\n",
    "    if next_holiday < date:\n",
    "        next_holiday = None        \n",
    "    return next_holiday\n",
    "\n",
    "\n",
    "# return_type = StructType([StructField('last_holiday', StringType()), StructField('next_holiday', StringType())])\n",
    "\n",
    "last_holiday_udf = udf(last_holiday, StringType())\n",
    "next_holiday_udf = udf(next_holiday, StringType())\n",
    "\n",
    "flight4 = flight3.withColumn('last_holiday', last_holiday_udf('start_date_str'))\n",
    "flight4 = flight4.withColumn('next_holiday', next_holiday_udf('start_date_str'))\n",
    "flight4 = flight4.withColumn('days_to_last_holiday', datediff('last_holiday', 'start_date_str'))\n",
    "flight4 = flight4.withColumn('days_to_next_holiday', datediff('next_holiday', 'start_date_str'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(flight4\n",
    "    .select('days_to_last_holiday', 'days_to_next_holiday', 'last_holiday', 'next_holiday', 'start_date')    \n",
    "    .distinct().show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# flight4.show()\n",
    "#'table_name', 'trip', 'stay_days'\n",
    "# flight5 = (flight4.filter((flight4.trip==1) & (flight4.start_date=='2017-10-01') & (flight4.company=='AirAsiaX'))\n",
    "#          .select('start_date', 'company', 'dep_time_local', 'stop_info', 'duration', 'search_date', 'price')\n",
    "#          .sort(F.desc('start_date'), 'company', 'dep_time_local', 'stop_info', 'duration', 'search_date'))       \n",
    "         \n",
    "flight5 = flight4.sort('table_name', 'trip', 'stay_days', \n",
    "                       'start_date', 'company', 'dep_time_local',\n",
    "                       'stop_info', 'duration', 'search_date')       \n",
    "    \n",
    "byVar = ['table_name', 'trip', 'stay_days', 'start_date', 'company', 'dep_time_local', 'stop_info', 'duration']\n",
    "\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "Threshold = 5\n",
    "\n",
    "w =  Window.partitionBy(byVar).orderBy('search_date').rowsBetween(0, sys.maxsize)\n",
    "flight6 = (flight5.withColumn('future_min_price', F.min(col('price')).over(w))\n",
    "          .withColumn('price_will_drop',                       \n",
    "                      (col('price') - col('future_min_price')) > Threshold ))\n",
    "flight6 = flight6.withColumn('price_will_drop_num', flight6.price_will_drop.cast('int'))\n",
    "flight6.cache()\n",
    "# min_price = (flight5\n",
    "#                   .groupBy(byVar)\n",
    "#                   .agg(F.min(col(\"price\"))).alias(\"min_price\"))\n",
    "\n",
    "# flight6 = flight5.join(min_price, on=byVar, how='left')\n",
    "# # flight6.select('price', 'min(price)').show(10)\n",
    "# # flight6.select('price', 'min(price)').show()\n",
    "# flight6 = flight6.withColumn('price_diff', col('price')-col('min(price)'))\n",
    "# flight6.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flight6.filter((flight6.trip==1) & \\\n",
    "               (flight6.start_date=='2017-10-01') & \\\n",
    "               (flight6.company=='AirAsiaX')) \\\n",
    "    .select('duration', 'search_date', 'price', 'future_min_price', 'price_will_drop') \\\n",
    "    .show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flight2.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flight6.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ['table_name', 'trip', 'stay_days', 'start_date', 'company', 'dep_time_local', 'stop_info', 'duration']\n",
    "# groupby(flight2.trip, flight2.stay_days, flight.dep_time_group).agg(func.mean('price')).show()\n",
    "# flight6.groupBy(col('trip'), col('stay_days'), col('lead_time')).agg(F.mean('price_will_drop_num')).show()\n",
    "flight6.groupBy(col('trip'), col('stay_days'), col('lead_time')). \\\n",
    "    agg(F.mean('price_will_drop_num')).orderBy(['trip', 'stay_days', 'lead_time'], ascending=[1, 1, 0]).show(1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save to parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flight6.coalesce(2).write.parquet(\"C:\\\\s3\\\\20170503_jsonl\\\\flight6.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# flight6 = spark.read.parquet(\"C:\\\\s3\\\\20170503_jsonl\\\\flight6.parquet\")\n",
    "flight6 = spark.read.parquet(\"/home/ubuntu/parquet/flight6.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flight6.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flight6.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flight7 = (flight6.withColumn('price_drop_amt', col('price') - col('future_min_price'))\n",
    "                  .withColumn('stop_minutes', col('stop1_minutes') + col('stop2_minutes'))\n",
    "                  .withColumn('few_tickets_left', col('ticket_left') != '0')\n",
    "                  .select('price_will_drop_num', 'price', 'few_tickets_left', 'from_city_name', 'to_city_name', 'trip', 'stay_days',\n",
    "                          'company', 'flight_code', 'plane', 'span_days', 'stop',\n",
    "                         'power', 'video', 'wifi', 'check_bag_not_inc', \n",
    "                         'dep_time_group', 'arr_time_group',  \n",
    "                         'lead_time', 'dep_weekday', 'dep_weeknum', \n",
    "                         'days_to_last_holiday', 'days_to_next_holiday',                         \n",
    "                         'duration_minutes', 'stop_minutes'\n",
    "                      ).filter(flight6.price > 0))\n",
    "flight7.dtypes\n",
    "flight7 = flight7.na.fill({'stop_minutes': 0, 'days_to_next_holiday': 999})\n",
    "flight7.show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to transform categorical variables? See\n",
    "http://stackoverflow.com/questions/32982425/encode-and-assemble-multiple-features-in-pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# One hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "column_vec_in = ['from_city_name', 'to_city_name', 'trip', 'company', 'flight_code', 'plane', \n",
    "                 'arr_time_group', 'dep_time_group', 'dep_weekday']\n",
    "column_vec_out = ['from_city_name_catVec','to_city_name_catVec', 'trip_catVec',\n",
    "                'company_catVec', 'flight_code_catVec', 'plane_catVec', \n",
    "                  'arr_time_group_catVec', 'dep_time_group_catVec', 'dep_weekday_catVec']\n",
    " \n",
    "indexers = [StringIndexer(inputCol=x, outputCol=x+'_tmp') for x in column_vec_in ]\n",
    " \n",
    "encoders = [OneHotEncoder(dropLast=False, inputCol=x+\"_tmp\", outputCol=y)\n",
    "            for x,y in zip(column_vec_in, column_vec_out)]\n",
    "\n",
    "tmp = [[i,j] for i,j in zip(indexers, encoders)]\n",
    "tmp = [i for sublist in tmp for i in sublist]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer, VectorAssembler, SQLTransformer\n",
    "\n",
    "# prepare labeled sets\n",
    "# 'price_will_drop',\n",
    "\n",
    "cols_now = ['price', 'few_tickets_left', 'stay_days',\n",
    "              'span_days', 'stop',\n",
    "             'power', 'video', 'wifi', 'check_bag_not_inc',                          \n",
    "             'lead_time', 'dep_weeknum', \n",
    "             'days_to_last_holiday', 'days_to_next_holiday',                         \n",
    "             'duration_minutes', 'stop_minutes',\n",
    "            'from_city_name_catVec','to_city_name_catVec', 'trip_catVec',\n",
    "            'company_catVec', 'flight_code_catVec', 'plane_catVec', \n",
    "            'arr_time_group_catVec', 'dep_time_group_catVec', 'dep_weekday_catVec']\n",
    "\n",
    "assembler_features = VectorAssembler(inputCols=cols_now, outputCol='features')\n",
    "# labelIndexer = StringIndexer(inputCol='price', outputCol=\"label\")\n",
    "# tmp += [assembler_features, labelIndexer]\n",
    "tmp += [assembler_features]\n",
    "pipeline = Pipeline(stages=tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# flight3.na.drop().count()\n",
    "\n",
    "# from pyspark.ml import Pipeline\n",
    "# from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# indexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\")\\\n",
    "#             .fit(flight3) for column in list(set(flight3.columns)-\\\n",
    "#                                              set(['stay_days', 'power', 'price', 'span_days',\\\n",
    "#                                                   'stop', 'video', 'wifi', 'duration_minutes',\\\n",
    "#                                                  'dep_weeknum', 'lead_time']))]\n",
    "\n",
    "# pipeline = Pipeline(stages=indexers)\n",
    "# flight4 = pipeline.fit(flight3).transform(flight3)\n",
    "\n",
    "# flight4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cols = ['from_city_name', 'to_city_name', 'trip', 'company', 'flight_code', 'plane', \n",
    "                 'arr_time_group', 'dep_time_group', 'dep_weekday']\n",
    "\n",
    "for c in cols:\n",
    "    flight7.groupBy(col(c)).count().show(50000)\n",
    "    \n",
    "    \n",
    "flight7 = flight7.na.replace('', 'unknown', 'plane')\n",
    "\n",
    "# http://stackoverflow.com/questions/40711229/pyspark-ml-error-urequirement-failed-cannot-have-an-empty-string-for-name\n",
    "# replacements = {\n",
    "#   'plane': 'unknown', 'another_col': 'another_replacement',\n",
    "#   'numeric_column_wont_be_replaced': 1.0\n",
    "# }\n",
    "\n",
    "# for k, v in replacements.items():\n",
    "#     # We can replace string only if target is string\n",
    "#     # In Python 2 str -> basestring\n",
    "#     if isinstance(v, str):\n",
    "#         df = df.na.replace(\"\", v, [k])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cols = ['price_will_drop', 'price', 'few_tickets_left', 'from_city_name', 'to_city_name', 'trip', 'stay_days',\n",
    "                          'company', 'flight_code', 'plane', 'span_days', 'stop',\n",
    "                         'power', 'video', 'wifi', 'check_bag_not_inc', \n",
    "                         'dep_time_group', 'arr_time_group',  \n",
    "                         'lead_time', 'dep_weekday', 'dep_weeknum', \n",
    "                         'days_to_last_holiday', 'days_to_next_holiday',                         \n",
    "                         'duration_minutes', 'stop_minutes']\n",
    "\n",
    "for c in cols:\n",
    "    display(c + \" :\" + str(flight7.where(col(c).isNull()).count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flight7.show(2)\n",
    "flight7.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# flight7 = flight7.na.drop()\n",
    "output = pipeline.fit(flight7).transform(flight7)\n",
    "output = output.withColumnRenamed('price_will_drop_num', 'label')\n",
    "output.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# flight4.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# flight4.select('stay_days',\n",
    "#  'power', \n",
    "#  'span_days',\n",
    "#  'stop',\n",
    "#  'video',\n",
    "#  'wifi',\n",
    "#  'duration_minutes',\n",
    "#  'dep_weeknum',\n",
    "#  'lead_time',\n",
    "#  'trip_index',\n",
    "#  'arr_time_group_index',\n",
    "#  'to_city_name_index',\n",
    "#  'ticket_left_index',\n",
    "#  'company_index',\n",
    "#  'from_city_name_index',\n",
    "#  'airline_code_index',\n",
    "#  'plane_index',\n",
    "#  'check_bag_not_inc_index',\n",
    "#  'dep_weekday_index',\n",
    "#  'dep_time_group_index').dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from pyspark.ml.linalg import Vectors\n",
    "# from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# # from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# # ignore = ['id', 'label', 'binomial_label']\n",
    "# # assembler = VectorAssembler(\n",
    "# #     inputCols=[x for x in df.columns if x not in ignore],\n",
    "# #     outputCol='features')\n",
    "\n",
    "# # assembler.transform(df)\n",
    "# # arr_time_group\n",
    "\n",
    "# assembler = VectorAssembler(    \n",
    "#     inputCols=[\n",
    "#  'stay_days',\n",
    "#  'power', \n",
    "#  'span_days',\n",
    "#  'stop',\n",
    "#  'video',\n",
    "#  'wifi',\n",
    "#  'duration_minutes', \n",
    "#  'dep_weeknum',\n",
    "#  'lead_time',\n",
    "#  'trip_index',\n",
    "#  'arr_time_group_index',\n",
    "#  'to_city_name_index',\n",
    "#  'ticket_left_index',\n",
    "#  'company_index',\n",
    "#  'from_city_name_index',\n",
    "#  'airline_code_index',\n",
    "#  'plane_index',\n",
    "#  'check_bag_not_inc_index',\n",
    "#  'dep_weekday_index',\n",
    "#  'dep_time_group_index'],\n",
    "#     outputCol=\"features\")\n",
    "\n",
    "# output = assembler.transform(flight4)\n",
    "# # print(\"Assembled columns 'hour', 'mobile', 'userFeatures' to vector column 'features'\")\n",
    "# # output.select(\"features\", \"clicked\").show(truncate=False)\n",
    "# output = output.withColumnRenamed('price', 'label')\n",
    "# output.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display(output.count(), flight6.count(), flight7.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# output.coalesce(2).write.parquet(\"C:\\\\s3\\\\20170503_jsonl\\\\output.parquet\")\n",
    "output.coalesce(2).write.parquet(\"/home/ubuntu/parquet/output.parquet/output.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output = spark.read.parquet(\"/home/ubuntu/parquet/output.parquet/output.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output = output.sample(False, 0.1, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output.count()\n",
    "output.printSchema()\n",
    "output.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# interlude to model in h2o (not sparkling water)  \n",
    "When converting the whole dataset to pandas dataframe, there was not enough RAM, JAVA was forced to shutdown!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output.printSchema()\n",
    "output2 = output.select('label', 'features')\n",
    "# output2 = output.select('label', 'features').rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# output2.count()\n",
    "flight_pd = output.select('label', 'price', 'few_tickets_left', \n",
    "                          'from_city_name', 'to_city_name', 'trip', 'stay_days',\n",
    "                          'company', 'flight_code', 'plane', 'span_days', 'stop',\n",
    "                         'power', 'video', 'wifi', 'check_bag_not_inc', \n",
    "                         'dep_time_group', 'arr_time_group',  \n",
    "                         'lead_time', 'dep_weekday', 'dep_weeknum', \n",
    "                         'days_to_last_holiday', 'days_to_next_holiday',                         \n",
    "                         'duration_minutes', 'stop_minutes').toPandas()\n",
    "flight_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# flight_pd.to_csv('C:\\\\s3\\\\20170503_jsonl\\\\flight_pd.csv', sep='\\t')\n",
    "flight_pd.to_csv('/home/ubuntu/parquet/flight_pd.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h2o.init()\n",
    "# from h2o.estimators.gbm import H2OGradientBoostingEstimator\n",
    "\n",
    "# hf = h2o.H2OFrame(flight_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "flight_pd=pd.read_csv(\"/home/ubuntu/parquet/flight_pd.csv\", delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# flight_pd.head()\n",
    "# help(pd.read_csv)\n",
    "flight_hex = h2o.H2OFrame(flight_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, valid, test = flight_hex.split_frame([0.6, 0.2], seed=1234)\n",
    "\n",
    "flight_X = flight_hex.col_names[2:]\n",
    "flight_y = flight_hex.col_names[1]    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from h2o.estimators.gbm import H2OGradientBoostingEstimator\n",
    "from h2o.estimators.random_forest import H2ORandomForestEstimator\n",
    "\n",
    "rf_v1 = H2ORandomForestEstimator(\n",
    "    model_id=\"rf_v1\",\n",
    "    ntrees=200,\n",
    "    stopping_rounds=2,\n",
    "    score_each_iteration=True,\n",
    "    seed=1000000)\n",
    "\n",
    "rf_v1.train(flight_X, flight_y, training_frame=train, validation_frame=valid)\n",
    "rf_v1\n",
    "rf_v1.score_history()\n",
    "model_path = h2o.save_model(model=rf_v1, path=\"C:\\\\s3\\\\20170503_jsonl\", force=True)\n",
    "print(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rf_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# output2.take(2)\n",
    "(trainingData, testData) = output2.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stopped here! 20170513"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Load training data\n",
    "# training = spark.read.format(\"libsvm\").load(\"data/mllib/sample_libsvm_data.txt\")\n",
    "\n",
    "lr = LogisticRegression(maxIter=100, regParam=0.1, elasticNetParam=0.8)\n",
    "\n",
    "# Fit the model\n",
    "t0 = time()\n",
    "lrModel = lr.fit(trainingData)\n",
    "tt = time() - t0\n",
    "\n",
    "# Print the coefficients and intercept for logistic regression\n",
    "print(\"Classifier trained in \" + str(tt) + \" seconds.\")\n",
    "print(\"Coefficients: \" + str(lrModel.coefficients))\n",
    "print(\"Intercept: \" + str(lrModel.intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Extract the summary from the returned LogisticRegressionModel instance trained\n",
    "# in the earlier example\n",
    "trainingSummary = lrModel.summary\n",
    "\n",
    "# Obtain the objective per iteration\n",
    "objectiveHistory = trainingSummary.objectiveHistory\n",
    "print(\"objectiveHistory:\")\n",
    "for objective in objectiveHistory:\n",
    "    print(objective)\n",
    "\n",
    "# Obtain the receiver-operating characteristic as a dataframe and areaUnderROC.\n",
    "trainingSummary.roc.show()\n",
    "print(\"areaUnderROC: \" + str(trainingSummary.areaUnderROC))\n",
    "\n",
    "# Set the model threshold to maximize F-Measure\n",
    "fMeasure = trainingSummary.fMeasureByThreshold\n",
    "maxFMeasure = fMeasure.groupBy().max('F-Measure').select('max(F-Measure)').head()\n",
    "bestThreshold = fMeasure.where(fMeasure['F-Measure'] == maxFMeasure['max(F-Measure)']) \\\n",
    "    .select('threshold').head()['threshold']\n",
    "lr.setThreshold(bestThreshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make predictions.\n",
    "predictions = lrModel.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "predictions.select('label', 'prediction').coalesce(1).write.csv('D://Data Science//pySpark//check_pred7.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "# from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "# from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Load and parse the data file, converting it to a DataFrame.\n",
    "# data = spark.read.format(\"libsvm\").load(\"data/mllib/sample_libsvm_data.txt\")\n",
    "\n",
    "# Automatically identify categorical features, and index them.\n",
    "# Set maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "featureIndexer =\\\n",
    "    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=100).fit(output)\n",
    "    \n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = output.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train a RandomForest model.\n",
    "# rf = RandomForestRegressor(featuresCol=\"indexedFeatures\", numTrees=1000, featureSubsetStrategy=\"auto\",\n",
    "#                                     impurity='variance', maxDepth=4, maxBins=32)\n",
    "gbt = GBTClassifier(labelCol=\"label\", featuresCol=\"indexedFeatures\", maxIter=10)\n",
    "\n",
    "# Chain indexer and forest in a Pipeline\n",
    "pipeline = Pipeline(stages=[featureIndexer, gbt])\n",
    "\n",
    "# Train model.  This also runs the indexer.\n",
    "model = pipeline.fit(trainingData)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "# evaluator = RegressionEvaluator(\n",
    "#     labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\")\n",
    "auc = evaluator.evaluate(predictions)\n",
    "print(\"Area under the curve (AUC) on test data = %g\" % auc)\n",
    "\n",
    "gbtModel = model.stages[1]\n",
    "print(gbtModel)  # summary only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Load training data\n",
    "# training = spark.read.format(\"libsvm\").load(\"data/mllib/sample_libsvm_data.txt\")\n",
    "flight7.withColumnRenamed('price_will_drop_num', 'label')\n",
    "\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "# Fit the model\n",
    "lrModel = lr.fit(training)\n",
    "\n",
    "# Print the coefficients and intercept for logistic regression\n",
    "print(\"Coefficients: \" + str(lrModel.coefficients))\n",
    "print(\"Intercept: \" + str(lrModel.intercept))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "how to score model manually: http://stackoverflow.com/questions/35731140/apply-model-scores-to-spark-dataframe-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions.select('label', 'prediction').coalesce(1).write.csv('D://Data Science//pySpark//check_pred6.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Load and parse the data file, converting it to a DataFrame.\n",
    "data = flight3\n",
    "\n",
    "# Automatically identify categorical features, and index them.\n",
    "# Set maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "# featureIndexer =\\\n",
    "#     VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(data)\n",
    "\n",
    "\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = data.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train a GBT model.\n",
    "gbt = GBTRegressor(featuresCol=\"indexedFeatures\", maxIter=10)\n",
    "\n",
    "# Chain indexer and GBT in a Pipeline\n",
    "pipeline = Pipeline(stages=[featureIndexer, gbt])\n",
    "\n",
    "# Train model.  This also runs the indexer.\n",
    "model = pipeline.fit(trainingData)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
    "\n",
    "gbtModel = model.stages[1]\n",
    "print(gbtModel)  # summary only\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# # Load and parse the data file, converting it to a DataFrame.\n",
    "# data = spark.read.format(\"libsvm\").load(\"data/mllib/sample_libsvm_data.txt\")\n",
    "\n",
    "# # Automatically identify categorical features, and index them.\n",
    "# # Set maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "featureIndexer =\\\n",
    "    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(flight3)\n",
    "\n",
    "# # Split the data into training and test sets (30% held out for testing)\n",
    "# (trainingData, testData) = data.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train a GBT model.\n",
    "gbt = GBTRegressor(featuresCol=\"indexedFeatures\", maxIter=10)\n",
    "\n",
    "# Chain indexer and GBT in a Pipeline\n",
    "pipeline = Pipeline(stages=[featureIndexer, gbt])\n",
    "\n",
    "# Train model.  This also runs the indexer.\n",
    "model = pipeline.fit(trainingData)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
    "\n",
    "gbtModel = model.stages[1]\n",
    "print(gbtModel)  # summary only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# display(flight.select(\"duration\").show())\n",
    "# display(flight2.select(\"duration_h\", 'duration_m').show())\n",
    "display(flight2.show(5))\n",
    "# flight2.toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def getHours(x):\n",
    "  return re.match('([0-9]+(?=h))', x)\n",
    "temp = flight.select(\"duration\").rdd.map(lambda x:getHours(x[0])).toDF()\n",
    "temp.select(\"duration\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flight.select(\"arr_time\").show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# flight2 = (flight.withColumn('arr_time_local', flight.arr_time.substr(1, 23).cast('date'))                 \n",
    "#           )\n",
    "\n",
    "# flight3 = flight2.withColumn('arr_time', flight2.arr_time.cast('timestamp'))\n",
    "\n",
    "# .select('arr_time').show(2, truncate=False)\n",
    "\n",
    "\n",
    "# flight2 = (flight.withColumn('duration_minutes', flight.duration.substr(1, 23).cast('date'))                 \n",
    "#           )\n",
    "\n",
    "# flight.selectExpr(\"duration\", \"regexp_extract(duration,'([0-9]+(?=h))', 1) as duration_hour\", \"regexp_extract(duration,'([0-9]+(?=m))', 1)\", ).show()\n",
    "flight.selectExpr(\"regexp_extract(duration,'([0-9]+(?=h))', 1) as duration_hour\").duration_hour.cast(\"double\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flight2.select('arr_time', 'arr_time_local').dtypes\n",
    "flight2.select('arr_time', 'arr_time_local').show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql import crosstab\n",
    "# from pyspark.sql.functions import *\n",
    "\n",
    "flight.crosstab('start_date','from_city_name').show()\n",
    "# flight.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "3\n",
    "down vote\n",
    "In order to simply cast a string column to a timestamp, the string column must be properly formatted.\n",
    "\n",
    "To retrieve the \"createdAt\" column as a timestamp, you can write the UDF function that would convert the string\n",
    "\n",
    "\"2016-07-01T16:37:41-0400\"\n",
    "to\n",
    "\n",
    "\"2016-07-01 16:37:41\"\n",
    "and convert the \"createdAt\" column to a new format (don't forget to handle the timezone field).\n",
    "\n",
    "Once you have a column containing timestamps as strings like \"2016-07-01 16:37:41\", a simple cast to timestamp would do the job, as you have it in your code.\n",
    "\n",
    "You can read more about Date/Time/String Handling in Spark here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "display(flight.select(\"arr_time\").take(3))\n",
    "display(flight2.select(\"arr_time\").take(3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flight2.withColumn('local', F.from_utc_timestamp(flight2.arr_time, \"AEST\")).select(\"local\").show()\n",
    "flight2.withColumn('local', F.from_utc_timestamp(flight2.arr_time, \"CTZ\")).select(\"local\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import pytz # $ pip install pytz\n",
    "from tzlocal import get_localzone # $ pip install tzlocal\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get local timezone    \n",
    "local_tz = get_localzone() \n",
    "print(local_tz)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test it\n",
    "# utc_now, now = datetime.utcnow(), datetime.now()\n",
    "ts = time.time()\n",
    "utc_now, now = datetime.utcfromtimestamp(ts), datetime.fromtimestamp(ts)\n",
    "print(utc_now)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "local_now = utc_now.replace(tzinfo=pytz.utc).astimezone(local_tz) # utc -> local\n",
    "print(local_now)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert local_now.replace(tzinfo=None) == now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test it\n",
    "# utc_now, now = datetime.utcnow(), datetime.now()\n",
    "ts = time.time()\n",
    "utc_now, now = datetime.utcfromtimestamp(ts), datetime.fromtimestamp(ts)\n",
    "\n",
    "local_now = utc_now.replace(tzinfo=pytz.utc).astimezone(local_tz) # utc -> local\n",
    "assert local_now.replace(tzinfo=None) == now\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Spark-Sql doesn't support date-time, and nor timezones\n",
    "* Using timestamp is the only solution\n",
    "* from_unixtime(at) parses the epoch time correctly, just that the printing of it as a string changes it due to timezone. It is safe to assume that the  from_unixtime will convert it correctly ( although printing it might show different results)\n",
    "* from_utc_timestamp will shift ( not just convert) the timestamp to that timezone, in this case it will subtract 8 hours to the time since (-08:00)\n",
    "* printing sql results messes up the times with respect to timezone param\n",
    "  \t \t\n",
    "* from_unixtime(at) does what from_utc_timestamp does too, it will parse a Unix timestamp integer (seconds since midnight 1970-01-01), and convert the time instant parsed from UTC to the system's default timezone. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flight2.selectExpr(\"from_utc_timestamp(arr_time, 'AEST') as testthis\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flight2.dtypes\n",
    "display(flight2.select('arr_time').take(3))\n",
    "display(flight.select('arr_time').take(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, month, dayofmonth, hour, minute, weekofyear, crosstab\n",
    "\n",
    "flight2.select(year(\"arr_time\").alias('year'), \n",
    "               month(\"arr_time\").alias('month'),\n",
    "               dayofmonth(\"arr_time\").alias('day'),\n",
    "               hour(\"arr_time\").alias('hour'),\n",
    "               minute(\"arr_time\").alias('minute'),\n",
    "               weekofyear('arr_time').alias('week_no'),\n",
    "               \"arr_time_zone\"\n",
    "              ).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_extract, col\n",
    "\n",
    "#flight.select(regexp_extract('arr_time', r'[+-][0-9]{2}:[0-9]{2}\\b', 1)).alias('d').collect\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([('2017-04-09T07:15:00.000+08:00',)], ['str'])\n",
    "df.select(regexp_extract('str', '(\\d+)-(\\d+)', 1).alias('d')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flight2.describe('price').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "# flight2.dtypes\n",
    "# flight2.first()\n",
    "display(flight2.select(max(\"start_date\")).show())\n",
    "display(flight2.select(min(\"start_date\")).show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flight2 = flight2.withColumn('zero_price', flight2.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# flight.describe().show()\n",
    "flight.describe(\"from_city_name\").show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import colflight.filter(flight.price > 0).groupby(flight.search_date_x, flight.from_city_name, flight.to_city_name).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "# flight_to_brisbane = flight.where(col(\"price\") > 0 & col(\"to_city_name\") == \"brisbane\").groupby(flight.search_date_x).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.where(col(\"v\").isin({\"foo\", \"bar\"})).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numeric = sqlContext.createDataFrame([\n",
    "    ('3.5,', '5.0', 'null'), ('2.0', '14.0', 'null'),  ('null', '38.0', 'null'),\n",
    "    ('null', 'null', 'null'),  ('1.0', 'null', '4.0')],\n",
    "    ('low', 'high', 'normal'))\n",
    "\n",
    "numeric_filtered_1 = numeric.where(numeric['LOW'] != 'null')\n",
    "numeric_filtered_1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flight.select('from_city_name', 'price').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "h2o tutorial \n",
    "https://github.com/h2oai/h2o-tutorials/blob/master/tutorials/pysparkling/Chicago_Crime_Demo.md"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/work/uwork/flight\r\n"
     ]
    }
   ],
   "source": [
    "! pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir ../out_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "sc = pyspark.SparkContext('local[*]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SQLContext\n",
    "sqlContext = SQLContext(sparkContext=sc)\n",
    "\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import TimestampType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df01 = (sqlContext.read.parquet(\"../s3/flight.pq.11/flight_10_1\")\n",
    "            .sample(False, .01, seed=0)\n",
    "            .withColumn('searchMonth', F.date_format(F.last_day(F.col('searchDate')), \"YYYYMM\")))\n",
    "df02 = (sqlContext.read.parquet(\"../s3/flight.pq.11/flight_6_30\")\n",
    "            .sample(False, .01, seed=0)\n",
    "            .withColumn('searchMonth', F.date_format(F.last_day(F.col('searchDate')), \"YYYYMM\")))\n",
    "df03 = (sqlContext.read.parquet(\"../s3/flight.pq.11/flight_1_6\")\n",
    "            .sample(False, .01, seed=0)\n",
    "            .withColumn('searchMonth', F.date_format(F.last_day(F.col('searchDate')), \"YYYYMM\")))\n",
    "df04 = (sqlContext.read.parquet(\"../s3/flight.pq.11/flight_1_12\")\n",
    "            .sample(False, .01, seed=0)\n",
    "            .withColumn('searchMonth', F.date_format(F.last_day(F.col('searchDate')), \"YYYYMM\")))\n",
    "df05 = (sqlContext.read.parquet(\"../s3/flight.pq.11/flight_6_1\")\n",
    "            .sample(False, .01, seed=0)\n",
    "            .withColumn('searchMonth', F.date_format(F.last_day(F.col('searchDate')), \"YYYYMM\")))\n",
    "df06 = (sqlContext.read.parquet(\"../s3/flight.pq.11/flight_10_14\")\n",
    "            .sample(False, .01, seed=0)\n",
    "            .withColumn('searchMonth', F.date_format(F.last_day(F.col('searchDate')), \"YYYYMM\")))\n",
    "\n",
    "# pq = df01.withColumn('searchMonth', F.date_format(F.last_day(F.col('searchDate')), \"YYYYMM\"))\n",
    "# pq2 = df02.withColumn('searchMonth', F.date_format(F.last_day(F.col('searchDate')), \"YYYYMM\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 236 ms, sys: 88 ms, total: 324 ms\n",
      "Wall time: 30min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# pq.coalesce(1).write.partitionBy(\"fromCity\", \"toCity\", \"searchMonth\").save(\"out_data/flight.pq.11\")\n",
    "# pq2.coalesce(1).write.partitionBy(\"fromCity\", \"toCity\", \"searchMonth\").mode(\"append\").save(\"out_data/flight.pq.11\")\n",
    "\n",
    "df01.coalesce(1).write.partitionBy(\"fromCity\", \"toCity\").mode(\"overwrite\").save(\"../out_data/flight.pq.11_v2\")\n",
    "df02.coalesce(1).write.partitionBy(\"fromCity\", \"toCity\").mode(\"append\").save(\"../out_data/flight.pq.11_v2\")\n",
    "df03.coalesce(1).write.partitionBy(\"fromCity\", \"toCity\").mode(\"append\").save(\"../out_data/flight.pq.11_v2\")\n",
    "df04.coalesce(1).write.partitionBy(\"fromCity\", \"toCity\").mode(\"append\").save(\"../out_data/flight.pq.11_v2\")\n",
    "df05.coalesce(1).write.partitionBy(\"fromCity\", \"toCity\").mode(\"append\").save(\"../out_data/flight.pq.11_v2\")\n",
    "df06.coalesce(1).write.partitionBy(\"fromCity\", \"toCity\").mode(\"append\").save(\"../out_data/flight.pq.11_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sqlContext.read.parquet(\"../out_data/flight.pq.11_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "224746"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-----+--------+\n",
      "|fromCity|     toCity|count|maxPrice|\n",
      "+--------+-----------+-----+--------+\n",
      "| Chengdu|     Taipei|29989|  3206.1|\n",
      "| Chengdu|     sydney|33502| 14297.7|\n",
      "|  sydney|      Wuhan|35851|12432.62|\n",
      "|  sydney|   shanghai|40458| 10943.5|\n",
      "|shanghai|Los Angeles|40658| 13969.7|\n",
      "|shanghai|     sydney|44288| 9578.82|\n",
      "+--------+-----------+-----+--------+\n",
      "\n",
      "CPU times: user 12 ms, sys: 0 ns, total: 12 ms\n",
      "Wall time: 908 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "(df.groupby('fromCity', 'toCity')\n",
    " .agg(F.count('price').alias('count'),\n",
    "      F.max('price').alias('maxPrice'))\n",
    " .orderBy('count').show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+-----+\n",
      "|leg2_carrierSummary_airlineName|count|\n",
      "+-------------------------------+-----+\n",
      "|                Xiamen Airlines|64028|\n",
      "|                       AirAsiaX|48019|\n",
      "|           China Southern Ai...|21031|\n",
      "|                      Air China|15063|\n",
      "|           China Eastern Air...|12181|\n",
      "|                 Cathay Pacific| 9206|\n",
      "|             Singapore Airlines| 7148|\n",
      "|                               | 6768|\n",
      "|               Sichuan Airlines| 5788|\n",
      "|                         United| 5559|\n",
      "|                 Qantas Airways| 4303|\n",
      "|              American Airlines| 4042|\n",
      "|                     Air Canada| 3050|\n",
      "|                Hainan Airlines| 2836|\n",
      "|                     Korean Air| 2554|\n",
      "|                Asiana Airlines| 2438|\n",
      "|                          Delta| 2042|\n",
      "|              Air Macau Company| 1329|\n",
      "|              Malaysia Airlines| 1203|\n",
      "|             All Nippon Airways| 1046|\n",
      "|            Philippine Airlines|  791|\n",
      "|               Virgin Australia|  789|\n",
      "|                 Japan Airlines|  704|\n",
      "|           Thai Airways Inte...|  694|\n",
      "|               Vietnam Airlines|  688|\n",
      "|                 China Airlines|  469|\n",
      "|           Aeroflot-Russian ...|  455|\n",
      "|               Garuda Indonesia|  172|\n",
      "|                Air New Zealand|  144|\n",
      "|             Hong Kong Airlines|   61|\n",
      "|                  Cathay Dragon|   45|\n",
      "|              Shanghai Airlines|   39|\n",
      "|              Shenzhen Airlines|   32|\n",
      "|               Juneyao Airlines|   16|\n",
      "|                    EVA Airways|   10|\n",
      "|               Turkish Airlines|    3|\n",
      "+-------------------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(df.groupBy('leg2_carrierSummary_airlineName')\n",
    ".agg(F.count('leg2_carrierSummary_airlineName')\n",
    ".alias('count')).orderBy(F.desc('count')).show(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------+-------------------+------------------+\n",
      "|weekofyear(depDate)|month(depDate)|dayofmonth(depDate)|dayofyear(depDate)|\n",
      "+-------------------+--------------+-------------------+------------------+\n",
      "|                 26|             6|                 29|               180|\n",
      "|                 48|            11|                 30|               334|\n",
      "|                 25|             6|                 20|               171|\n",
      "|                 46|            11|                 18|               322|\n",
      "|                  2|             1|                  8|                 8|\n",
      "|                 46|            11|                 14|               318|\n",
      "|                 52|            12|                 30|               364|\n",
      "|                 42|            10|                 20|               293|\n",
      "|                 51|            12|                 20|               354|\n",
      "|                 52|            12|                 28|               362|\n",
      "|                 30|             7|                 25|               206|\n",
      "|                 39|             9|                 29|               272|\n",
      "|                 41|            10|                 10|               283|\n",
      "|                 46|            11|                 17|               321|\n",
      "|                 33|             8|                 20|               232|\n",
      "|                 28|             7|                 10|               191|\n",
      "|                 36|             9|                  8|               251|\n",
      "|                 23|             6|                  9|               160|\n",
      "|                 35|             8|                 30|               242|\n",
      "|                 34|             8|                 26|               238|\n",
      "+-------------------+--------------+-------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(F.weekofyear(F.col('depDate')), F.month(F.col('depDate')),\n",
    "         F.dayofmonth(F.col('depDate')), F.dayofyear(F.col('depDate'))).distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+-----+\n",
      "|leg1_arrivalTime_hour|count|\n",
      "+---------------------+-----+\n",
      "|                 null|    0|\n",
      "+---------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(df.groupBy('leg1_arrivalTime_hour')\n",
    ".agg(F.count('leg1_arrivalTime_hour')\n",
    ".alias('count')).orderBy(F.desc('count')).show(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------+-----------------------------------+-----+\n",
      "|leg2_carrierSummary_displayUrgencyMessage|leg2_carrierSummary_noOfTicketsLeft|count|\n",
      "+-----------------------------------------+-----------------------------------+-----+\n",
      "|                                    false|                                  0|24756|\n",
      "|                                    false|                                  5|12532|\n",
      "|                                    false|                                  6|  858|\n",
      "|                                    false|                                  7|  960|\n",
      "|                                    false|                                  8|  986|\n",
      "|                                    false|                                  9|18554|\n",
      "|                                     true|                                  1| 2068|\n",
      "|                                     true|                                  2| 1456|\n",
      "|                                     true|                                  3| 1311|\n",
      "|                                     true|                                  4| 3897|\n",
      "|                                     true|                                  5| 1696|\n",
      "+-----------------------------------------+-----------------------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(df.groupBy('leg2_carrierSummary_displayUrgencyMessage','leg2_carrierSummary_noOfTicketsLeft')\n",
    ".agg(F.count('leg2_carrierSummary_noOfTicketsLeft')\n",
    ".alias('count')).orderBy('leg2_carrierSummary_displayUrgencyMessage','leg2_carrierSummary_noOfTicketsLeft').show(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookforwardDays = 19\n",
    "ndays = lambda i: i * 86400 # number of seconds in a day\n",
    "byVar = ['toCity', 'depDate', 'stayDays', 'timeline1_departureTime_time', 'timeline2_departureTime_time',\n",
    "         'timeline1_carrier_airlineCode', 'timeline2_carrier_airlineCode']\n",
    "\n",
    "w = (Window.partitionBy('routeCombKey')\n",
    "     .orderBy(F.col('searchDate').cast('timestamp').cast('long'))\n",
    "     .rowsBetween(0, ndays(lookforwardDays)))\n",
    "\n",
    "threshold = 20\n",
    "\n",
    "udfGetWeekNumer = lambda dt: F.udf(dt.isocalendar()[1], IntegerType())\n",
    "# udfGetAvg = lambda x: F.udf(sum(x)/float(len(x)))\\n\\n\n",
    "\n",
    "df2 = (df.filter(df.price > 0)\n",
    "       .withColumn('routeCombKey', F.concat_ws('-', F.col('fromCity'),\n",
    "                                   F.col('toCity'),\n",
    "                                   F.col('stayDays'), F.col('depDate'),\n",
    "                                   F.col('timeline1_departureTime_time'),\n",
    "                                   F.col('timeline2_departureTime_time'),\n",
    "                                   F.col('timeline1_carrier_airlineCode'),\n",
    "                                   F.col('timeline2_carrier_airlineCode')))\n",
    "                                   # join_udf(F.col('timeline1_departureTime_time')),\n",
    "                                   # join_udf(F.col('timeline2_departureTime_time')),\n",
    "                                   # join_udf(F.col('timeline1_carrier_airlineCode')),\n",
    "                                   # join_udf(F.col('timeline2_carrier_airlineCode'))))\n",
    "       .withColumn('futureMinPrice', F.min(F.col('price')).over(w))\n",
    "       .withColumn('priceWillDrop', (F.col('price') - F.col('futureMinPrice') > threshold).cast('int'))\n",
    "       .withColumn('saving0', F.col('price') - F.col('futureMinPrice'))\n",
    "       .withColumn('temp0', F.lit(0))\n",
    "       .withColumn('saving', F.greatest('saving0', 'temp0'))\n",
    "       .drop('saving0', 'temp0')\n",
    "       .withColumn('leadTime', F.datediff(F.col('depDate'), F.col('searchDate')))\n",
    "       .withColumn('depWeekOfYear', F.weekofyear(F.col('depDate')))\n",
    "       .withColumn('retWeekOfYear', F.weekofyear(F.col('leg2_departureTime_date')))\n",
    "       .withColumn('depMonth', F.month(F.col('depDate')))\n",
    "       .withColumn('depDayOfMonth', F.dayofmonth(F.col('depDate')))\n",
    "       .withColumn('depDayOfYear', F.dayofyear(F.col('depDate')))\n",
    "       .withColumn('depDayOfWeek', F.date_format(F.col('depDate'), 'EEEE'))\n",
    "       .withColumn('retDayOfWeek', F.date_format(F.col('leg2_departureTime_date'), 'EEEE'))\n",
    "       .withColumn('searchDayOfWeek', F.date_format(F.col('searchDate'), 'EEEE'))\n",
    "       .withColumn('leg1_noOfTicketsLeft', \n",
    "                   F.when(F.col('leg1_carrierSummary_noOfTicketsLeft') > 0, \n",
    "                          F.col('leg1_carrierSummary_noOfTicketsLeft')).otherwise(99))\n",
    "        .withColumn('leg2_noOfTicketsLeft', \n",
    "                    F.when(F.col('leg2_carrierSummary_noOfTicketsLeft') > 0, \n",
    "                           F.col('leg2_carrierSummary_noOfTicketsLeft')).otherwise(99))\n",
    "       .withColumn('leg1_cabinClass_0', F.col('timeline1_carrier_cabinClass').getItem(0))\n",
    "       .withColumn('leg1_cabinClass_1', F.col('timeline1_carrier_cabinClass').getItem(1))\n",
    "       .withColumn('leg1_cabinClass_2', F.col('timeline1_carrier_cabinClass').getItem(2))\n",
    "       .withColumn('leg2_cabinClass_0', F.col('timeline2_carrier_cabinClass').getItem(0))\n",
    "       .withColumn('leg2_cabinClass_1', F.col('timeline2_carrier_cabinClass').getItem(1))\n",
    "       .withColumn('leg2_cabinClass_2', F.col('timeline2_carrier_cabinClass').getItem(2))\n",
    "       .select('price', 'priceWillDrop', 'futureMinPrice', 'saving',\n",
    "               'fromCity', 'toCity',\n",
    "               'searchDate',\n",
    "               'routeCombKey',\n",
    "               'leadTime', \n",
    "               'leg1_stops', 'leg2_stops',\n",
    "               'leg1_noOfTicketsLeft', 'leg2_noOfTicketsLeft',\n",
    "               'leg1_carrierSummary_airlineName', 'leg2_carrierSummary_airlineName',\n",
    "               'leg1_departureTime_hour', 'leg2_departureTime_hour',\n",
    "               'depWeekOfYear', 'depDayOfWeek','retWeekOfYear', 'retDayOfWeek',\n",
    "               'searchDayOfWeek',\n",
    "               'leg1_cabinClass_0', 'leg1_cabinClass_1', 'leg1_cabinClass_2',\n",
    "               'leg2_cabinClass_0', 'leg2_cabinClass_1', 'leg2_cabinClass_2',\n",
    "               'trip')\n",
    "      )\n",
    "\n",
    "# df2.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>priceWillDrop</th>\n",
       "      <th>futureMinPrice</th>\n",
       "      <th>saving</th>\n",
       "      <th>fromCity</th>\n",
       "      <th>toCity</th>\n",
       "      <th>searchDate</th>\n",
       "      <th>routeCombKey</th>\n",
       "      <th>leadTime</th>\n",
       "      <th>leg1_stops</th>\n",
       "      <th>...</th>\n",
       "      <th>retWeekOfYear</th>\n",
       "      <th>retDayOfWeek</th>\n",
       "      <th>searchDayOfWeek</th>\n",
       "      <th>leg1_cabinClass_0</th>\n",
       "      <th>leg1_cabinClass_1</th>\n",
       "      <th>leg1_cabinClass_2</th>\n",
       "      <th>leg2_cabinClass_0</th>\n",
       "      <th>leg2_cabinClass_1</th>\n",
       "      <th>leg2_cabinClass_2</th>\n",
       "      <th>trip</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>478.2</td>\n",
       "      <td>0</td>\n",
       "      <td>478.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Chengdu</td>\n",
       "      <td>Taipei</td>\n",
       "      <td>2017-06-03</td>\n",
       "      <td>Chengdu-Taipei-0-2017-06-12-10:55am-6:05pm-1:2...</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1332.3</td>\n",
       "      <td>0</td>\n",
       "      <td>1332.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Chengdu</td>\n",
       "      <td>Taipei</td>\n",
       "      <td>2017-06-07</td>\n",
       "      <td>Chengdu-Taipei-0-2017-07-04-4:05pm-8:20pm-1:25...</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>885.4</td>\n",
       "      <td>0</td>\n",
       "      <td>885.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Chengdu</td>\n",
       "      <td>Taipei</td>\n",
       "      <td>2017-05-25</td>\n",
       "      <td>Chengdu-Taipei-0-2017-07-04-8:20am-1:40pm-1:25...</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>882.7</td>\n",
       "      <td>0</td>\n",
       "      <td>882.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Chengdu</td>\n",
       "      <td>Taipei</td>\n",
       "      <td>2017-05-16</td>\n",
       "      <td>Chengdu-Taipei-0-2017-07-12-3:10pm-10:35pm-1:2...</td>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>473.1</td>\n",
       "      <td>0</td>\n",
       "      <td>473.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Chengdu</td>\n",
       "      <td>Taipei</td>\n",
       "      <td>2017-06-06</td>\n",
       "      <td>Chengdu-Taipei-0-2017-07-29-8:35pm-8:15am-1:25...</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    price  priceWillDrop  futureMinPrice  saving fromCity  toCity  searchDate  \\\n",
       "0   478.2              0           478.2     0.0  Chengdu  Taipei  2017-06-03   \n",
       "1  1332.3              0          1332.3     0.0  Chengdu  Taipei  2017-06-07   \n",
       "2   885.4              0           885.4     0.0  Chengdu  Taipei  2017-05-25   \n",
       "3   882.7              0           882.7     0.0  Chengdu  Taipei  2017-05-16   \n",
       "4   473.1              0           473.1     0.0  Chengdu  Taipei  2017-06-06   \n",
       "\n",
       "                                        routeCombKey  leadTime  leg1_stops  \\\n",
       "0  Chengdu-Taipei-0-2017-06-12-10:55am-6:05pm-1:2...         9           1   \n",
       "1  Chengdu-Taipei-0-2017-07-04-4:05pm-8:20pm-1:25...        27           1   \n",
       "2  Chengdu-Taipei-0-2017-07-04-8:20am-1:40pm-1:25...        40           1   \n",
       "3  Chengdu-Taipei-0-2017-07-12-3:10pm-10:35pm-1:2...        57           1   \n",
       "4  Chengdu-Taipei-0-2017-07-29-8:35pm-8:15am-1:25...        53           1   \n",
       "\n",
       "  ...   retWeekOfYear  retDayOfWeek  searchDayOfWeek leg1_cabinClass_0  \\\n",
       "0 ...            None          None         Saturday                 3   \n",
       "1 ...            None          None        Wednesday                 3   \n",
       "2 ...            None          None         Thursday                 3   \n",
       "3 ...            None          None          Tuesday                 3   \n",
       "4 ...            None          None          Tuesday                 3   \n",
       "\n",
       "  leg1_cabinClass_1  leg1_cabinClass_2  leg2_cabinClass_0  leg2_cabinClass_1  \\\n",
       "0                 3               None                  3                  3   \n",
       "1                 2               None                  3                  3   \n",
       "2                 3               None                  3                  3   \n",
       "3                 3               None                  3                  3   \n",
       "4                 3               None                  3                  3   \n",
       "\n",
       "  leg2_cabinClass_2 trip  \n",
       "0              None    1  \n",
       "1              None    1  \n",
       "2              None    1  \n",
       "3              None    1  \n",
       "4              None    1  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|randVar|count|\n",
      "+-------+-----+\n",
      "|    8.0|18223|\n",
      "|    0.0| 9011|\n",
      "|    7.0|18258|\n",
      "|    1.0|18413|\n",
      "|    4.0|18386|\n",
      "|    3.0|18143|\n",
      "|    2.0|18068|\n",
      "|   10.0| 9152|\n",
      "|    6.0|18153|\n",
      "|    5.0|18374|\n",
      "|    9.0|18322|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3 = df2.filter(F.col('searchDate') <= F.lit('2017-06-23').cast(TimestampType()))\n",
    "df3 = df3.withColumn('randVar', F.round(F.rand()*10, 0))\n",
    "# df3.filter(F.col('searchDate') >= F.lit('2017-06-16').cast(TimestampType())).toPandas().to_pickle(\"D:\\flight.pq.11\\df3_test.pkl\")\n",
    "df3.groupby('randVar').agg(F.count('randVar').alias('count')).show(100)\n",
    "\n",
    "listRand = [x['randVar'] for x in df3.select('randVar').distinct().collect()]\n",
    "\n",
    "dfArray = [df3.where(F.col('randVar') == x) for x in listRand]\n",
    "\n",
    "# spark.conf.set(\"spark.driver.maxResultSize\", \"10g\")\n",
    "\n",
    "i = 0\n",
    "\n",
    "for df_temp in dfArray:\n",
    "    df_temp.toPandas().to_pickle('../out_data/df3_test_' + str(listRand[i]) + '.pkl')\n",
    "    i += 1\n",
    "\n",
    "# (df2.filter(F.col('searchDate') <= F.lit('2017-06-23').cast(TimestampType()))\n",
    "#     .toPandas().to_pickle(\"../df2_v2.pkl\"))\n",
    "#     .limit(4000000)\n",
    "\n",
    "# (df2.filter(F.col('searchDate') <= F.lit('2017-06-23').cast(TimestampType()))\n",
    "#     .toPandas().to_pickle(\"../df2_v2.pkl\"))\n",
    "#     .limit(4000000)\n",
    "\n",
    "# get_ipython().run_cell_magic(u'time', u'', u'# pyspark --driver-memory 5g --executor-memory 25g\\n\\nimport pyspark.sql.functions as F\\n\\n# set default parquet partition to be 128mb\\n# spark._jsc.hadoopConfiguration().set(\"dfs.block.size\", \"64m\")\\n# spark._jsc.hadoopConfiguration().setInt(\"parquet.block.size\", 128*1024*1024)\\n\\n# Read in original parquet files\\nparquetFile = spark.read.parquet(\"C:\\\\Data\\Flight Data\\\\flight.pq.11\\\\flight_10_1\")\\nparquetFile2 = spark.read.parquet(\"C:\\\\Data\\\\Flight Data\\\\flight.pq.11\\\\flight_10_14\")\\n\\n# Add searchMonth\\nparquetFile = parquetFile.withColumn(\\'searchMonth\\', F.date_format(F.last_day(F.col(\\'searchDate\\')), \"YYYYMM\"))\\nparquetFile2 = parquetFile2.withColumn(\\'searchMonth\\', F.date_format(F.last_day(F.col(\\'searchDate\\')), \"YYYYMM\"))\\n\\n# Save first parquet file, partition by fromCity, toCity and searchMonth\\nparquetFile.coalesce(1).write.partitionBy(\"fromCity\", \"toCity\", \"searchMonth\").save(\"C:\\\\Data\\\\Flight Data\\\\flight.pq.11\\\\flight.pq\")\\n\\n# Save additional parquet files, partition by fromCity, toCity and searchMonth\\nparquetFile2.coalesce(1).write.partitionBy(\"fromCity\", \"toCity\", \"searchMonth\").mode(\"append\").save(\"C:\\Data\\\\Flight Data\\\\flight.pq.11\\\\flight.pq\")\\n\\n# Next: convert this to a jar file, save on s3, launch ec2 spot instance and run the jar file')\n",
    "# df2_in = pd.read_pickle(\"D:\\flight.pq.11\\df2.pkl\")\n",
    "# df2_in.searchDate.value_counts()\n",
    "# parquetFile.count()\n",
    "# parquetFile.show(1)\n",
    "# parquetFile.select('searchDate').distinct().orderBy('searchDate').show(10000, truncate=False)\n",
    "# parquetFile.select('price').distinct().orderBy('price').show(10000, truncate=False)\n",
    "# parquetFile.filter(parquetFile.)\n",
    "# parquetFile.select('depDate').distinct().orderBy('depDate').show(100, truncate=False)\n",
    "# (parquetFile.searchDate == '2017-06-12')\n",
    "# parquetFile.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coding: utf-8\n",
    "In[1]:\n",
    "\n",
    "spark\n",
    "In[7]:\n",
    "\n",
    "import pandas as pd\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import TimestampType\n",
    "from pyspark.sql.window import Window\n",
    "import sys\n",
    "pd.set_option('display.max_columns', None)\n",
    "In[3]:\n",
    "df = spark.read.parquet(\"C:\\Data\\Flight Data\\flight.pq.11\\data\\flight_v2.pq\")\n",
    "df.count()\n",
    "In[4]:\n",
    "\n",
    "get_ipython().run_cell_magic(u'time', u'', u\"df.groupby('fromCity', 'toCity').agg(F.count('price').alias('count'), F.max('price').alias('maxPrice')).orderBy('count').show()\")\n",
    "In[5]:\n",
    "\n",
    "get_ipython().run_cell_magic(u'time', u'', u\"df.groupby('searchDate').agg(F.avg('price')).orderBy('searchDate').show(100)\")\n",
    "In[6]:\n",
    "\n",
    "df.printSchema()\n",
    "In[7]:\n",
    "\n",
    "df.filter(F.col('searchMonth')=='201707').agg(F.avg('price')).show()\n",
    "In[11]:\n",
    "\n",
    "get_ipython().run_cell_magic(u'time', u'', u'df.dropDuplicates().count()')\n",
    "In[53]:\n",
    "\n",
    "get_ipython().run_cell_magic(u'time', u'', u'dfv1.dropDuplicates().count()')\n",
    "In[54]:\n",
    "\n",
    "get_ipython().run_cell_magic(u'time', u'', u'dfv2.dropDuplicates().count()')\n",
    "In[102]:\n",
    "\n",
    "df = spark.read.parquet(\"C:\\Data\\Flight Data\\flight.pq.11\\flight_10_1_14.pq\")\n",
    "df.count()\n",
    "In[6]:\n",
    "\n",
    "df.printSchema()\n",
    "In[38]:\n",
    "(df.groupBy('leg2_carrierSummary_airlineName')\n",
    ".agg(F.count('leg2_carrierSummary_airlineName')\n",
    ".alias('count')).orderBy(F.desc('count')).show(1000))\n",
    "df.select(F.weekofyear(F.col('depDate')), F.month(F.col('depDate')),\n",
    "F.dayofmonth(F.col('depDate')), F.dayofyear(F.col('depDate'))).distinct().show()\n",
    "(df.groupBy('leg1_arrivalTime_hour')\n",
    ".agg(F.count('leg1_arrivalTime_hour')\n",
    ".alias('count')).orderBy(F.desc('count')).show(1000))\n",
    "(df.groupBy('leg2_carrierSummary_displayUrgencyMessage','leg2_carrierSummary_noOfTicketsLeft')\n",
    ".agg(F.count('leg2_carrierSummary_noOfTicketsLeft')\n",
    ".alias('count')).orderBy('leg2_carrierSummary_displayUrgencyMessage','leg2_carrierSummary_noOfTicketsLeft').show(1000))\n",
    "\n",
    "(df.groupBy('timeline1_carrier_bookingCode','timeline1_carrier_cabinClass')\n",
    ".agg(F.count('leg2_carrierSummary_noOfTicketsLeft').alias('count'),\n",
    "F.avg('price')).orderBy('timeline1_carrier_bookingCode','timeline1_carrier_cabinClass').show(1000))\n",
    "timeline1_carrier_bookingCode\n",
    "timeline1_carrier_cabinClass\n",
    "In[4]:\n",
    "\n",
    "df.printSchema()\n",
    "In[20]:\n",
    "\n",
    "df.limit(2).toPandas().to_csv(\"D:\\flight.pq.11\\df.csv\", index=False)\n",
    "In[8]:\n",
    "\n",
    "get_ipython().run_cell_magic(u'time', u'', u\"\\nlookforwardDays = 19\\ndays = lambda i: i * 86400 # number of seconds in a day\\n\\nbyVar = ['toCity', 'depDate', 'stayDays', \\n 'timeline1_departureTime_time', 'timeline2_departureTime_time',\\n 'timeline1_carrier_airlineCode', 'timeline2_carrier_airlineCode']\\nw = (Window.partitionBy('routeCombKey')\\n .orderBy(F.col('searchDate').cast('timestamp').cast('long'))\\n .rowsBetween(0, days(lookforwardDays)))\\nthreshold = 20\\n\\nudfGetWeekNumer = lambda dt: F.udf(dt.isocalendar()[1], IntegerType())\\n\\n# udfGetAvg = lambda x: F.udf(sum(x)/float(len(x)))\\n\\ndf2 = (df.filter(df.price > 0)\\n .withColumn('routeCombKey', F.concat_ws('-', F.col('fromCity'), \\n F.col('toCity'),\\n F.col('stayDays'), F.col('depDate'), \\n F.col('timeline1_departureTime_time'),\\n F.col('timeline2_departureTime_time'),\\n F.col('timeline1_carrier_airlineCode'),\\n F.col('timeline2_carrier_airlineCode')))\\n \\n# join_udf(F.col('timeline1_departureTime_time')),\\n# join_udf(F.col('timeline2_departureTime_time')),\\n# join_udf(F.col('timeline1_carrier_airlineCode')),\\n# join_udf(F.col('timeline2_carrier_airlineCode'))))\\n .withColumn('futureMinPrice', F.min(F.col('price')).over(w))\\n .withColumn('priceWillDrop', (F.col('price') - F.col('futureMinPrice') > threshold).cast('int'))\\n .withColumn('saving0', F.col('price') - F.col('futureMinPrice'))\\n .withColumn('temp0', F.lit(0))\\n .withColumn('saving', F.greatest('saving0', 'temp0'))\\n .drop('saving0', 'temp0')\\n .withColumn('leadTime', F.datediff(F.col('depDate'), F.col('searchDate'))) \\n .withColumn('depWeekOfYear', F.weekofyear(F.col('depDate')))\\n .withColumn('retWeekOfYear', F.weekofyear(F.col('leg2_departureTime_date')))\\n .withColumn('depMonth', F.month(F.col('depDate')))\\n .withColumn('depDayOfMonth', F.dayofmonth(F.col('depDate')))\\n .withColumn('depDayOfYear', F.dayofyear(F.col('depDate')))\\n .withColumn('depDayOfWeek', F.date_format(F.col('depDate'), 'EEEE'))\\n .withColumn('retDayOfWeek', F.date_format(F.col('leg2_departureTime_date'), 'EEEE'))\\n .withColumn('searchDayOfWeek', F.date_format(F.col('searchDate'), 'EEEE'))\\n .withColumn('leg1_noOfTicketsLeft', \\n F.when(F.col('leg1_carrierSummary_noOfTicketsLeft') > 0, \\n F.col('leg1_carrierSummary_noOfTicketsLeft')).otherwise(99)) \\n .withColumn('leg2_noOfTicketsLeft', \\n F.when(F.col('leg2_carrierSummary_noOfTicketsLeft') > 0, \\n F.col('leg2_carrierSummary_noOfTicketsLeft')).otherwise(99))\\n .withColumn('leg1_cabinClass_0', F.col('timeline1_carrier_cabinClass').getItem(0))\\n .withColumn('leg1_cabinClass_1', F.col('timeline1_carrier_cabinClass').getItem(1))\\n .withColumn('leg1_cabinClass_2', F.col('timeline1_carrier_cabinClass').getItem(2))\\n .withColumn('leg2_cabinClass_0', F.col('timeline2_carrier_cabinClass').getItem(0))\\n .withColumn('leg2_cabinClass_1', F.col('timeline2_carrier_cabinClass').getItem(1))\\n .withColumn('leg2_cabinClass_2', F.col('timeline2_carrier_cabinClass').getItem(2)) \\n .select('price', 'priceWillDrop', 'futureMinPrice', 'saving',\\n 'fromCity', 'toCity',\\n 'searchDate',\\n 'routeCombKey',\\n 'leadTime', \\n 'leg1_stops', 'leg2_stops',\\n 'leg1_noOfTicketsLeft', 'leg2_noOfTicketsLeft',\\n 'leg1_carrierSummary_airlineName', 'leg2_carrierSummary_airlineName',\\n 'leg1_departureTime_hour', 'leg2_departureTime_hour',\\n 'depWeekOfYear', 'depDayOfWeek','retWeekOfYear', 'retDayOfWeek',\\n 'searchDayOfWeek',\\n 'leg1_cabinClass_0', 'leg1_cabinClass_1', 'leg1_cabinClass_2',\\n 'leg2_cabinClass_0', 'leg2_cabinClass_1', 'leg2_cabinClass_2',\\n 'trip')\\n )\\n\\n, \\n# df2.cache()\")\n",
    "In[9]:\n",
    "\n",
    "get_ipython().run_cell_magic(u'time', u'', u'df2.limit(5).toPandas()')\n",
    "In[10]:\n",
    "\n",
    "df2.cache()\n",
    "In[6]:\n",
    "\n",
    "df3 = df2.filter(F.col('searchDate') <= F.lit('2017-06-23').cast(TimestampType()))\n",
    "df3 = df3.withColumn('randVar', F.round(F.rand()*10, 0))\n",
    "df3.filter(F.col('searchDate') >= F.lit('2017-06-16').cast(TimestampType())).toPandas().to_pickle(\"D:\\flight.pq.11\\df3_test.pkl\")\n",
    "In[45]:\n",
    "\n",
    "df3.groupby('randVar').agg(F.count('randVar').alias('count')).show(100)\n",
    "In[7]:\n",
    "\n",
    "listRand = [x['randVar'] for x in df3.select('randVar').distinct().collect()]\n",
    "In[9]:\n",
    "\n",
    "dfArray = [df3.where(F.col('randVar') == x) for x in listRand]\n",
    "In[15]:\n",
    "\n",
    "spark.conf.set(\"spark.driver.maxResultSize\", \"10g\")\n",
    "In[11]:\n",
    "\n",
    "i = 0\n",
    "for df_temp in dfArray:\n",
    "df_temp.toPandas().to_pickle('D:\\flight.pq.11\\df3_test_' + str(listRand[i]) + '.pkl')\n",
    "i += 1\n",
    "In[16]:\n",
    "\n",
    "(df2.filter(F.col('searchDate') <= F.lit('2017-06-23').cast(TimestampType()))\n",
    ".toPandas().to_pickle(\"D:\\flight.pq.11\\df2_v2.pkl\"))\n",
    ".limit(4000000)\n",
    "In[15]:\n",
    "\n",
    "(df2.filter(F.col('searchDate') <= F.lit('2017-06-23').cast(TimestampType()))\n",
    ".toPandas().to_pickle(\"D:\\flight.pq.11\\df2_v2.pkl\"))\n",
    ".limit(4000000)\n",
    "In[18]:\n",
    "\n",
    "get_ipython().run_cell_magic(u'time', u'', u'# pyspark --driver-memory 5g --executor-memory 25g\\n\\nimport pyspark.sql.functions as F\\n\\n# set default parquet partition to be 128mb\\n# spark._jsc.hadoopConfiguration().set(\"dfs.block.size\", \"64m\")\\n# spark._jsc.hadoopConfiguration().setInt(\"parquet.block.size\", 128*1024*1024)\\n\\n# Read in original parquet files\\nparquetFile = spark.read.parquet(\"C:\\\\Data\\Flight Data\\\\flight.pq.11\\\\flight_10_1\")\\nparquetFile2 = spark.read.parquet(\"C:\\\\Data\\\\Flight Data\\\\flight.pq.11\\\\flight_10_14\")\\n\\n# Add searchMonth\\nparquetFile = parquetFile.withColumn(\\'searchMonth\\', F.date_format(F.last_day(F.col(\\'searchDate\\')), \"YYYYMM\"))\\nparquetFile2 = parquetFile2.withColumn(\\'searchMonth\\', F.date_format(F.last_day(F.col(\\'searchDate\\')), \"YYYYMM\"))\\n\\n# Save first parquet file, partition by fromCity, toCity and searchMonth\\nparquetFile.coalesce(1).write.partitionBy(\"fromCity\", \"toCity\", \"searchMonth\").save(\"C:\\\\Data\\\\Flight Data\\\\flight.pq.11\\\\flight.pq\")\\n\\n# Save additional parquet files, partition by fromCity, toCity and searchMonth\\nparquetFile2.coalesce(1).write.partitionBy(\"fromCity\", \"toCity\", \"searchMonth\").mode(\"append\").save(\"C:\\Data\\\\Flight Data\\\\flight.pq.11\\\\flight.pq\")\\n\\n# Next: convert this to a jar file, save on s3, launch ec2 spot instance and run the jar file')\n",
    "In[8]:\n",
    "\n",
    "df2_in = pd.read_pickle(\"D:\\flight.pq.11\\df2.pkl\")\n",
    "In[12]:\n",
    "\n",
    "df2_in.searchDate.value_counts()\n",
    "In[59]:\n",
    "parquetFile.count()\n",
    "parquetFile.show(1)\n",
    "parquetFile.select('searchDate').distinct().orderBy('searchDate').show(10000, truncate=False)\n",
    "parquetFile.select('price').distinct().orderBy('price').show(10000, truncate=False)\n",
    "parquetFile.filter(parquetFile.)\n",
    "parquetFile.select('depDate').distinct().orderBy('depDate').show(100, truncate=False)\n",
    "(parquetFile.searchDate == '2017-06-12')\n",
    "In[ ]:\n",
    "\n",
    "parquetFile.show(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
